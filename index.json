{
    "content/administration/administration.html":  {
                                                       "href":  "content/administration/administration.html",
                                                       "title":  "Administration",
                                                       "keywords":  "Administration Reset the Edge Data Store application, EDS adapters, and the EDS Storage component with the EDS administration level functions. The examples in the following administration topics use curl, a commonly available tool on both Windows and Linux. The same operations can be performed with the EdgeCmd command line utility or with any programming language or tool that supports making REST calls. For a partial list of REST tools, see Configuration tools . If available on your device, use a browser to complete data retrieval steps (GET commands) to validate successful configurations."
                                                   },
    "content/administration/management-tools.html":  {
                                                         "href":  "content/administration/management-tools.html",
                                                         "title":  "Configuration tools",
                                                         "keywords":  "Configuration tools Edge Data Store and adapters can be configured with either the EdgeCmd utility, OSIsoft\u0027s proprietary tool for configuring EDS and adapters, or commonly-used REST tools. EdgeCmd utility The EdgeCmd utility enables EDS and adapter configuration on both Linux and Windows operating systems. For more information on using the EdgeCmd utility, see the EdgeCmd utility help . REST tools The following tools can be used to make REST calls. curl curl is a command line tool used to make HTTP calls and is supported on both Windows and Linux operating systems. curl can be scripted using Bash or PowerShell on either Linux or Windows, and can be used to perform EDS administrative and programming tasks. curl commands are used in configuration and management examples throughout this documentation. Postman In instances where EDS is installed on a platform with a GUI component, Postman is a useful REST tool for learning more about EDS REST APIs and creating REST calls. C#, Python, Go EDS is designed to use platform-independent programming, and any modern programming language can be used to make REST calls to administer and write programs for EDS. Since the administrative and programming interfaces use REST, applications can manage EDS and read and write data. For example, an application can access the Diagnostics namespace locally to monitor and act upon the local system state. System Tools Use Windows tools like PuTTY and WinSCP to facilitate working across platforms, such as to copy files and remotely access Linux command lines."
                                                     },
    "content/administration/reset-edge-data-store.html":  {
                                                              "href":  "content/administration/reset-edge-data-store.html",
                                                              "title":  "Reset Edge Data Store",
                                                              "keywords":  "Reset Edge Data Store When applied at the system level, the Reset command deletes all event and configuration data and restarts Edge Data Store. Note: All configuration and stored data will be lost as a result of performing this action. To reset EDS: Start any tool capable of making HTTP requests. Execute a POST command to the following endpoint, replacing \u003cport_number\u003e with the port specified for EDS: http://localhost:\u003cport_number\u003e/api/v1/administration/System/Reset http:  localhost:\u003cport_number\u003e api v1 administration System Reset Example using curl or EdgeCmd and the default port: curl EdgeCmd curl -d \"\" http://localhost:5590/api/v1/Administration/System/Reset http:  localhost:5590 api v1 Administration System Reset edgecmd reset An HTTP status 204 message indicates success."
                                                          },
    "content/administration/reset-storage-component.html":  {
                                                                "href":  "content/administration/reset-storage-component.html",
                                                                "title":  "Reset the Storage component",
                                                                "keywords":  "Reset the Storage component When applied at the storage component level, the Reset command deletes all event and configuration data related to the Storage component and restarts Edge Data Store. To reset the Storage component: Open a tool capable of making HTTP requests. Run a POST command to the following endpoint, replacing \u003cport_number\u003e with the port specified for EDS: http://localhost:\u003cport_number\u003e/api/v1/administration/Storage/Reset http:  localhost:\u003cport_number\u003e api v1 administration Storage Reset Example using curl or EdgeCmd and the default port: curl EdgeCmd curl -d \"\" http://localhost:5590/api/v1/Administration/Storage/Reset http:  localhost:5590 api v1 Administration Storage Reset edgecmd storage reset An HTTP status 204 message indicates success."
                                                            },
    "content/administration/retrieve-product-version-information.html":  {
                                                                             "href":  "content/administration/retrieve-product-version-information.html",
                                                                             "title":  "Retrieve product version information",
                                                                             "keywords":  "Retrieve product version information The product version information includes the Edge Data Store application version, the adapter framework version, the .NET runtime version, and the operating system. This information can be useful for troubleshooting purposes. To retrieve the product version information of EDS: Open a tool capable of making HTTP requests. Run a GET command to the following endpoint, replacing \u003cport_number\u003e with the port specified for EDS: http://localhost:\u003cport_number\u003e/api/v1/diagnostics/productinformation http:  localhost:\u003cport_number\u003e api v1 diagnostics productinformation Example using curl and EdgeCmd and the default port number: curl EdgeCmd curl -v http://localhost:5590/api/v1/Diagnostics/ProductInformation http:  localhost:5590 api v1 Diagnostics ProductInformation edgecmd get version Example Response { \"Product Name\": \"Edge Data Store\", \"Product Version\": \"1.1.1.46\", \"Adapter Framework Version\": \"1.6.0.24\", \"Runtime Version\": \".NET 6.0.9\", \"Operating System\": \"Microsoft Windows 10.0.19044\" }"
                                                                         },
    "content/administration/stop-and-start-eds-adapter.html":  {
                                                                   "href":  "content/administration/stop-and-start-eds-adapter.html",
                                                                   "title":  "Stop and start an EDS adapter",
                                                                   "keywords":  "Stop and start an EDS adapter By default, when Edge Data Store starts, all currently configured EDS adapter instance are started and remain running until the product shuts down. Stop an EDS adapter To stop an EDS adapter instance: Open a tool capable of making HTTP requests. Run a POST command to the following endpoint, replacing \u003cadapterId\u003e with the adapter instance to stop and \u003cport_number\u003e with the port specified for EDS: http://localhost:\u003cport_number\u003e/api/v1/administration/\u003cadapterId\u003e/Stop http:  localhost:\u003cport_number\u003e api v1 administration \u003cadapterId\u003e Stop Example Stop the OpcUa1 adapter using curl or EdgeCmd and the default port: curl EdgeCmd curl -d \"\" http://localhost:5590/api/v1/Administration/OpcUa1/Stop http:  localhost:5590 api v1 Administration OpcUa1 Stop edgecmd -cid \u003cComponentID\u003e stop An HTTP status 204 message indicates success. Start an EDS adapter To start an EDS adapter instance: Open a tool capable of making HTTP requests. Run a POST command to the following endpoint, replacing \u003cadapterId\u003e with the adapter instance to start and \u003cport_number\u003e with the port specified for EDS: http://localhost:\u003cport_number\u003e/api/v1/administration/\u003cadapterId\u003e/Start http:  localhost:\u003cport_number\u003e api v1 administration \u003cadapterId\u003e Start Example Stop the Modbus1 adapter using curl or EdgeCmd and the default port: curl EdgeCmd curl -d \"\" http://localhost:5590/api/v1/Administration/Modbus1/Start http:  localhost:5590 api v1 Administration Modbus1 Start edgecmd -cid \u003cComponentID\u003e start An HTTP status 204 message indicates success."
                                                               },
    "content/configuration/buffering.html":  {
                                                 "href":  "content/configuration/buffering.html",
                                                 "title":  "Buffering configuration",
                                                 "keywords":  "Buffering configuration You can configure PI adapters to buffer data egressed from the adapter to endpoints. Buffering is configured through the buffering configuration parameters in the system configuration. Note: OSIsoft recommends that you do not modify the default buffering location unless it is necessary. Changes to the buffering configuration parameters only take effect during adapter service startup. Configure buffering Complete the following steps to configure buffering. Use the PUT method in conjunction with the http://localhost:5590/api/v1/configuration/system/buffering http:  localhost:5590 api v1 configuration system buffering REST endpoint to initialize the configuration. Using a text editor, create an empty text file. Copy and paste an example configuration for buffering into the file. For sample JSON, see Examples - Retrieve the buffering configuration . Update the example JSON parameters for your environment. For a table of all available parameters, see Buffering parameters . Save the file. For example, as ConfigureBuffering.json . Open a command line session. Change directory to the location of ConfigureBuffering.json . Enter the following curl command (which uses the PUT method) to initialize the buffering configuration. curl EdgeCmd curl -d \"@ConfigureBuffering.json\" -H \"Content-Type: application/json\" application json\" -X PUT \"http://localhost:5590/api/v1/configuration/system/buffering\" \"http:  localhost:5590 api v1 configuration system buffering\" edgecmd set buffering -file ConfigureBuffering.json Notes: If you installed the adapter to listen on a non-default port, update 5590 to the port number in use. For a list of other REST operations you can perform, like updating or replacing a buffering configuration, see REST URLs . Buffering schema The full schema definition for the system buffering is in the System_Buffering_schema.json file located in one of the following folders: Windows: %ProgramFiles%\\OSIsoft\\Adapters\\\u003cAdapterName\u003e\\Schemas Linux: /opt/OSIsoft/Adapters/\u003cAdapterName\u003e/Schemas  opt OSIsoft Adapters \u003cAdapterName\u003e Schemas Buffering parameters The following parameters are available for configuring buffering: Parameter Required Type Description EnablePersistentBuffering Optional boolean Enables or disables on-disk buffering Allowed value: true or false Default value: true Note: If you disable persistent buffering, in-memory buffering is used. On-disk and in-memory buffering are limited by value in the MaxBufferSizeMB property. MaxBufferSizeMB Optional integer Defines the maximum size of the buffer that is persisted on disk 1 or used in memory 2 . The unit is specified in MB (1 Megabyte = 1048576 bytes). Consider the capacity and the type of storage medium to determine a suitable value for this parameter. Minimum value: 1 Maximum value: 2147483647 Default value: 1024 Note: The MaxBufferSizeMB property is applied to each configured endpoint. For example, if you set the MaxBufferSizeMB to 1024 and you configured the adapter to send data to two endpoints (for example, PI Server and OCS), the total maximum resources used for buffering will be 2048 . The health endpoint is an exception fixed at 20 MB. BufferLocation Required string Defines the location of the buffer files. Absolute paths are required. Consider the access-control list (ACL) when you set this parameter. BufferLocation is used to buffer files when EnablePersistentBuffering is true . Allowed value: Valid path to a folder location in the file system Default value: Windows: %ProgramData%\\OSIsoft\\Adapters\\{AdapterInstance}\\Buffers Linux: /usr/share/OSIsoft/Adapters/{AdapterInstance}/Buffers  usr share OSIsoft Adapters {AdapterInstance} Buffers 1 Buffering to disk - disk is only used if required; Data is only written to the disk buffer if queued in the memory buffer for more than 5 seconds. The MaxBufferSizeMB is applied per configured endpoint except the health endpoint. An adapter creates 20 MB buffer files that are stored in BufferLocation . When MaxBufferSizeMB is reached, the oldest buffer file is deleted and a new buffer file is created. The health endpoint is fixed at 20 MB. When the health endpoint buffer file becomes full, a new buffer file is created and the previous buffer file is deleted. Note: The following rules apply in case of an error when creating a new buffer file: Attempt to delete oldest buffer file and retry. If unable to buffer, errors are logged to indicate data loss. If a buffer file is corrupted, an attempt is made to recover individual records and any failure to recover records is logged. 2 Buffering only to memory : The MaxBufferSizeMB is applied per configured endpoint except the health endpoint. When MaxBufferSizeMB is reached, the oldest messages in the memory buffer are removed. Depending on the size of a new message, several old messages may be removed. The health endpoint is fixed at 20 MB. When the health endpoint buffer file becomes full, the oldest messages in the memory buffer are removed and new messages are added. Examples The following examples are buffering configurations made through the curl REST client. Retrieve the buffering configuration curl -X GET \"http://localhost:5590/api/v1/configuration/system/buffering\" \"http:  localhost:5590 api v1 configuration system buffering\" Sample output: { \"bufferLocation\": \"C:/ProgramData/OSIsoft/Adapters/\u003cAdapterName\u003e/Buffers\", \"C: ProgramData OSIsoft Adapters \u003cAdapterName\u003e Buffers\", \"maxBufferSizeMB\": 1024, \"enablePersistentBuffering\": true } 200 OK response indicates success. Update MaxBufferSizeMb parameter curl -d \"{ \\\"MaxBufferSizeMB\\\": 100 }\" -H \"Content-Type: application/json\" application json\" -X PATCH \"http://localhost:5590/api/v1/configuration/system/buffering\" \"http:  localhost:5590 api v1 configuration system buffering\" 204 No Content response indicates success. REST URLs Relative URL HTTP verb Action api/v1/configuration/system/buffering api v1 configuration system buffering GET Gets the buffering configuration api/v1/configuration/system/buffering api v1 configuration system buffering PUT Replaces the existing buffering configuration api/v1/configuration/system/buffering api v1 configuration system buffering PATCH Update parameter, partial configuration"
                                             },
    "content/configuration/configuration.html":  {
                                                     "href":  "content/configuration/configuration.html",
                                                     "title":  "Configuration",
                                                     "keywords":  "Configuration Configure each functional area of Edge Data Store to define connectivity and system behavior appropriate to your installation. The topics contained in this section provide instructions for each area. The examples in the Configuration topics use curl, a commonly available tool on both Windows and Linux. You can use any programming language or tool that supports making REST calls to perform the same operations. You can also use the EdgeCmd utility to configure Edge Data Store components. To validate successful configurations, use a browser to retrieve data using GET commands. For more information on tools for Edge Data Store configuration, see Configuration tools ."
                                                 },
    "content/configuration/eds-configuration.html":  {
                                                         "href":  "content/configuration/eds-configuration.html",
                                                         "title":  "Edge Data Store configuration",
                                                         "keywords":  "Edge Data Store configuration Edge Data Store requires configuration, which can be performed either for each individual component or for the system as a whole. Use the following procedures to configure Edge Data Store as a whole system with either a minimum or a maximum configuration. All parameters are described in detail in the individual component sections. Configure minimum Edge Data Store The following JSON file represents the minimal configuration of an Edge Data Store. There are no Modbus TCP EDS adapter or OPC UA EDS adapter components and the Storage component configurations are set to the default. If you configure a system with this JSON file, any existing Modbus TCP EDS adapter or OPC UA EDS adapter components are disabled and removed. No storage data is deleted or modified and OMF and SDS data access is not impacted. Save or copy the example JSON in a file named EdgeMinimumConfiguration.json in any directory on a device with Edge Data Store installed. Run the following curl command or EdgeCmd from the directory where the file is located: curl EdgeCmd curl -d \"@EdgeMinimumConfiguration.json\" -H \"Content-Type: application/json\" application json\" -X PUT http://localhost:5590/api/v1/configuration http:  localhost:5590 api v1 configuration edgecmd set application -file EdgeMinimumConfiguration.json The following will be set as the configuration of a running Edge Data Store. The configuration takes effect immediately after the command completes. { \"Storage\": { \"EgressEndpoints\": [], \"Schedules\": [], \"DataSelectors\": [], \"EgressConfigurations\": [], \"Egresses\": [], \"Runtime\": { \"streamStorageLimitMb\": 2, \"streamStorageTargetMb\": 1, \"ingressDebugExpiration\": \"0001-01-01T00:00:00\", \"checkpointRateInSec\": 30, \"transactionLogLimitMB\": 250, \"enableMetrics\": false }, \"Logging\": { \"logLevel\": \"Information\", \"logFileSizeLimitBytes\": 34636833, \"logFileCountLimit\": 31 } }, \"System\": { \"Logging\": { \"logLevel\": \"Information\", \"logFileSizeLimitBytes\": 34636833, \"logFileCountLimit\": 31 }, \"HealthEndpoints\": [], \"Components\": [ { \"componentId\": \"Storage\", \"componentType\": \"Storage\" } ], \"Buffering\": { \"bufferLocation\": \"C:/ProgramData/OSIsoft/EdgeDataStore/Buffers\", \"C: ProgramData OSIsoft EdgeDataStore Buffers\", \"maxBufferSizeMB\": 1024, \"enablePersistentBuffering\": true }, \"General\": { \"enableDiagnostics\": true, \"metadataLevel\": \"Medium\", \"healthPrefix\": null } } } This example results in a minimal configuration of Edge Data Store. It only supports OMF and SDS operations using REST. No egress is configured, so no data will be sent to either AVEVA Data Hub or PI Web API. Configure maximum Edge Data Store The following JSON file represents the maximum configuration of an Edge Data Store. There are Modbus TCP EDS adapter components and OPC UA EDS adapter components and egress is configured to send to both PI Web API and AVEVA Data Hub from both the default (operational data) and diagnostics (diagnostic data) namespace. Using any text editor, create a JSON file using the following example. Fill in any credentials or IP addresses with appropriate values for your environment. Save the edited JSON in a file named EdgeMaximumConfiguration.json in any directory. Run the following curl command or EdgeCmd from the same directory where the file is located: curl EdgeCmd curl -d \"@EdgeMaximumConfiguration.json\" -H \"Content-Type: application/json\" application json\" -X PUT http://localhost:5590/api/v1/configuration http:  localhost:5590 api v1 configuration edgecmd set application -file EdgeMaximumConfiguration.json The following will be set as the configuration for the running Edge Data Store. The configuration takes effect immediately after the command completes. { \"Modbus1\": { \"Logging\": { \"logLevel\": \"Information\", \"logFileSizeLimitBytes\": 34636833, \"logFileCountLimit\": 31 }, \"DataSource\": { \"devices\": [ { \"id\": \"Device\", \"ipAddress\": \"\u003cModbus IP address\u003e\", \"port\": 502 } ], \"simultaneousRequests\": 1, \"maxResponseDataLength\": 250, \"connectTimeout\": \"0:00:15\", \"reconnectInterval\": \"0:00:05\", \"requestTimeout\": \"0:00:09\", \"delayBetweenRequests\": \"0:00:00\", \"streamIdPrefix\": null, \"defaultStreamIdPattern\": \"{DeviceId}.{UnitId}.{RegisterType}.{RegisterOffset}\" }, \"DataSelection\": [ { \"deviceId\": \"Device\", \"scheduleId\": \"Schedule_500\", \"unitId\": 1, \"registerType\": \"Holding16\", \"registerOffset\": 1, \"dataTypeCode\": 20, \"bitMap\": \"16151413\", \"conversionFactor\": 2, \"conversionOffset\": 3.4, \"selected\": true, \"name\": null, \"streamId\": null, \"dataFilterId\": null }, { \"deviceId\": \"Device\", \"scheduleId\": \"Schedule_500\", \"unitId\": 1, \"registerType\": \"Holding16\", \"registerOffset\": 2, \"dataTypeCode\": 20, \"bitMap\": \"16151413\", \"conversionFactor\": 2, \"conversionOffset\": 3.4, \"selected\": true, \"name\": null, \"streamId\": null, \"dataFilterId\": null }, { \"deviceId\": \"Device\", \"scheduleId\": \"Schedule_500\", \"unitId\": 1, \"registerType\": \"Holding16\", \"registerOffset\": 3, \"dataTypeCode\": 20, \"bitMap\": \"16151413\", \"conversionFactor\": 2, \"conversionOffset\": 3.4, \"selected\": true, \"name\": null, \"streamId\": null, \"dataFilterId\": null }, { \"deviceId\": \"Device\", \"scheduleId\": \"Schedule_500\", \"unitId\": 1, \"registerType\": \"Holding16\", \"registerOffset\": 4, \"dataTypeCode\": 20, \"bitMap\": \"16151413\", \"conversionFactor\": 2, \"conversionOffset\": 3.4, \"selected\": true, \"name\": null, \"streamId\": null, \"dataFilterId\": null }, { \"deviceId\": \"Device\", \"scheduleId\": \"Schedule_500\", \"unitId\": 1, \"registerType\": \"Holding16\", \"registerOffset\": 5, \"dataTypeCode\": 20, \"bitMap\": \"16151413\", \"conversionFactor\": 2, \"conversionOffset\": 3.4, \"selected\": true, \"name\": null, \"streamId\": null, \"dataFilterId\": null } ], \"DataFilters\": [ { \"id\": \"DuplicateData\", \"absoluteDeadband\": 0, \"percentChange\": null, \"expirationPeriod\": \"1:00:00\" } ], \"Schedules\": [ { \"id\": \"Schedule_500\", \"period\": \"0:00:00.5\", \"offset\": \"0:00:00\" } ] }, \"Storage\": { \"EgressEndpoints\": [], \"Schedules\": [], \"DataSelectors\": [], \"EgressConfigurations\": [], \"Runtime\": { \"streamStorageLimitMb\": 2, \"streamStorageTargetMb\": 1, \"ingressDebugExpiration\": \"0001-01-01T00:00:00\", \"checkpointRateInSec\": 30, \"transactionLogLimitMB\": 250, \"enableMetrics\": false }, \"Logging\": { \"logLevel\": \"Information\", \"logFileSizeLimitBytes\": 34636833, \"logFileCountLimit\": 31 } }, \"OpcUa1\": { \"Logging\": { \"logLevel\": \"Information\", \"logFileSizeLimitBytes\": 34636833, \"logFileCountLimit\": 31 }, \"DataSource\": { \"endpointUrl\": \"opc.tcp://\u003cOPC \"opc.tcp:  \u003cOPC UA server IP and port\u003e/OSIsoftTestServer\", port\u003e OSIsoftTestServer\", \"useSecureConnection\": false, \"userName\": null, \"password\": null, \"incomingTimestamp\": \"Source\", \"dataCollectionMode\": \"CurrentWithBackfill\", \"streamIdPrefix\": \"OpcUa\", \"defaultStreamIdPattern\": \"{NamespaceIndex}.{Identifier}\" }, \"DataSelection\": [ { \"nodeId\": \"ns=2;s=Line1.HeatExchanger1001.ColdSideInletTemperature\", \"selected\": true, \"name\": \"Cold Side Inlet Temperature\", \"streamId\": \"2.Line1.HeatExchanger1001.ColdSideInletTemperature\", \"dataFilterId\": null }, { \"nodeId\": \"ns=2;s=Line1.HeatExchanger1001.ColdSideOutletTemperature\", \"selected\": false, \"name\": \"Cold Side Outlet Temperature\", \"streamId\": \"2.Line1.HeatExchanger1001.ColdSideOutletTemperature\", \"dataFilterId\": null }, { \"nodeId\": \"ns=2;s=Line1.HeatExchanger1001.HotSideInletTemperature\", \"selected\": true, \"name\": \"Hot Side Inlet Temperature\", \"streamId\": \"2.Line1.HeatExchanger1001.HotSideInletTemperature\", \"dataFilterId\": null }, { \"nodeId\": \"ns=2;s=Line1.HeatExchanger1001.HotSideOutletTemperature\", \"selected\": true, \"name\": \"Hot Side Outlet Temperature\", \"streamId\": \"2.Line1.HeatExchanger1001.HotSideOutletTemperature\", \"dataFilterId\": null }, { \"nodeId\": \"ns=2;s=Line1.HeatExchanger1002.ColdSideInletTemperature\", \"selected\": true, \"name\": \"Cold Side Inlet Temperature\", \"streamId\": \"2.Line1.HeatExchanger1002.ColdSideInletTemperature\", \"dataFilterId\": null }, { \"nodeId\": \"ns=2;s=Line1.HeatExchanger1002.ColdSideOutletTemperature\", \"selected\": false, \"name\": \"Cold Side Outlet Temperature\", \"streamId\": \"2.Line1.HeatExchanger1002.ColdSideOutletTemperature\", \"dataFilterId\": null }, { \"nodeId\": \"ns=2;s=Line1.HeatExchanger1002.HotSideInletTemperature\", \"selected\": false, \"name\": \"Hot Side Inlet Temperature\", \"streamId\": \"2.Line1.HeatExchanger1002.HotSideInletTemperature\", \"dataFilterId\": null }, { \"nodeId\": \"ns=2;s=Line1.HeatExchanger1002.HotSideOutletTemperature\", \"selected\": true, \"name\": \"Hot Side Outlet Temperature\", \"streamId\": \"2.Line1.HeatExchanger1002.HotSideOutletTemperature\", \"dataFilterId\": null }, { \"nodeId\": \"ns=2;s=Line1.SF_Pump_001.Power\", \"selected\": false, \"name\": \"Power\", \"streamId\": \"2.Line1.SF_Pump_001.Power\", \"dataFilterId\": null }, { \"nodeId\": \"ns=2;s=Line1.SF_Pump_001.Efficiency\", \"selected\": false, \"name\": \"Efficiency\", \"streamId\": \"2.Line1.SF_Pump_001.Efficiency\", \"dataFilterId\": null }, { \"nodeId\": \"ns=2;s=Line1.SF_Pump_001.Flowrate\", \"selected\": false, \"name\": \"Flowrate\", \"streamId\": \"2.Line1.SF_Pump_001.Flowrate\", \"dataFilterId\": null }, { \"nodeId\": \"ns=2;s=Line1.SF_Pump_002.Power\", \"selected\": false, \"name\": \"Power\", \"streamId\": \"2.Line1.SF_Pump_002.Power\", \"dataFilterId\": null }, { \"nodeId\": \"ns=2;s=Line1.SF_Pump_002.Efficiency\", \"selected\": false, \"name\": \"Efficiency\", \"streamId\": \"2.Line1.SF_Pump_002.Efficiency\", \"dataFilterId\": null }, { \"nodeId\": \"ns=2;s=Line1.SF_Pump_002.Flowrate\", \"selected\": false, \"name\": \"Flowrate\", \"streamId\": \"2.Line1.SF_Pump_002.Flowrate\", \"dataFilterId\": null }, { \"nodeId\": \"ns=2;s=Line1.Tank1.Level\", \"selected\": false, \"name\": \"Level\", \"streamId\": \"2.Line1.Tank1.Level\", \"dataFilterId\": null }, { \"nodeId\": \"ns=2;s=Line1.Tank1.Mass\", \"selected\": false, \"name\": \"Mass\", \"streamId\": \"2.Line1.Tank1.Mass\", \"dataFilterId\": null }, { \"nodeId\": \"ns=2;s=Line1.Tank1.Volume\", \"selected\": false, \"name\": \"Volume\", \"streamId\": \"2.Line1.Tank1.Volume\", \"dataFilterId\": null }, { \"nodeId\": \"ns=2;s=Line1.Tank2.Level\", \"selected\": false, \"name\": \"Level\", \"streamId\": \"2.Line1.Tank2.Level\", \"dataFilterId\": null }, { \"nodeId\": \"ns=2;s=Line1.Tank2.Mass\", \"selected\": false, \"name\": \"Mass\", \"streamId\": \"2.Line1.Tank2.Mass\", \"dataFilterId\": null }, { \"nodeId\": \"ns=2;s=Line1.Tank2.Volume\", \"selected\": false, \"name\": \"Volume\", \"streamId\": \"2.Line1.Tank2.Volume\", \"dataFilterId\": null } ], \"DataFilters\": [ { \"id\": \"DuplicateData\", \"absoluteDeadband\": 0, \"percentChange\": null, \"expirationPeriod\": \"1:00:00\" } ], \"IntervalsToRecover\": [ { \"startTime\": \"2022-07-12T16:24:46.0546317Z\", \"endTime\": null, \"lastReadTime\": null } ], \"ClientSettings\": { \"maxBrowseReferencesToReturn\": 0, \"browseBlockSize\": 10, \"reconnectDelay\": \"0:00:30\", \"recreateSubscriptionDelay\": \"0:00:10\", \"readBlockSize\": 1000, \"sessionRequestTimeout\": \"0:02:00\", \"connectionTimeout\": \"0:00:30\", \"readTimeout\": \"0:00:30\", \"sessionAllowInsecureCredentials\": false, \"sessionMaxOperationsPerRequest\": 1000, \"browseTimeout\": \"0:01:00\", \"maxMonitoredItemsPerCall\": 1000, \"maxNotificationsPerPublish\": 0, \"publishingInterval\": \"0:00:01\", \"createMonitoredItemsTimeout\": \"0:00:30\", \"monitoredItemDataChangeTrigger\": \"StatusValue\", \"samplingInterval\": \"0:00:00.5\", \"monitoredItemQueueSize\": 2, \"maxInternalQueueSize\": 500000, \"historyReadBlockSize\": 10, \"historyReadTimeout\": \"0:01:00\" }, \"Discoveries\": [], \"HistoryRecoveries\": [] }, \"System\": { \"Logging\": { \"logLevel\": \"Information\", \"logFileSizeLimitBytes\": 34636833, \"logFileCountLimit\": 31 }, \"HealthEndpoints\": [], \"Components\": [ { \"componentId\": \"OpcUa1\", \"componentType\": \"OpcUa\" }, { \"componentId\": \"Modbus1\", \"componentType\": \"Modbus\" }, { \"componentId\": \"Storage\", \"componentType\": \"Storage\" } ], \"Buffering\": { \"bufferLocation\": \"C:/ProgramData/OSIsoft/EdgeDataStore/Buffers\", \"C: ProgramData OSIsoft EdgeDataStore Buffers\", \"maxBufferSizeMB\": 1024, \"enablePersistentBuffering\": true }, \"General\": { \"enableDiagnostics\": true, \"metadataLevel\": \"Medium\", \"healthPrefix\": null } } } This example results in a minimal configuration of Edge Data Store. It only supports OMF and SDS operations using REST. No egress is configured, so no data is sent to either AVEVA Data Hub or PI Web API. Configure maximum Edge Data Store The following JSON file represents the maximum configuration of an Edge Data Store. There are Modbus TCP EDS adapter components and OPC UA EDS adapter components and egress is configured to send to both PI Web API and AVEVA Data Hub from both the default (operational data) and diagnostics (diagnostic data) namespace. Using any text editor, create a JSON file using the following example. Fill in any credentials or IP addresses with appropriate values for your environment. Save the edited JSON in a file named EdgeMaximumConfiguration.json in any directory. Run the following curl command from the same directory where the file is located: curl -d \"@EdgeMaximumConfiguration.json\" -H \"Content-Type: application/json\" application json\" -X PUT http://localhost:5590/api/v1/configuration http:  localhost:5590 api v1 configuration The following will be set as the configuration for the running Edge Data Store. The configuration takes effect immediately after the command completes. { \"Modbus1\": { \"Logging\": { \"logLevel\": \"Information\", \"logFileSizeLimitBytes\": 34636833, \"logFileCountLimit\": 31 }, \"DataSource\": { \"devices\": [ { \"id\": \"Device\", \"ipAddress\": \"\u003cModbus IP address\u003e\", \"port\": 502 } ], \"simultaneousRequests\": 1, \"maxResponseDataLength\": 250, \"reconnectInterval\": \"0:00:05\", \"requestTimeout\": \"0:00:09\", \"delayBetweenRequests\": \"0:00:00\", \"streamIdPrefix\": null, \"defaultStreamIdPattern\": \"{DeviceId}.{UnitId}.{RegisterType}.{RegisterOffset}\" }, \"DataSelection\": [ { \"deviceId\": \"Device\", \"scheduleId\": \"Schedule_500\", \"unitId\": 1, \"registerType\": \"Holding16\", \"registerOffset\": 1, \"dataTypeCode\": 20, \"bitMap\": \"16151413\", \"conversionFactor\": 2, \"conversionOffset\": 3.4, \"selected\": true, \"name\": null, \"streamId\": null, \"dataFilterId\": null }, { \"deviceId\": \"Device\", \"scheduleId\": \"Schedule_500\", \"unitId\": 1, \"registerType\": \"Holding16\", \"registerOffset\": 2, \"dataTypeCode\": 20, \"bitMap\": \"16151413\", \"conversionFactor\": 2, \"conversionOffset\": 3.4, \"selected\": true, \"name\": null, \"streamId\": null, \"dataFilterId\": null }, { \"deviceId\": \"Device\", \"scheduleId\": \"Schedule_500\", \"unitId\": 1, \"registerType\": \"Holding16\", \"registerOffset\": 3, \"dataTypeCode\": 20, \"bitMap\": \"16151413\", \"conversionFactor\": 2, \"conversionOffset\": 3.4, \"selected\": true, \"name\": null, \"streamId\": null, \"dataFilterId\": null }, { \"deviceId\": \"Device\", \"scheduleId\": \"Schedule_500\", \"unitId\": 1, \"registerType\": \"Holding16\", \"registerOffset\": 4, \"dataTypeCode\": 20, \"bitMap\": \"16151413\", \"conversionFactor\": 2, \"conversionOffset\": 3.4, \"selected\": true, \"name\": null, \"streamId\": null, \"dataFilterId\": null }, { \"deviceId\": \"Device\", \"scheduleId\": \"Schedule_500\", \"unitId\": 1, \"registerType\": \"Holding16\", \"registerOffset\": 5, \"dataTypeCode\": 20, \"bitMap\": \"16151413\", \"conversionFactor\": 2, \"conversionOffset\": 3.4, \"selected\": true, \"name\": null, \"streamId\": null, \"dataFilterId\": null } ], \"DataFilters\": [ { \"id\": \"DuplicateData\", \"absoluteDeadband\": 0, \"percentChange\": null, \"expirationPeriod\": \"1:00:00\" } ], \"Schedules\": [ { \"id\": \"Schedule_500\", \"period\": \"0:00:00.5\", \"offset\": \"0:00:00\" } ] }, \"Storage\": { \"EgressEndpoints\": [ { \"id\": \"Endpoint-ADH\", \"endpoint\": \"\u003cAVEVA Data Hub OMF URL for your tenant and namespace\u003e\", \"userName\": null, \"password\": null, \"clientId\": \"\u003cAVEVA Data Hub ClientId\u003e\", \"clientSecret\": \"\u003cAVEVA Data Hub ClientSecret\u003e\", \"debugExpiration\": null, \"tokenEndpoint\": null, \"validateEndpointCertificate\": true }, { \"id\": \"Endpoint-PWA\", \"endpoint\": \"https://\u003cyour \"https:  \u003cyour PI Web API server\u003e/piwebapi/omf/\", server\u003e piwebapi omf \", \"userName\": \"\u003cusername\u003e\", \"password\": \"\u003cpassword\u003e\", \"clientId\": null, \"clientSecret\": null, \"debugExpiration\": null, \"tokenEndpoint\": null, \"validateEndpointCertificate\": true } ], \"Schedules\": [ { \"id\": \"Schedule1\", \"period\": \"0:00:50\", \"startTime\": null } ], \"DataSelectors\": [ { \"id\": \"DataSelector1\", \"streamFilter\": \"TypeId:TestType*\", \"absoluteDeadband\": 5, \"percentChange\": null, \"expirationPeriod\": null } ], \"EgressConfigurations\": [ { \"id\": \"ADH\", \"name\": null, \"description\": null, \"enabled\": true, \"endpointId\": \"Endpoint-ADH\", \"scheduleId\": \"Schedule1\", \"dataSelectorIds\": null, \"namespaceId\": \"default\", \"backfill\": false, \"streamPrefix\": \"ChangeMe\", \"typePrefix\": \"ChangeMe\" }, { \"id\": \"PWA\", \"name\": null, \"description\": null, \"enabled\": true, \"endpointId\": \"Endpoint-PWA\", \"scheduleId\": \"Schedule1\", \"dataSelectorIds\": [ \"DataSelector1\" ], \"namespaceId\": \"default\", \"backfill\": false, \"streamPrefix\": \"ChangeMe\", \"typePrefix\": \"ChangeMe\" }, { \"id\": \"ADHDiag\", \"name\": null, \"description\": null, \"enabled\": true, \"endpointId\": \"Endpoint-ADH\", \"scheduleId\": \"Schedule1\", \"dataSelectorIds\": [ \"DataSelector1\" ], \"namespaceId\": \"diagnostics\", \"backfill\": false, \"streamPrefix\": \"ChangeMe\", \"typePrefix\": \"ChangeMe\" }, { \"id\": \"PWADiag\", \"name\": null, \"description\": null, \"enabled\": true, \"endpointId\": \"Endpoint-PWA\", \"scheduleId\": \"Schedule1\", \"dataSelectorIds\": null, \"namespaceId\": \"diagnostics\", \"backfill\": false, \"streamPrefix\": \"ChangeMe\", \"typePrefix\": \"ChangeMe\" } ], \"Runtime\": { \"streamStorageLimitMb\": 2, \"streamStorageTargetMb\": 1, \"ingressDebugExpiration\": \"0001-01-01T00:00:00\", \"checkpointRateInSec\": 30, \"transactionLogLimitMB\": 250, \"enableMetrics\": false }, \"Logging\": { \"logLevel\": \"Information\", \"logFileSizeLimitBytes\": 34636833, \"logFileCountLimit\": 31 } }, \"OpcUa1\": { \"Logging\": { \"logLevel\": \"Information\", \"logFileSizeLimitBytes\": 34636833, \"logFileCountLimit\": 31 }, \"DataSource\": { \"endpointUrl\": \"opc.tcp://\u003cOPC \"opc.tcp:  \u003cOPC UA server IP and port\u003e/OSIsoftTestServer\", port\u003e OSIsoftTestServer\", \"useSecureConnection\": false, \"userName\": null, \"password\": null, \"incomingTimestamp\": \"Source\", \"dataCollectionMode\": \"CurrentWithBackfill\", \"streamIdPrefix\": \"OpcUa\", \"defaultStreamIdPattern\": \"{NamespaceIndex}.{Identifier}\" }, \"DataSelection\": [ { \"nodeId\": \"ns=2;s=Line1.HeatExchanger1001.ColdSideInletTemperature\", \"selected\": true, \"name\": \"Cold Side Inlet Temperature\", \"streamId\": \"2.Line1.HeatExchanger1001.ColdSideInletTemperature\", \"dataFilterId\": null }, { \"nodeId\": \"ns=2;s=Line1.HeatExchanger1001.ColdSideOutletTemperature\", \"selected\": false, \"name\": \"Cold Side Outlet Temperature\", \"streamId\": \"2.Line1.HeatExchanger1001.ColdSideOutletTemperature\", \"dataFilterId\": null }, { \"nodeId\": \"ns=2;s=Line1.HeatExchanger1001.HotSideInletTemperature\", \"selected\": true, \"name\": \"Hot Side Inlet Temperature\", \"streamId\": \"2.Line1.HeatExchanger1001.HotSideInletTemperature\", \"dataFilterId\": null }, { \"nodeId\": \"ns=2;s=Line1.HeatExchanger1001.HotSideOutletTemperature\", \"selected\": true, \"name\": \"Hot Side Outlet Temperature\", \"streamId\": \"2.Line1.HeatExchanger1001.HotSideOutletTemperature\", \"dataFilterId\": null }, { \"nodeId\": \"ns=2;s=Line1.HeatExchanger1002.ColdSideInletTemperature\", \"selected\": true, \"name\": \"Cold Side Inlet Temperature\", \"streamId\": \"2.Line1.HeatExchanger1002.ColdSideInletTemperature\", \"dataFilterId\": null }, { \"nodeId\": \"ns=2;s=Line1.HeatExchanger1002.ColdSideOutletTemperature\", \"selected\": false, \"name\": \"Cold Side Outlet Temperature\", \"streamId\": \"2.Line1.HeatExchanger1002.ColdSideOutletTemperature\", \"dataFilterId\": null }, { \"nodeId\": \"ns=2;s=Line1.HeatExchanger1002.HotSideInletTemperature\", \"selected\": false, \"name\": \"Hot Side Inlet Temperature\", \"streamId\": \"2.Line1.HeatExchanger1002.HotSideInletTemperature\", \"dataFilterId\": null }, { \"nodeId\": \"ns=2;s=Line1.HeatExchanger1002.HotSideOutletTemperature\", \"selected\": true, \"name\": \"Hot Side Outlet Temperature\", \"streamId\": \"2.Line1.HeatExchanger1002.HotSideOutletTemperature\", \"dataFilterId\": null }, { \"nodeId\": \"ns=2;s=Line1.SF_Pump_001.Power\", \"selected\": false, \"name\": \"Power\", \"streamId\": \"2.Line1.SF_Pump_001.Power\", \"dataFilterId\": null }, { \"nodeId\": \"ns=2;s=Line1.SF_Pump_001.Efficiency\", \"selected\": false, \"name\": \"Efficiency\", \"streamId\": \"2.Line1.SF_Pump_001.Efficiency\", \"dataFilterId\": null }, { \"nodeId\": \"ns=2;s=Line1.SF_Pump_001.Flowrate\", \"selected\": false, \"name\": \"Flowrate\", \"streamId\": \"2.Line1.SF_Pump_001.Flowrate\", \"dataFilterId\": null }, { \"nodeId\": \"ns=2;s=Line1.SF_Pump_002.Power\", \"selected\": false, \"name\": \"Power\", \"streamId\": \"2.Line1.SF_Pump_002.Power\", \"dataFilterId\": null }, { \"nodeId\": \"ns=2;s=Line1.SF_Pump_002.Efficiency\", \"selected\": false, \"name\": \"Efficiency\", \"streamId\": \"2.Line1.SF_Pump_002.Efficiency\", \"dataFilterId\": null }, { \"nodeId\": \"ns=2;s=Line1.SF_Pump_002.Flowrate\", \"selected\": false, \"name\": \"Flowrate\", \"streamId\": \"2.Line1.SF_Pump_002.Flowrate\", \"dataFilterId\": null }, { \"nodeId\": \"ns=2;s=Line1.Tank1.Level\", \"selected\": false, \"name\": \"Level\", \"streamId\": \"2.Line1.Tank1.Level\", \"dataFilterId\": null }, { \"nodeId\": \"ns=2;s=Line1.Tank1.Mass\", \"selected\": false, \"name\": \"Mass\", \"streamId\": \"2.Line1.Tank1.Mass\", \"dataFilterId\": null }, { \"nodeId\": \"ns=2;s=Line1.Tank1.Volume\", \"selected\": false, \"name\": \"Volume\", \"streamId\": \"2.Line1.Tank1.Volume\", \"dataFilterId\": null }, { \"nodeId\": \"ns=2;s=Line1.Tank2.Level\", \"selected\": false, \"name\": \"Level\", \"streamId\": \"2.Line1.Tank2.Level\", \"dataFilterId\": null }, { \"nodeId\": \"ns=2;s=Line1.Tank2.Mass\", \"selected\": false, \"name\": \"Mass\", \"streamId\": \"2.Line1.Tank2.Mass\", \"dataFilterId\": null }, { \"nodeId\": \"ns=2;s=Line1.Tank2.Volume\", \"selected\": false, \"name\": \"Volume\", \"streamId\": \"2.Line1.Tank2.Volume\", \"dataFilterId\": null } ], \"DataFilters\": [ { \"id\": \"DuplicateData\", \"absoluteDeadband\": 0, \"percentChange\": null, \"expirationPeriod\": \"1:00:00\" } ], \"IntervalsToRecover\": [ { \"startTime\": \"2022-07-12T16:24:46.0546317Z\", \"endTime\": null, \"lastReadTime\": null } ], \"ClientSettings\": { \"maxBrowseReferencesToReturn\": 0, \"browseBlockSize\": 10, \"reconnectDelay\": \"0:00:30\", \"recreateSubscriptionDelay\": \"0:00:10\", \"readBlockSize\": 1000, \"sessionRequestTimeout\": \"0:02:00\", \"connectionTimeout\": \"0:00:30\", \"readTimeout\": \"0:00:30\", \"sessionAllowInsecureCredentials\": false, \"sessionMaxOperationsPerRequest\": 1000, \"browseTimeout\": \"0:01:00\", \"maxMonitoredItemsPerCall\": 1000, \"maxNotificationsPerPublish\": 0, \"publishingInterval\": \"0:00:01\", \"createMonitoredItemsTimeout\": \"0:00:30\", \"monitoredItemDataChangeTrigger\": \"StatusValue\", \"samplingInterval\": \"0:00:00.5\", \"monitoredItemQueueSize\": 2, \"maxInternalQueueSize\": 500000, \"historyReadBlockSize\": 10, \"historyReadTimeout\": \"0:01:00\" }, \"Discoveries\": [], \"HistoryRecoveries\": [] }, \"System\": { \"Logging\": { \"logLevel\": \"Information\", \"logFileSizeLimitBytes\": 34636833, \"logFileCountLimit\": 31 }, \"HealthEndpoints\": [], \"Components\": [ { \"componentId\": \"OpcUa1\", \"componentType\": \"OpcUa\" }, { \"componentId\": \"Modbus1\", \"componentType\": \"Modbus\" }, { \"componentId\": \"Storage\", \"componentType\": \"Storage\" } ], \"Buffering\": { \"bufferLocation\": \"C:/ProgramData/OSIsoft/EdgeDataStore/Buffers\", \"C: ProgramData OSIsoft EdgeDataStore Buffers\", \"maxBufferSizeMB\": 1024, \"enablePersistentBuffering\": true }, \"General\": { \"enableDiagnostics\": true, \"metadataLevel\": \"Medium\", \"healthPrefix\": null } } }"
                                                     },
    "content/configuration/general-configuration.html":  {
                                                             "href":  "content/configuration/general-configuration.html",
                                                             "title":  "General configuration",
                                                             "keywords":  "General configuration You can configure Edge Data Store\u0027s components to produce and store diagnostics data at a designated health endpoint and to send metadata for created streams. Edge Data Store\u0027s specific metadata is sent to Edge Data Store regardless of whether EnableDiagnostics is Enabled or its MetadataLevel. This only affects data sent to health endpoints. For more information about available diagnostics data, see Edge Data Store diagnostics . Configure general To update the general configuration: Using any text editor, create a JSON file that contains the storage runtime configuration. For the content structure, see Parameters . Save the JSON file with the name General..json . From the same directory where the file exists, run the following curl script: curl EdgeCmd curl -d \"{ \\\"EnableDiagnostics\\\":true, \\\"MetadataLevel\\\":Medium, \\\"HealthPrefix\\\":\\\"Machine1\\\" }\" -X PUT \"http://localhost:5590/api/v1/configuration/system/general\" \"http:  localhost:5590 api v1 configuration system general\" Alternatively, run a PUT command to the following endpoint, setting the parameters as needed: http://localhost:5590/api/v1/configuration/system/general http:  localhost:5590 api v1 configuration system general Note: 5590 is the default port number. If you selected a different port number, replace it with that value. edgecmd set general -EnableDiagnostics true -MetadataLevel Medium -HealthPrefix Machine1 General schema The full schema definition for the general configuration is in the System_General_schema.json file located in one of the following folders: Windows: %ProgramFiles%\\OSIsoft\\Adapters\\\u003cAdapterName\u003e\\Schemas Linux: /opt/OSIsoft/Adapters/\u003cAdapterName\u003e/Schemas  opt OSIsoft Adapters \u003cAdapterName\u003e Schemas Parameters The following table lists the parameters for general configuration. Parameter Required Type Description EnableDiagnostics Optional boolean Determines if diagnostics are enabled Allowed value: true or false Default value: true MetadataLevel Optional reference Defines amount of metadata sent to OMF endpoints. Allowed value: None , Low , Medium , and High Default value: Medium HealthPrefix Optional reference Prefix to use for health and diagnostics stream and asset IDs. Default value: null Example Retrieve the general configuration curl EdgeCmd curl -X GET \"http://localhost:{port}/api/v1/configuration/system/general\" \"http:  localhost:{port} api v1 configuration system general\" edgecmd get general Sample output: { \"EnableDiagnostics\": true, \"MetadataLevel\": \"Medium\", \"HealthPrefix\": \"Machine1\" } REST URLs Relative URL HTTP verb Action api/v1/configuration/system/General api v1 configuration system General GET Gets the general configuration api/v1/configuration/system/General api v1 configuration system General PUT Replaces the existing general configuration api/v1/configuration/system/General api v1 configuration system General PATCH Allows partial updating of general configuration"
                                                         },
    "content/configuration/storage.html":  {
                                               "href":  "content/configuration/storage.html",
                                               "title":  "Storage",
                                               "keywords":  "Storage The EDS storage component includes a storage component based on Sequential Data Store (SDS) technology, with a rich array of functionality available through REST APIs. Configure storage runtime characteristics, logging, and data egress to customize the storage component. For more information about SDS and SDS REST APIs, see SDS reference ."
                                           },
    "content/configuration/storage-runtime-config.html":  {
                                                              "href":  "content/configuration/storage-runtime-config.html",
                                                              "title":  "Storage runtime configuration",
                                                              "keywords":  "Storage runtime configuration The Edge Data Store storage component is installed with default configurations that are sufficient for most scenarios; however, you can configure the runtime characteristics of the storage component. Note: Consult with Technical Support before you modify the default configuration. To update the storage runtime configuration: Using any text editor, create a JSON file that contains the storage runtime configuration. For the content structure, see Parameters . Save the JSON file with the name Storage_Runtime..json . From the same directory where the file exists, run the following curl script or EdgeCmd: curl EdgeCmd curl -d \"@Storage_Runtime..json\" -H \"Content-Type: application/json\" application json\" -X PUT http://localhost:5590/api/v1/configuration/storage/Runtime http:  localhost:5590 api v1 configuration storage Runtime edgecmd set runtime -file Storage_Runtime..json Note: The @ symbol is a required prefix for this command. Parameters The following table lists all available runtime parameters for EDS storage configuration. Parameter Required Type Description IngressDebugExpiration Required string Sets the date and time when debugging should be disabled. If you specify a future date and time, incoming OMF messages are logged and the HTTP request and response content is stored to disk for review. The debug logging stops at the date and time specified. Set the value to null to disable logging. Every incoming OMF message logs a request and response log file. Log files are located in C:\\ProgramData\\OSIsoft\\EdgeDataStore\\Logs\\IngressDebugLogs\\ and grouped by the associated OMF message type. Each log file name is in the format: {ticks}-{operationId}-{Request/Response}.txt {ticks}-{operationId}-{Request Response}.txt . Valid formats are UTC: yyyy-mm-ddThh:mm:ssZ and Local: mm-dd-yyyy hh:mm:ss . When you activate logging, the content of an incoming OMF message, including the headers, is written to multiple files in the Logs directory. Those files are written to the IngressDebugLogs directory in the usual logs directory for every incoming OMF type, container, and data message. StreamStorageLimitMb Required integer The maximum size in megabytes that a stream can reach. When a stream exceeds the specified size, older data is deleted from the file until the stream is at or below the StreamStorageTargetMb value. The target value, set in the StreamStorageTargetMb property, needs to be smaller than the maximum specified in this property. Minimum value: 2 . Maximum value: 2147483647 . StreamStorageTargetMb Required integer The size in megabytes that a stream will be reduced to after StreamStorageLimitMb size is reached for a single stream. When a stream exceeds the size specified in the StreamStorageLimitMb property, older data is deleted from the file until the stream is at or below the StreamStorageTargetMb value. The target value needs to be smaller than the maximum specified in the StreamStorageLimitMb property. Minimum value: 1 . Maximum value: 2147483647 . TransactionLogLimitMB No integer Maximum size in megabytes for transaction log file. When a transaction log exceeds this size, it is deleted, which reduces the amount of data that you can recover if the host device loses power. Minimum value: 1 . Maximum value: 2147483647 . CheckpointRateInSec No integer Defines, in seconds, how often the storage component ensures recent data and configuration changes are flushed to storage. A setting of 0 disables checkpointing. Disabling checkpointing reduces the resiliency of the product, which can result in data loss if the host device loses power. Minimum value: 0 . Maximum value: 86400 . EnableMetrics No Boolean Enables EDS to create a new stream in the diagnostics namespace to track some metrics about internal storage operations. These metrics have no value outside of troubleshooting specific issues with the help of OSIsoft support. This should be set to false unless directed by OSIsoft support. Examples The following is a valid runtime configuration example. { \"streamStorageLimitMb\": 2, \"streamStorageTargetMb\": 1, \"ingressDebugExpiration\": \"0001-01-01T00:00:00\", \"checkpointRateInSec\": 30, \"transactionLogLimitMB\": 250, \"enableTransactionLog\": true, \"enableMetrics\": false }"
                                                          },
    "content/configuration/system-components-configuration.html":  {
                                                                       "href":  "content/configuration/system-components-configuration.html",
                                                                       "title":  "System components configuration",
                                                                       "keywords":  "System components configuration Edge Data Store components are Modbus TCP EDS adapter, OPC UA EDS adapter, and the Storage component. These components are only active if they are configured for the system to use them. EDS itself needs only a small amount of configuration - the list of components and the HTTP Port used for REST calls. The default System_Components.json file for the System component contains the following information: [ { \"ComponentId\": \"Storage\", \"ComponentType\": \"Storage\" } ] The Storage component is required for Edge Data Store to run and only one Storage component instance is supported. Each Modbus device needs a separate Modbus TCP EDS adapter component instance to connect to EDS, and each OPC UA device needs a separate OPC UA EDS adapter component instance to connect to EDS. Multiple Modbus TCP EDS adapter component instances and the OPC UA EDS adapter component instances are supported. Add system components To add system components: Using any text editor, create a JSON file with a ComponentId and ComponentType . The following example adds a Modbus TCP EDS adapter instance. { \"ComponentId\": \"Modbus1\", \"ComponentType\": \"Modbus\" } Note: The ComponentId must be a unique value. This example uses the ComponentId \"Modbus1,\" since it is the first Modbus TCP EDS adapter. Save the JSON file with the name AddComponent.json . From the same directory where the file exists, run the following curl script or EdgeCmd: curl EdgeCmd curl -d \"@AddComponent.json\" -H \"Content-Type: application/json\" application json\" http://localhost:5590/api/v1/configuration/system/components http:  localhost:5590 api v1 configuration system components edgecmd add components -type Modbus -id Modbus1 After the command completes successfully, the new component is available for configuration and use. Parameters for system components The following parameters are used to define system components. Parameters Required Type Nullable Description ComponentId Required string Yes The unique ID of the component instance. It can be any alphanumeric string, for example Storage. ComponentType Required string Yes The type of the component, for example Storage . There are three types of components: Storage identified by Storage , OPC UA EDS Adapter identified by OpcUa , and Modbus TCP EDS Adapter identified by Modbus . System components example [ { \"componentId\": \"OpcUa1\", \"componentType\": \"OpcUa\" }, { \"componentId\": \"Modbus1\", \"componentType\": \"Modbus\" }, { \"componentId\": \"Storage\", \"componentType\": \"Storage\" } ]"
                                                                   },
    "content/configuration/system-configuration.html":  {
                                                            "href":  "content/configuration/system-configuration.html",
                                                            "title":  "System configuration",
                                                            "keywords":  "System configuration Edge Data Store uses JSON configuration files in a protected directory on Windows and Linux to store configuration information that is read on startup. While the files are accessible to view, you use must REST or the EdgeCmd command line tool to make any changes to the files. As part of making Edge Data Store as secure as possible, any passwords or secrets are stored in encrypted form with cryptographic key material stored separately in a secure location. Editing the files directly may cause unexpected behavior. Note: Use REST to edit any single component or facet of the system or configure the system as a whole with a single REST call."
                                                        },
    "content/configuration/system-health-endpoints-configuration.html":  {
                                                                             "href":  "content/configuration/system-health-endpoints-configuration.html",
                                                                             "title":  "Configure health endpoints",
                                                                             "keywords":  "Configure health endpoints To monitor the status of Edge Data Store, configure health information egress for its components to an OMF endpoint capable of receiving health messages. Health data is transmitted at a one minute interval. Configure system health endpoints To configure system health endpoints: Using any text editor, create a JSON file containing system health endpoints. For content structure, see System health endpoints example . Update the parameters as needed. For a table of all available parameters, see Parameters . Save the file with the name System_HealthEndpoints..json . Use any tool capable of making HTTP requests to execute a POST command with the contents of that file to the following endpoint: http://localhost:5590/api/v1/configuration/System/HealthEndpoints http:  localhost:5590 api v1 configuration System HealthEndpoints , updating the port number if needed. Examples using curl and EdgeCmd (run this command from the same directory where the file is located): curl EdgeCmd curl -d \"@System_HealthEndpoints.json\" -H \"Content-Type: application/json\" application json\" http://localhost:5590/api/v1/configuration/System/HealthEndpoints http:  localhost:5590 api v1 configuration System HealthEndpoints edgecmd set HealthEndpoints -file System_HealthEndpoints.json Parameters The following parameters are available for configuring system health endpoints. Note: Parameters Buffering and MaxBufferSizeMb have been removed from the JSON. Configure buffering at Buffering configuration . Parameter Required Type Nullable Description ClientId Optional string Yes The ID of the client used for authentication to AVEVA Data Hub. ClientSecret Optional string Yes The client secret used for authentication to AVEVA Data Hub. Endpoint Required string Yes The URL of the ingress point which accepts OMF health messages. Id Optional string Yes The Id of the health endpoint configuration. The Id can be any alphanumeric string; for example, Endpoint1 . If you do not specify an Id , Edge Data Store generates one automatically. Password Optional string Yes The password used for authentication to PI Web API OMF endpoint UserName Optional string Yes The user name used for authentication to PI Web API OMF endpoint TokenEndpoint Optional string Yes Retrieves an AVEVA Data Hub token from an alternative endpoint. Allowed values are well-formed http or https endpoint string. Default value: null . ValidateEndpointCertificate Optional Boolean No The OSIsoft Adapter validates the endpoint certificate if set to true (recommended). If set to false, the OSIsoft Adapter accepts any endpoint certificate. OSIsoft strongly recommends using disabled endpoint certificate validation for testing purposes only. EnableDiagnostics Optional Boolean No Determines if diagnostics are enabled System health endpoints example [ { \"endpoint\": \"https://\u003cpi \"https:  \u003cpi web api server\u003e/piwebapi/omf/\", server\u003e piwebapi omf \", \"UserName\": \"\u003cusername\u003e\", \"Password\": \"\u003cpassword\u003e\", \"buffering\": \"none\", \"maxBufferSizeMB\": 0 }, { \"Endpoint\": \"https://\u003cAVEVA \"https:  \u003cAVEVA Data Hub OMF endpoint\u003e\", \"ClientId\": \"\u003cclientid\u003e\", \"ClientSecret\": \"\u003cclientsecret\u003e\", \"buffering\": \"disk\", \"maxBufferSizeMB\": 0 } ]"
                                                                         },
    "content/configuration/system-port-configuration.html":  {
                                                                 "href":  "content/configuration/system-port-configuration.html",
                                                                 "title":  "System port configuration",
                                                                 "keywords":  "System port configuration The appsettings.json file specifies the port on which Edge Data Store listens for REST API calls. EDS uses the same port for configuration and for writing data to OMF and SDS. The default port is 5590 . Valid ports are in the range of 1024-65535. During installation, a prompt allows you to specify a non-default port. Reconfigure system port Before changing the port, ensure that no other service or application on the device running EDS uses that port because only one application or service can use a port. After reconfiguring the port, you must restart EDS for the changes to take effect. To reconfigure the port: Open the appsettings.json file in a text editor. This file is located in C:\\Program Files\\OSIsoft\\EdgeDataStore\\ on Windows, in /opt/OSIsoft/EdgeDataStore/  opt OSIsoft EdgeDataStore  on Linux, or in the location specified during installation. Change the port value to a valid value between 1024 and 65535 and save the file. Restart the Edge Data Store service. Sample JSON file { \"ApplicationSettings\": { \"Port\": 5590, \"ApplicationDataDirectory\": \"EdgeDataStore\" } }"
                                                             },
    "content/data-ingress/eds-data-ingress.html":  {
                                                       "href":  "content/data-ingress/eds-data-ingress.html",
                                                       "title":  "Data ingress",
                                                       "keywords":  "Data ingress Edge Data Store supports the following protocols to ingress data: OPC UA EDS adapter : Use standard OPC UA equipment and protocols to send data to EDS. Modbus TCP EDS adapter : Use standard Modbus TCP equipment and protocols to send data to EDS. OMF Ingress: Use OSIsoft Message Format to send data from a custom application into EDS. OMF is a REST and JSON based data format designed for simplicity of custom application design. For more information, see OSIsoft Message Format . SDS Ingress: Use OSIsoft Sequential Data Store (SDS) REST to send data from a custom application into EDS. SDS offers the most options to send, store, and retrieve data from EDS. For more information about using REST API calls with SDS, see Write data API ."
                                                   },
    "content/diagnostics/diagnostics.html":  {
                                                 "href":  "content/diagnostics/diagnostics.html",
                                                 "title":  "Diagnostics configuration",
                                                 "keywords":  "Diagnostics configuration Edge Data Store and its components produce performance and system data for diagnostic purposes and store that data locally in the storage component. You can query it locally or egress it to PI Web API endpoints, the AVEVA Data Hub, or both. Diagnostic data is produced and saved by default, but can be disabled using the enableDiagnostics parameter in the System component. All diagnostics streams will have an ID format of {prefix}{DeviceName}.{StreamId} ; for example, APrefixLocalhost.Storage.default.default.Counts . EDS stores diagnostic data within the diagnostics namespace. Local access to this data is available through SDS. For more information, see Read data . Egress diagnostics data To egress diagnostics related data, specify diagnostics as the NamespaceId in EgressConfigurations . For details and instructions, see Data egress configuration . Edge Data Store diagnostics The Diagnostics.System dynamic type includes these values which are logged in a stream with the ID System.Diagnostics. This diagnostic stream contains system-level information related to the host platform that Edge Data Store is running on. Type Property Description string timestamp Timestamp of event int ProcessIdentifier Process id of the host process string StartTime When the host process started long WorkingSet Amount of physical memory, in bytes, allocated for the host process double TotalProcessorTime (uom=s) Total processor time for the host process expressed in seconds double TotalUserProcessorTime (uom=s) User processor time for the host process expressed in seconds double TotalPrivilegedProcessorTime (uom=s) Privileged processor time for the host process expressed in seconds int ThreadCount Number of threads in the host process int HandleCount Number of handles opened by the host process double ManagedMemorySize (uom=MB) Number of bytes currently thought to be allocated in managed memory double PrivateMemorySize (uom=MB) Amount of paged memory, in bytes, allocated for the host process double PeakPagedMemorySize (uom=MB) Maximum amount of memory in the virtual memory paging file, in bytes, used by the host process. Note: On Linux platforms, this value is always reported as 0. double StorageTotalSize (uom=MB) Total size of the storage medium in use by the Edge Data Store double StorageFreeSpace (uom=MB) Free space available EDS adapter diagnostics Each EDS adapter of the Edge Data Store produces its own diagnostics streams. Stream count The Diagnostics.StreamCountEvent dynamic type includes these values, which are logged in a stream with the id {DeviceName}.{componentid}.StreamCount . The stream count and type count include only types and streams created for sequential data received from a data source. Type Property Description string timestamp Timestamp of event int StreamCount Number of streams created by the adapter instance int TypeCount Number of types created by the adapter instance IO rate The Diagnostics.Adapter.IORate dynamic type includes these values, which are logged in a stream with the id {DeviceName}.{componentid}.IORate . IO rate includes only sequential data collected from a data source. Type Property Description string timestamp Timestamp of event double IORate 10-minute rolling average of data rate (streams/second) (streams second) Error rate The Diagnostics.Adapter.ErrorRate dynamic type includes these values, and are logged in a stream with the Id {DeviceName}.{componentid}.ErrorRate . Type Property Description string timestamp Timestamp of event double ErrorRate 10-minute rolling average of error rate (streams/second) (streams second) Edge Storage diagnostics The Storage component of Edge Data Store produces the following diagnostics streams. Storage.default.default.Counts The Storage.default.default.Counts stream includes counts of the types, streams, and stream views of the default namespace. Type Property Description string timestamp Timestamp of event integer TypeCount Count of types integer StreamCount Count of streams integer StreamViewCount Count of stream views Storage.default.diagnostics.Counts The Storage.default.diagnostics.Counts stream includes counts of the types, streams, and stream views of the diagnostics namespace. Type Property Description string timestamp Timestamp of event integer TypeCount Count of types integer StreamCount Count of streams integer StreamViewCount Count of stream views Storage.Total.Counts The Storage.Totals.Counts stream includes counts of the types, streams, and stream views of all namespaces of the storage component. Type Property Description string timestamp Timestamp of event integer TypeCount Count of types integer StreamCount Count of streams integer StreamViewCount Count of stream views IO rate The Diagnostics.Storage.IORate dynamic type includes these values, which are logged in a stream with the id {DeviceName}.Storage.{EgressType}.{EgressID}.IORate , where {EgressType} can be Periodic or Manual . IO rate includes only sequential data successfully sent to an endpoint for the referenced egress configuration. Type Property Description string timestamp Timestamp of event double IORate 10-minute rolling average of data rate (streams/second) (streams second)"
                                             },
    "content/docker/edge-docker.html":  {
                                            "href":  "content/docker/edge-docker.html",
                                            "title":  "Install Edge Data Store with Docker",
                                            "keywords":  "Install Edge Data Store with Docker Docker is a set of tools you can use on Linux to manage application deployments. This topic provides examples of how to create a Docker container with Edge Data Store (EDS). Note: You should only use Docker to install EDS if your environment requires it and you are proficient with Docker. Docker is not required to use EDS. Create a startup script To create a startup script for EDS: Use a text editor to create a script similar to one of the following examples: Note: The script varies slightly by processor. ARM32 #!/bin/sh #! bin sh if [ -z $portnum ] ; then exec /EdgeDataStore_linux-arm/OSIsoft.Data.System.Host  EdgeDataStore_linux-arm OSIsoft.Data.System.Host else exec /EdgeDataStore_linux-arm/OSIsoft.Data.System.Host  EdgeDataStore_linux-arm OSIsoft.Data.System.Host --port:$portnum fi ARM64 #!/bin/sh #! bin sh if [ -z $portnum ] ; then exec /EdgeDataStore_linux-arm64/OSIsoft.Data.System.Host  EdgeDataStore_linux-arm64 OSIsoft.Data.System.Host else exec /EdgeDataStore_linux-arm64/OSIsoft.Data.System.Host  EdgeDataStore_linux-arm64 OSIsoft.Data.System.Host --port:$portnum fi AMD64 #!/bin/sh #! bin sh if [ -z $portnum ] ; then exec /EdgeDataStore_linux-x64/OSIsoft.Data.System.Host  EdgeDataStore_linux-x64 OSIsoft.Data.System.Host else exec /EdgeDataStore_linux-x64/OSIsoft.Data.System.Host  EdgeDataStore_linux-x64 OSIsoft.Data.System.Host --port:$portnum fi Name the script edsdockerstart.sh and save it to the directory where you plan to create the container. Create a Docker container To create a Docker container that runs EDS: Create the following Dockerfile in the directory where you want to create and run the container. Note: Dockerfile is the required name for the file. Use appropriate variation for your operating system. ARM32 FROM ubuntu:20.04 WORKDIR /   RUN apt-get update \u0026\u0026 DEBIAN_FRONTEND=noninteractive apt-get install -y ca-certificates libicu66 libssl1.1 curl COPY edsdockerstart.sh /   RUN chmod +x /edsdockerstart.sh  edsdockerstart.sh ADD ./EdgeDataStore_linux-arm.tar.gz . EdgeDataStore_linux-arm.tar.gz . ENTRYPOINT [\"/edsdockerstart.sh\"] [\" edsdockerstart.sh\"] ARM64 FROM ubuntu:20.04 WORKDIR /   RUN apt-get update \u0026\u0026 DEBIAN_FRONTEND=noninteractive apt-get install -y ca-certificates libicu66 libssl1.1 curl COPY edsdockerstart.sh /   RUN chmod +x /edsdockerstart.sh  edsdockerstart.sh ADD ./EdgeDataStore_linux-arm64.tar.gz . EdgeDataStore_linux-arm64.tar.gz . ENTRYPOINT [\"/edsdockerstart.sh\"] [\" edsdockerstart.sh\"] AMD64 (x64) FROM ubuntu:20.04 WORKDIR /   RUN apt-get update \u0026\u0026 DEBIAN_FRONTEND=noninteractive apt-get install -y ca-certificates libicu66 libssl1.1 curl COPY edsdockerstart.sh /   RUN chmod +x /edsdockerstart.sh  edsdockerstart.sh ADD ./EdgeDataStore_linux-x64.tar.gz . EdgeDataStore_linux-x64.tar.gz . ENTRYPOINT [\"/edsdockerstart.sh\"] [\" edsdockerstart.sh\"] Copy the EdgeDataStore_linux-platform.tar.gz file into the same directory as the Dockerfile . Copy the edsdockerstart.sh script into the same directory as the Dockerfile . Run the following command line in the same directory: Note: You may need to include the sudo command. docker build -t edgedatastore . Docker container startup The following procedures contain instructions on how to run EDS inside a Docker container with different options enabled. Note: Before running the Docker container, determine whether to store the data in the container or in a host directory. Run the Docker container with REST access enabled To run EDS inside a Docker container with access to its REST API from the local host, complete the following: Use the Docker container image edgedatastore you previously created. Type the following in the command line: Note: You may need to include the sudo command. docker run -d --network host edgedatastore Port 5590 is accessible from the host and you can make REST calls to EDS from applications on the local host computer. In this example, all data retained by EDS is stored in the container itself. When you delete the container, the stored data is also deleted. Run the Docker container with persistent storage To run EDS inside a Docker container while using the host for persistent storage, complete the following steps. Note: This procedure also enables access to EDS REST API from the local host. Use the docker container image edgedatastore you previously created. Type the following in the command line: Note: You may need to include the sudo command. docker run -d --network host -v /edgeds:/usr/share/OSIsoft/  edgeds: usr share OSIsoft  edgedatastore Port 5590 is accessible from the host and you can make REST calls to EDS from applications on the local host computer. In this example, all data written to the container is written to the host directory instead and the host directory is a directory on the local machine, \u003c!-- customize --\u003e /edgeds  edgeds . You can specify any directory. Change port number To use a port other than 5590 , you can specify a portnum variable on the docker run command line. For example, to start EDS using port 6000 instead of 5590 , use the following command: docker run -d -e portnum=6000 --network host edgedatastore This command accesses the REST API with port 6000 instead of port 5590 . The following curl command returns the configuration for the container. curl http://localhost:6000/api/v1/configuration http:  localhost:6000 api v1 configuration Remove REST access If you remove the --network host option from the Docker run command, REST access is not possible from outside of the container. This may be useful if you want to host an application in the same container as EDS without external REST access enabled. Upgrade To upgrade a Docker container with persistent storage to the latest version of EDS, you should follow the process above for creating a new container image. Then, when you run the container, use the same persistent storage that you previously used. This allows you to carry over all of the configuration data to the upgraded container. Note: If you previously used the REST API to change the port number the container listened on, you will need to follow the Change port number section to reenable listening on the specified port."
                                        },
    "content/edge-data-store.html":  {
                                         "href":  "content/edge-data-store.html",
                                         "title":  "Edge Data Store",
                                         "keywords":  "Edge Data Store Edge Data Store (EDS) is a lightweight data collection and storage application designed to capture data at the edge of networks for historical storage and analysis. It runs on small, rugged devices or embedded in existing industrial hardware and is designed to be resilient and require minimal installation and administration. While not a replacement for a PI System or AVEVA Data Hub, EDS augments the PI System and AVEVA Data Hub by collecting and storing data in situations where deploying a full system is impractical. The following diagram shows conceptually how EDS captures data and sends to permanent storage: EDS collects data using any of the following methods: Built-in OPC UA connectivity Built-in Modbus TCP connectivity Custom application using OSIsoft Message Format (OMF) Custom application using REST API Once collected, EDS stores the data locally in configurable data storage until it can be sent to permanent storage in a PI System or in AVEVA Data Hub through periodic egress. The data can also be read from local storage by custom applications that use REST APIs. Edge Data Store architecture EDS runs on both Linux and Windows platforms and is comprised of separate components that each perform a specific function within EDS. The following diagram shows Edge Data Store architecture with all of its components and how the data flow through those components: EDS components are shown in grey within the Edge Data Store in the diagram: Modbus TCP EDS adapter ??? Collects data from Modbus TCP devices and writes it to data storage OPC UA EDS adapter ??? Collects data from OPC UA devices and writes it to data storage Data Storage ??? Stores data locally until it can be egressed Data egress ??? Sends data from storage to PI Server or AVEVA Data Hub Note: EDS also supports OSIsoft Cloud Services as an egress destination. Health ??? Records health information of components and sends it to PI Server or AVEVA Data Hub Blue boxes in the diagram show ways to interact with EDS from the local device: OMF REST ??? Use OSIsoft Message Format to write data to the data storage component programmatically SDS REST APIs ??? Use SDS REST APIs to read data from and write data to the data storage component programmatically Configuration ??? Use REST or the EdgeCmd tool to configure EDS as a whole or each component individually and to view the current configuration EDS requires an endpoint to connect to REST APIs on the local device, which is shown outlined in blue in the diagram. By default, the endpoint uses port 5590 ; however, you can configure it to use another port. Orange arrows show data flowing into EDS and blue arrows show data flowing out of EDS. For detailed information about configuring each component of EDS, see Configuration ."
                                     },
    "content/egress/configure-data-egress.html":  {
                                                      "href":  "content/egress/configure-data-egress.html",
                                                      "title":  "Configure periodic data egress",
                                                      "keywords":  "Configure periodic data egress Periodic data egress is a recurring task that sends the timeseries data collected by EDS to long term storage in either AVEVA Data Hub or PI Server. You can create multiple egress destinations and multiple periodic egress tasks. Periodic egress runs on a regular schedule to ensure that data is sent to long term storage. Once the AVEVA Data Hub or PI Server destinations are prepared to receive OMF messages, configure data egress to create the connection to the destination and specify the details of the data egress, including the data to include and the frequency to send it. For more information on egress destinations, see Prepare egress destinations . To support the reuse of common configuration blocks, EDS egress configuration is divided into four facets, which can be configured together or separately: EgressEndpoints - Describes the egress endpoint connectivity information Schedules - Describes the timing of data egress DataSelectors - Describes which data to egress and includes stream and data filtering EgressConfigurations - Ties together the three previous facets and includes settings for type and stream prefixing, backfill, and more Warnings: You cannot add configurations manually because some parameters are stored to disk encrypted. You must use the REST endpoints to add/edit add edit configurations. For additional endpoints, see REST URLs . If you delete or remove an egress configuration and then recreate it with Backfill set to true , duplicate data will appear on any stream that was previously egressed successfully. New streams will not see duplicate data. Create egress configuration To configure EDS for data egress: Create a JSON file. For content structure, see the following Examples . Update the parameters as needed. For a table of all available parameters, see Parameters . Save the JSON file to any directory on the device where Edge Data Store is installed. Use any tool capable of making HTTP requests to send the contents of the JSON file to the appropriate configuration endpoints: Use Case Filename Command Endpoint Configures multiple egress facets StorageEgress.json PUT http://localhost:5590/api/v1/configuration http:  localhost:5590 api v1 configuration Creates EgressEndpoints only EgressEndpoints.json POST http://localhost:5590/api/v1/configuration/storage/egressendpoints http:  localhost:5590 api v1 configuration storage egressendpoints Creates Schedules only Schedules.json POST http://localhost:5590/api/v1/configuration/storage/schedules http:  localhost:5590 api v1 configuration storage schedules Creates DataSelectors only DataSelectors.json POST http://localhost:5590/api/v1/configuration/storage/dataselectors http:  localhost:5590 api v1 configuration storage dataselectors Creates EgressConfigurations only EgressConfigurations.json POST http://localhost:5590/api/v1/configuration/storage/egressconfigurations http:  localhost:5590 api v1 configuration storage egressconfigurations Examples using cURL and EdgeCmd, which must be run from the directory where the JSON file is saved: curl EdgeCmd curl -d \"@{Filename}\" -H \"Content-Type: application/json\" application json\" -X {Command} {Endpoint} EdgeCmd {Command} {Endpoint} -file {Filename} Note: The @ symbol is a required prefix for this command. {Filename} , {Command} and {Endpoint} should be replaced by the corresponding filename, command, and endpoint. To configure multiple egress facets together: curl EdgeCmd curl -d \"@StorageEgress.json\" -H \"Content-Type: application/json\" application json\" -X PUT http://localhost:5590/api/v1/configuration http:  localhost:5590 api v1 configuration edgecmd set application -file StorageEgress.json To configure the Schedules facet: curl EdgeCmd curl -d \"@Schedules.json\" -H \"Content-Type: application/json\" application json\" -X POST http://localhost:5590/api/v1/configuration/storage/schedules http:  localhost:5590 api v1 configuration storage schedules edgecmd set schedules -file Schedules.json Parameters The following table lists egress parameters for EgressEndpoints . Parameter Required Type Description Id Required string Unique identifier of the endpoint configuration Endpoint Required string Destination that accepts OMF v1.2 and older messages. Supported destinations include AVEVA Data Hub and PI Server. Username Required for PI Server endpoint string User name used for authentication to PI Web API OMF endpoint. If domain is required, the backslash must be escaped (for example, domain \\\\ username ). Password Required for PI Server endpoint string Password used for authentication to PI Web API OMF endpoint ClientId Required for AVEVA Data Hub endpoint string Client ID used for authentication to AVEVA Data Hub OMF endpoint ClientSecret Required for AVEVA Data Hub endpoint string Client Secret used for authentication with the AVEVA Data Hub OMF endpoint DebugExpiration Optional string Enables logging of detailed information for each outbound HTTP request pertaining to this egress endpoint to disk. The value represents the date and time this logging will stop. Examples of valid strings representing date and time: UTC: yyyy-mm-ddThh:mm:ssZ , Local: yyyy-mm-ddThh:mm:ss . For more information, see Troubleshoot Edge Data Store . TokenEndpoint Optional for AVEVA Data Hub endpoint string Used to retrieve an AVEVA Data Hub token from an alternative endpoint. This is not normally necessary with AVEVA Data Hub. Only use if directed to do so by customer support. ValidateEndpointCertificate Optional boolean Validate endpoint certificate (recommended). If false , egress accepts any endpoint certificate. Use for testing only with self-signed certificates. Defaults to true . The following table lists egress parameters for Schedules . Parameter Required Type Description Id Required string Unique identifier of the schedule configuration Period Required string Frequency of time between each egress action beginning at or after the StartTime . Must be a string in the following format d.hh:mm:ss.## . See StartTime for additional information. StartTime Optional string The date and time when egress actions will begin. Valid formats are: UTC: yyyy-mm-ddThh:mm:ssZ and Local: yyyy-mm-ddThh:mm:ss . Use the StartTime parameter if you want data egress to begin at or after a specific time instead of beginning immediately. If you do not specify a StartTime , egress begins as soon as you submit the configuration and will occur again whenever the length of the Period completes. For example, a Period of 00:15:00 without a defined StartTime results in immediate data egress when you submit the configuration and then every 15 minutes thereafter. Conversely, if you use a StartTime of 2022-10-02T06:00:00 , a Period of 00:15:00 , and you submit your configuration at 6:07 on October 2, 2022, egress will begin at 6:15 and will continue every 15 minutes thereafter. The following table lists egress parameters for DataSelectors . Parameter Required Type Description Id Required string Unique identifier of the data selector configuration StreamFilter Optional string A filter used to determine which streams and types are egressed. For more information on valid filters, see Search in SDS . AbsoluteDeadband Optional string Specifies the absolute change in data value that will cause the current value to pass the filter test. At least one of AbsoluteDeadband or PercentChange must be specified. PercentChange Optional string Specifies the percent change from previous value that will cause the current value to pass the filter test. At least one of AbsoluteDeadband or PercentChange must be specified. ExpirationPeriod Optional string The length in time that can elapse after an event before automatically storing the next event. The expected format is HH:MM:SS.### . The following table lists egress parameters for EgressConfigurations . Parameter Required Type Description Id Optional string Unique identifier of the configuration Name Optional string Friendly name Description Optional string Friendly description Enabled Optional boolean An indicator of whether egress is enabled when the egress endpoint is loaded. Defaults to true . EndpointId Required string Id of the endpoint selected for egress ScheduleId Required string Id of the schedule selected for egress DataSelectorIds Optional array Ids of the data selectors for egress NamespaceId Optional string Represents the namespace that will be egressed. There are two available namespaces: default and diagnostics . The default namespace is default . Backfill Optional boolean Indicates whether data will be backfilled. Data backfill occurs when you run the egress endpoint for the first time after application startup. This results in all data from the earliest to the latest stored index being egressed. Set to true to backfill data. Defaults to false . StreamPrefix Optional string Prefix applied to any streams that are egressed. A null string or a string containing only empty spaces will be ignored. The following restricted characters are not allowed: /   : ? # [ ] @ ! $ \u0026 \u0027 ( ) \\ * + , ; = % | \u003c \u003e { } ` \" TypePrefix Optional string Prefix applied to any types that are egressed. A null string or a string containing only empty spaces will be ignored. The following restricted characters are not allowed: /   : ? # [ ] @ ! $ \u0026 \u0027 ( ) \\ * + , ; = % | \u003c \u003e { } ` \" Examples The following are valid configuration examples for egress. Configure multiple egress facets in a single request PUT http://localhost:5590/api/v1/configuration http:  localhost:5590 api v1 configuration Create configuration for egress of all data for all streams to AVEVA Data Hub every 15 seconds. { \"Storage\": { \"EgressEndpoints\": [ { \"Id\": \"Endpoint-AVEVA Data Hub\", \"Endpoint\": \"https://{OcsLocation}/api/Tenants/{tenantId}/Namespaces/{namespaceId}/omf\", \"https:  {OcsLocation} api Tenants {tenantId} Namespaces {namespaceId} omf\", \"ClientId\": \"{clientId}\", \"ClientSecret\": \"{clientSecret}\" } ], \"Schedules\": [ { \"Id\": \"Schedule-15sec\", \"Period\": \"00:00:15\" } ], \"EgressConfigurations\": [ { \"Id\": \"AVEVA Data Hub\", \"EndpointId\": \"Endpoint-AVEVA Data Hub\", \"ScheduleId\": \"Schedule-15sec\" } ] } } Create configuration for egress of some data for some streams, to AVEVA Data Hub and PI Server, every 2 days starting January 1st, 2022 at 9:00. EgressEndpoints/Schedules/DataSelectors EgressEndpoints Schedules DataSelectors definitions are shared. { \"Storage\": { \"EgressEndpoints\": [ { \"Id\": \"Endpoint-AVEVA Data Hub\", \"Endpoint\": \"https://{OcsLocation}/api/Tenants/{tenantId}/Namespaces/{namespaceId}/omf\", \"https:  {OcsLocation} api Tenants {tenantId} Namespaces {namespaceId} omf\", \"ClientId\": \"{clientId}\", \"ClientSecret\": \"{clientSecret}\" }, { \"Id\": \"Endpoint-PI\", \"Endpoint\": \"https://{webApiLocation}/piwebapi/omf/\", \"https:  {webApiLocation} piwebapi omf \", \"Username\": \"{username}\", \"Password\": \"{password}\" } ], \"Schedules\": [ { \"Id\": \"Schedule1\", \"StartTime\": \"2022-01-01T09:00:00\", \"Period\": \"2.00:00:00\" } ], \"DataSelectors\": [ { \"Id\": \"DataSelector1\", \"StreamFilter\": \"TypeId:TestType1\", \"AbsoluteDeadband\": 50, \"ExpirationPeriod\": \"12:00:00\" }, { \"Id\": \"DataSelector2\", \"StreamFilter\": \"TypeId:TestType1\", \"PercentChange\": 80 } ], \"EgressConfigurations\": [ { \"Id\": \"AVEVA Data Hub\", \"EndpointId\": \"Endpoint-AVEVA Data Hub\", \"ScheduleId\": \"Schedule1\", \"DataSelectorIds\": [\"DataSelector1\"] }, { \"Id\": \"PI\", \"EndpointId\": \"Endpoint-PI\", \"ScheduleId\": \"Schedule1\", \"DataSelectorIds\": [\"DataSelector1\", \"DataSelector2\"] } ] } } Create EgressEndpoints only POST http://localhost:5590/api/v1/configuration/storage/egressendpoints http:  localhost:5590 api v1 configuration storage egressendpoints Add a single new egress endpoint to PI. { \"Id\": \"Endpoint-PI\", \"Endpoint\": \"https://{webApiLocation}/piwebapi/omf/\", \"https:  {webApiLocation} piwebapi omf \", \"Username\": \"{username}\", \"Password\": \"{password}\" } Add multiple new egress endpoints to AVEVA Data Hub and PI with the use of a domain in the username. All properties are explicitly listed. [ { \"Id\": \"Endpoint-AVEVA Data Hub\", \"Endpoint\": \"https://{OcsLocation}/api/Tenants/{tenantId}/Namespaces/{namespaceId}/omf\", \"https:  {OcsLocation} api Tenants {tenantId} Namespaces {namespaceId} omf\", \"ClientId\": \"{clientId}\", \"ClientSecret\": \"{clientSecret}\", \"Username\": null, \"Password\": null, \"TokenEndpoint\": null, \"ValidateEndpointCertificate\": true, \"DebugExpiration\": null }, { \"Id\": \"Endpoint-PI-WithDomain\", \"Endpoint\": \"https://{webApiLocation}/piwebapi/omf/\", \"https:  {webApiLocation} piwebapi omf \", \"ClientId\": null, \"ClientSecret\": null, \"Username\": \"{domain}\\\\{username}\", \"Password\": \"{password}\", \"TokenEndpoint\": null, \"ValidateEndpointCertificate\": true, \"DebugExpiration\": null } ] Create Schedules only POST http://localhost:5590/api/v1/configuration/storage/schedules http:  localhost:5590 api v1 configuration storage schedules Add a single new schedule for egress every 15 seconds. { \"Id\": \"Schedule-15sec\", \"Period\": \"00:00:15\" } Add multiple new schedules for egress every minute and every hour starting January 1st, 2022 at 9:00. All properties are explicitly listed. [ { \"Id\": \"Schedule-1min\", \"StartTime\": null, \"Period\": \"00:01:00\" }, { \"Id\": \"Schedule-1hr\", \"StartTime\": \"2022-01-01T09:00:00\", \"Period\": \"01:00:00\" } ] Create DataSelectors only POST http://localhost:5590/api/v1/configuration/storage/dataselectors http:  localhost:5590 api v1 configuration storage dataselectors Add single new data selector for egress of data filtered by percent change of 10 for streams whose Id contains \"Modbus\" or \"Opc\". { \"Id\": \"DataFilterByPercentChange-StreamFilterById\", \"StreamFilter\": \"Id:*Modbus* OR Id:*Opc*\", \"PercentChange\": 10 } Add a single new data selector for data egress filtered by an absolute deadband of 5 and expiration period of 10 minutes for all streams. All properties are explicitly listed. { \"Id\": \"DataFilterByAbsoluteDeadbandWithExpiration\", \"StreamFilter\": null, \"AbsoluteDeadband\": 5, \"PercentChange\": null, \"ExpirationPeriod\": \"00:10:00\" } Add multiple new data selectors for egress of all data for streams with a specific TypeId value and streams with a field that begins with Unique . [ { \"Id\": \"StreamFilterByTypeId\", \"StreamFilter\": \"TypeId:myType\", \"AbsoluteDeadband\": 0 }, { \"Id\": \"StreamFilterByFieldName\", \"StreamFilter\": \"Unique*\", \"PercentChange\": 0 } ] Create EgressConfigurations only POST http://localhost:5590/api/v1/configuration/storage/egressconfigurations http:  localhost:5590 api v1 configuration storage egressconfigurations Add a single new configuration for egress of all data for all streams to AVEVA Data Hub every 15 seconds. { \"Id\": \"AVEVA Data Hub\", \"EndpointId\": \"Endpoint-AVEVA Data Hub\", \"ScheduleId\": \"Schedule-15sec\" } Add a single new configuration for egress of all data for all streams to PI every 15 seconds and include both the type and stream prefix. All properties are explicitly listed. { \"Id\": \"PI\", \"Name\": null, \"Description\": null, \"Enabled\": true, \"EndpointId\": \"Endpoint-PI\", \"ScheduleId\": \"Schedule-15sec\", \"DataSelectorIds\": null, \"NamespaceId\": \"default\" \"Backfill\": false, \"StreamPrefix\": \"1ValidPrefix.\", \"TypePrefix\": \"AlsoValid_\", } Add a single new configuration for egress of all data for streams with a specific TypeId value, to AVEVA Data Hub every 15 seconds. { \"EndpointId\": \"Endpoint-AVEVA Data Hub\", \"ScheduleId\": \"Schedule-15sec\", \"DataSelectorIds\": [\"StreamFilterByTypeId\"] } Add a single new configuration for egress of all data for streams containing a field that begins with Unique but with data filtered by percent change of 10 for streams whose Id contains Modbus or Opc to PI Server every minute. { \"EndpointId\": \"Endpoint-PI\", \"ScheduleId\": \"Schedule-1min\", \"DataSelectorIds\": [\"StreamFilterByFieldName\", \"DataFilterByPercentChange-StreamFilterById\"] } Add multiple new configurations for egress with backfill, diagnostics data, or PI Server endpoint with domain in username. EgressEndpoints , Schedules , and DataSelectors definitions are shared. [ { \"EndpointId\": \"Endpoint-AVEVA Data Hub\", \"ScheduleId\": \"Schedule-15sec\", \"Backfill\": true }, { \"EndpointId\": \"Endpoint-AVEVA Data Hub\", \"ScheduleId\": \"Schedule-1hr\", \"DataSelectorIds\": [\"DataFilterByAbsoluteDeadbandWithExpiration\"], \"NamespaceId\": \"diagnostics\" }, { \"EndpointId\": \"Endpoint-PI-WithDomain\", \"ScheduleId\": \"Schedule-1hr\", \"DataSelectorIds\": [\"StreamFilterByTypeId\", \"DataFilterByAbsoluteDeadbandWithExpiration\"] } ] REST URLs The following table shows examples of REST URLS. The {egressFacet} parameter can be any of the four egress facets: EgressEndpoints , Schedules , DataSelectors , or EgressConfigurations . Relative URL HTTP verb Action api/v1/configuration/storage/{egressFacet} api v1 configuration storage {egressFacet} GET Gets all configured objects of the egressFacet . api/v1/configuration/storage/{egressFacet} api v1 configuration storage {egressFacet} DELETE Deletes all configured objects of the egressFacet . api/v1/configuration/storage/{egressFacet} api v1 configuration storage {egressFacet} POST Adds an array of objects to the egressFacet , fails if any object already exists. api/v1/configuration/storage/{egressFacet} api v1 configuration storage {egressFacet} POST Adds a single object to the egressFacet , fails if the object already exists. api/v1/configuration/storage/{egressFacet} api v1 configuration storage {egressFacet} PUT Replaces all objects in the egressFacet . api/v1/configuration/storage/{egressFacet}/{id} api v1 configuration storage {egressFacet} {id} GET Gets the configured object with id in the egressFacet . api/v1/configuration/storage/{egressFacet}/{id} api v1 configuration storage {egressFacet} {id} DELETE Deletes the configured object with id in the egressFacet . api/v1/configuration/storage/{egressFacet}/{id} api v1 configuration storage {egressFacet} {id} PUT Replaces the object with id in the egressFacet , fails if the object does not exist. api/v1/configuration/storage/{egressFacet}/{id} api v1 configuration storage {egressFacet} {id} PATCH Allows partial updating of the configured object with id in the egressFacet ."
                                                  },
    "content/egress/egress.html":  {
                                       "href":  "content/egress/egress.html",
                                       "title":  "Data egress",
                                       "keywords":  "Data egress Edge Data Store provides an egress mechanism to transfer data through OMF to AVEVA Data Hub or a PI Server. For each egress destination, you need to prepare the destination and configure an endpoint. Prepare egress destinations to ensure that AVEVA Data Hub or PI Server are properly configured to receive OMF messages and record information needed to create a connection to the destination. Configure an egress endpoint to specify the connection information for a destination and the details of the data transfer. Each endpoint is independent of all other egress endpoints, and more than one endpoint for the same destination is allowed. Note: Only streams with a single, timeseries-based index can be egressed. For more information, see Egress execution details . EDS allows the following types of egress: Periodic egress - Once configured, periodic egress runs at regularly scheduled intervals. Manual egress - Manual egress is configured and run on an as needed basis. Periodic egress ensures that all the data EDS collects is sent to permanent storage on a regular basis. Manual egress gives you the flexibility to retrieve data whenever you need it. For example, say you have EDS configured to collect data on a wind turbine. You configure periodic egress to send data to AVEVA Data Hub every Friday at noon. A strong storm passes through the area of the wind turbine over the weekend, and you do not want to wait until Friday afternoon to review what occurred with the turbine during the storm. You send a manual egress request to EDS to send the weekend data to AVEVA Data Hub, so that you can start analyzing the data today. Note: OSIsoft Cloud Services is also supported as an egress destination. The setup process is similar to AVEVA Data Hub."
                                   },
    "content/egress/egress-execution-details.html":  {
                                                         "href":  "content/egress/egress-execution-details.html",
                                                         "title":  "Egress execution details",
                                                         "keywords":  "Egress execution details After you configure a valid periodic or manual egress, selected data will be transferred to the referenced endpoint on the defined schedule, independently of other egress instances. Note: EDS can only egress streams with a single timeseries-based index. EDS uses OMF messages to egress data, and knowing how those messages are constructed can help with understanding how data is egressed. OMF defines three types of messages: types, containers, and data. Types and containers define the data being egressed and data is the actual timeseries data. For EDS, types and containers are sent only on the first egress for an endpoint; subsequently, only new or changed types and containers are egressed. Types are created first; then containers are created based on types; and then, data is egressed to containers. If type creation fails with a HttpStatusCode Conflict (403) , EDS will still try to egress the related containers. Container creation must be successful for data to be egressed. Type, container, and data items are batched into one or more OMF messages for egress. Per the requirements defined in the OMF specification, a single message cannot exceed 192KB in size. Compression is automatically applied to outbound egress messages. On the destination, failure to add a single item results in the message failing. In that case, Edge Data Store egresses each item individually, per type or stream (that is each type, each stream, all collected data for a single stream). Types, containers, and data will continue to be egressed as long as the destination continues to respond to HTTP requests - retrying previous failures as needed. If periodic egress fails due to HTTP exceptions, EDS will retry sending the request with a default backoff. If EDS receives a Retry-After header in the response from the endpoint, then the backoff is adjusted accordingly. If the message still fails to send after retries, EDS will either move onto the next message in the queue or wait for 5 minutes before retrying process again. If manual egress fails due to HTTP exceptions, EDS will stop the egress instance and start the next manual egress in the queue. For data collection and egress, in-memory and on-disk storage are used to track the last successfully-egressed data event per stream. EDS egresses data in the order it is collected and egress configurations can include future data. Note: When EDS successfully egresses an event with a future timestamp, only values after the associated timestamp of that event are egressed."
                                                     },
    "content/egress/manual-egress.html":  {
                                              "href":  "content/egress/manual-egress.html",
                                              "title":  "Configure manual data egress",
                                              "keywords":  "Configure manual data egress Manual data egress is a task that sends the timeseries data collected by EDS to long term storage in either AVEVA Data Hub or PI Server. You can create multiple manual egress tasks. Once you prepare the AVEVA Data Hub or PI Server destinations to receive OMF messages and configure EgressEndpoints for the chosen destination, you can send manual egress requests as needed. For example, you may need to backfill data or want to review data for an event as soon as possible. For more information on egress destinations, see Prepare egress destinations . Make requests in JSON using parameters, similar to periodic egress, to specify which data to egress and when the egress should happen. You can either save the parameters in a file to send them or send the request directly. In addition to creating manual egress requests, you can cancel, resume, and delete these requests. For a list of other REST operations you can perform, see REST URLs . Note: The maximum number of manual egress jobs is 50, including completed jobs. If the maximum number is reached, new manual egress requests are rejected. Use the delete REST operation to remove egress jobs that you no longer need. Send manual data egress request To send a manual data egress request: Create a JSON file. For content structure, see the following Example manual egress request . Note: If you prefer, you can send the JSON request directly without saving the parameters to a file. Update the parameters as needed. For descriptions of all available parameters, see Parameters . Save the JSON file to any directory on the device where Edge Data Store is installed. For example, as ManualEgress.json . Use any tool capable of making HTTP requests to send the contents of the JSON request to the following configuration endpoint using POST : http://localhost:5590/api/v1/configuration/storage/manualegresses http:  localhost:5590 api v1 configuration storage manualegresses Example using cURL, which must be run from the directory where the JSON file is saved: curl -d \"@ManualEgress.json\" -H \"Content-Type: application/json\" application json\" -X POST http://localhost:5590/api/v1/configuration/storage/manualegresses http:  localhost:5590 api v1 configuration storage manualegresses Parameters The following table lists the parameters for manual egress. Parameter Required Type Description Id Optional string Unique identifier of the request. EndpointId Required string Unique identifier of the endpoint destination. Note: You must configure the endpoint before sending manual egress requests. Period Optional string The frequency of time between each egress action after the initial egress. Must be a string in the format d.hh:mm:ss.## . See ScheduledTime for additional information. If you do not set the Period , the default value is 01:00:00 , which is 1 hour. If the entire range of data specified by the StartIndex and EndIndex is in the past, the Period is not used. ScheduledTime Optional string The date and time when the egress request will begin. Valid formats are: UTC: yyyy-mm-ddThh:mm:ssZ and Local: yyyy-mm-ddThh:mm:ss . Use the ScheduledTime parameter if you want data egress to begin at or after a specific time instead of beginning immediately. If you do not specify a ScheduledTime , EDS uses the time the request is received. Note: Only one manual egress job runs at a time. StartIndex Optional string Identifies the first data point to transfer. If null , it is interpreted as the date and time egress is scheduled to start. The Valid formats are: UTC: yyyy-mm-ddThh:mm:ssZ , Local: yyyy-mm-ddThh:mm:ss , and Relative: +d.hh:mm:ss.## or -d.hh:mm:ss.## . Relative time strings are calculated based on the ScheduledTime . If the ScheduledTime is not specified, the relative time string is calculated based on the time the egress job request is received. EndIndex Optional string Identifies the last data point to transfer. If null , it is interpreted as the date and time egress is scheduled to start. Valid formats are: UTC: yyyy-mm-ddThh:mm:ssZ , Local: yyyy-mm-ddThh:mm:ss , and Relative: +d.hh:mm:ss.## . Relative time strings are calculated based on the StartIndex . Relative time strings must be positive to ensure a range of data is selected for egress. DataSelectors Optional array An array of configuration settings to select data for egress. See the DataSelectors parameters in the following table. The following table lists egress parameters for DataSelectors . Parameter Required Type Description Id Required string Unique identifier of the data selector configuration. StreamFilter Optional string A filter used to determine which streams and types are egressed. For more information on valid filters, see Search in SDS . AbsoluteDeadband Optional string Specifies the absolute change in data value that will cause the current value to pass the filter test. PercentChange Optional string Specifies the percent change from previous value that will cause the current value to pass the filter test. ExpirationPeriod Optional string The length in time that can elapse after an event before automatically storing the next event. The expected format is HH:MM:SS.### . Example manual egress request The following is an example manual egress request. { \"Id\": \"Egress1\", \"EndpointId\": \"AVEVA Data Hub_Location\", \"Period\": \"00:00:30\", \"ScheduledTime\": \"2022-08-10T21:20:00Z\", \"StartIndex\": \"2022-08-08T18:20:00Z\", \"EndIndex\": \"+ 04:00:00\", \"DataSelectors\": [ { \"id\": \"PercentChangeFilter\", \"percentChange\": 1 } ] } Example manual egress response The following is an example response for a manual egress request. { \"id\": \"Egress1\", \"endpointId\": \"AVEVA Data Hub_Location\", \"period\": \"00:00:30\", \"requestTimeUtc\": \"2022-08-10T13:45:58.607148Z\", \"scheduledTime\": \"2022-08-10T21:20:00Z\", \"dataSelectors\": [ { \"id\": \"PercentChangeFilter\", \"streamFilter\": null, \"absoluteDeadband\": null, \"percentChange\": 1, \"expirationPeriod\": null } ], \"startIndex\": \"2022-08-08T18:20:00Z\", \"endIndex\": \"+ 04:00:00\", \"startIndexDateTimeUtc\": \"2022-08-08T18:20:00Z\", \"endIndexDateTimeUtc\": \"2022-08-08T22:20:00Z\", \"checkpoint\": null, \"progress\": 0, \"status\": \"Active\", \"errors\": null } Response parameters Response parameters include information sent in the manual egress request and additional information about how the request will be processed. The following table lists the response parameters for manual egress. Parameter Description RequestTimeUtc The date and time EDS received the egress request. StartIndexDateTimeUtc The date and time of the first data point to transfer in Coordinated Universal Time (UTC). If you specified a relative time, this is the calculated result. EndIndexDateTimeUtc The date and time of the last data point to transfer Coordinated Universal Time (UTC). If you specified a relative time, this is the calculated result. Checkpoint The latest timestamp that the egress has completed with the range between StartIndex and EndIndex . Progress Current percent complete of the egress job. Status Status of the egress job. Values are Active , Canceled , Complete , and Failed . Errors Errors encountered during egress. REST URLs Relative URL HTTP verb Action api/v1/configuration/storage/manualegresses api v1 configuration storage manualegresses POST Requests a manual egress job api/v1/configuration/storage/manualegresses api v1 configuration storage manualegresses GET Returns the state of all manual egress jobs in the queue api/v1/configuration/storage/manualegresses api v1 configuration storage manualegresses DELETE Cancels all active manual egress job and removes states api/v1/configuration/storage/manualegresses/\u003cId\u003e api v1 configuration storage manualegresses \u003cId\u003e GET Returns the state of a specific egress job api/v1/configuration/storage/manualegresses/\u003cId\u003e api v1 configuration storage manualegresses \u003cId\u003e DELETE Deletes the state of an individual egress job api/v1/configuration/storage/manualegresses/\u003cId\u003e/cancel api v1 configuration storage manualegresses \u003cId\u003e cancel POST Cancels an egress job api/v1/configuration/storage/manualegresses/\u003cId\u003e/resume api v1 configuration storage manualegresses \u003cId\u003e resume POST Resumes a canceled or failed egress job Note: Replace \u003cId\u003e with the Id of the egress job for which you want to perform the action."
                                          },
    "content/egress/prepare-egress-destinations.html":  {
                                                            "href":  "content/egress/prepare-egress-destinations.html",
                                                            "title":  "Prepare egress destinations",
                                                            "keywords":  "Prepare egress destinations AVEVA Data Hub and PI Server destinations require additional configuration to receive OMF messages. AVEVA Data Hub destinations To prepare AVEVA Data Hub to receive OMF messages from EDS, create an OMF connection in AVEVA Data Hub. Creating an OMF connection results in an available OMF endpoint that EDS can use for the egress mechanism. To create an OMF connection to AVEVA Data Hub: In AVEVA Data Hub, create a client. For details, see Clients in the AVEVA Data Hub documentation. The Client Id and Client Secret are used for the corresponding properties in the EgressEndpoints configuration. In AVEVA Data Hub, create an OMF connection. For details, see Configure an OMF connection in the AVEVA Data Hub documentation. The OMF Endpoint URL for the connection is used as the EgressEndpoints configuration Endpoint parameter. PI Server destinations To prepare a PI Server to receive OMF messages from EDS, a PI Web API OMF endpoint must be available to be used as the EgressEndpoints configuration Endpoint parameter. To create an OMF connection to PI Server: Install PI Web API and enable the OSIsoft Message Format (OMF) Services feature. During configuration, choose an AF database and PI Data Archive where metadata and data will be stored. Username and Password are used in the EgressEndpoints configuration to connect to the PI Server. The referenced account needs permissions to create AF elements, element templates, and PI points. Configure PI Web API to use Basic authentication. For complete steps, as well as best practices and recommendations, see the PI Web API User Guide . Note: The certificate used by PI Web API must be trusted by the device running EDS, otherwise the EgressEndpoints configuration ValidateEndpointCertificate property needs to be set to false . This can be the case with a self-signed certificate , which should only be used for testing purposes. Note: To continue to send OMF egress messages to the PI Web API endpoint after upgrading PI Web API, restart the EDS service. \u003c!-- What is used for the EndpointId parameter in this case? --\u003e"
                                                        },
    "content/feedback.html":  {
                                  "href":  "content/feedback.html",
                                  "title":  "Technical support and feedback",
                                  "keywords":  "Technical support and feedback OSIsoft provides several ways to report issues and provide feedback on Edge Data Store. Technical support For technical assistance with Edge Data Store, contact OSIsoft Technical Support through the OSIsoft Customer Portal . The Technical Support team will respond to resolve your issue. Remote access to your facilities may be necessary during the session. Note: You must have an account set up in the OSIsoft Customer Portal before you can open a support case. If you do not have a portal account, see How to Get a Login to OSIsoft Customer Portal . Alternatively, call OSIsoft Technical Support at +1 510-297-5828. When contacting OSIsoft Technical Support, provide the following information to enable timely resolution of your issue: Product name, version, and build numbers Details about your computer platform (CPU type, operating system, and version number) Date and time the issue started Log files during the time the issue occurred Details of any environment changes prior to the start of the issue Summary of the issue Product feedback To submit product feedback, including enhancement requests, visit the Edge Data Store feedback page . The OSIsoft product team will review and address feedback for potential inclusion in future product updates. Documentation feedback To submit documentation feedback, send an email to documentation@aveva.com and include the following information: Documentation topic URL Details of the issue The technical documentation team will review and address your feedback in future documentation updates."
                              },
    "content/health/health.html":  {
                                       "href":  "content/health/health.html",
                                       "title":  "Edge Data Store health",
                                       "keywords":  "Edge Data Store health Edge Data Store and its components produce health information to provide insight into their status, which is critical for monitoring data collection. When configured, EDS transfers health information to OMF endpoints, including the types and containers that represent available health information. To enable this functionality, configure one or more health endpoints. EDS also produces diagnostic data. You can use diagnostic data to find more information about a particular component instance. Diagnostic data lives alongside the health data and you can egress it using a health endpoint and setting EnableDiagnostics to true . You can configure EnableDiagnostics in the system configuration. For more information on available adapter diagnostics data, see Diagnostics configuration . EDS adapter health The following health types and streams are created to reflect the health of EDS adapters. Note: If a Health prefix is assigned in System_General\u0027s Health Prefix field (xref: GeneralConfiguration), all streams will be prefixed with {HealthPrefix} . The EDS static type includes these properties and servers as a root AF element with the ID. Property Type Description Id string Edge Data Stores - root AF element Description string Collection of Adapter assets EDS adapter component health The DataCollectorService static type includes the following properties, which are logged in a stream with the ID {machinename}.{EdgeDataStore} . The stream is linked to root AF element (Adapters). Property Type Description Id string {machinename}.EdgeDataStore Description string EDS Service Health Type string Edge Data Store Version string {EDS Version} Device status The DeviceStatus dynamic type includes the following values, which are logged in a stream with the ID Adapters. {machinename}.{componentid}.DeviceStatus . The stream is linked to {machinename}.{componentid} static stream. Property Type Description Time string Timestamp of event DeviceStatus string Device status value Next health message expected The NextHealthMessageExpected dynamic type includes the following values, which are logged in a stream with the ID Adapters.{machinename}.{componentid}.NextHealthMessageExpected . The stream is linked to {machinename}.{componentid} static stream. Heart beat message is expected once a minute. Property Type Description Time string Timestamp of event NextHealthMessageExpected string Time when next health message is expected. Storage health The Storage Health static type includes the following properties, which are logged in a stream with the ID {machinename}.Storage . The stream is linked to Edge Data Store\u0027s service element {machinename}.EdgeDataStore . Property Type Description Id string {machinename}.Storage Description string Storage health Host string {machinename} Version string {storageversion} Storage device status The DeviceStatus dynamic type includes the following values, which are logged in a stream with the ID Storage. {machinename}.DeviceStatus . The stream is linked to {machinename}.Storage static stream. Property Type Description Time string Timestamp of event DeviceStatus string Device status value Storage next health message expected The NextHealthMessageExpected dynamic type includes the following values, which are logged in a stream with the ID Storage.{machinename}.NextHealthMessageExpected . The stream is linked to {machinename}.Storage static stream. Heart beat message is expected once a minute. Property Type Description Time string Timestamp of event NextHealthMessageExpected string Time when next health message is expected. Example health strucutre:"
                                   },
    "content/index.html":  {
                               "href":  "content/index.html",
                               "title":  "Edge Data Store",
                               "keywords":  "Edge Data Store Edge Data Store (EDS) is a lightweight data collection and storage application designed to capture data at the edge of networks for historical storage and analysis. It runs on small, rugged devices or embedded in existing industrial hardware and is designed for resilience and require minimal installation and administration. While not a replacement for a PI System or AVEVA Data Hub, EDS augments the PI System and AVEVA Data Hub by collecting and storing data in situations where deploying a full system is impractical. It can collect data that is beyond the reach of automation systems, in unreliable network conditions, and in environments too rough for traditional computers. Edge Data Store can run almost anywhere you can install a sensor, such as beam pumps, mining trucks, wind mills, and more. The following diagram shows conceptually how EDS captures data and sends to permanent storage: EDS collects data using any of the following methods: Built-in OPC UA connectivity Built-in Modbus TCP connectivity Custom application using OSIsoft Message Format (OMF) Custom application using REST API Once collected, EDS stores the data locally in configurable data storage within EDS until it can be sent to permanent storage in a PI System or in AVEVA Data Hub through periodic egress. The data can also be read from local storage with custom applications that use REST APIs. Edge Data Store architecture EDS runs on both Linux and Windows platforms and is comprised of separate components that each perform a specific function within EDS. The following diagram shows Edge Data Store architecture with all of its components and how the data flow through those components: EDS components are shown in grey within the Edge Data Store in the diagram: Modbus TCP EDS adapter ??? Collects data from Modbus TCP devices and writes it to data storage OPC UA EDS adapter ??? Collects data from OPC UA devices and writes it to data storage Data Storage ??? Stores data locally until it can be egressed Data egress ??? Sends data from storage to PI Server or AVEVA Data Hub Health ??? Records health information of components and sends it to PI Server or AVEVA Data Hub Blue boxes in the diagram show ways to interact with EDS from the local device: OMF REST ??? Use OSIsoft Message Format to write data to the data storage component programmatically SDS REST APIs ??? Use SDS REST APIs to read data from and write data to the data storage component programmatically Configuration ??? Use REST or the EdgeCmd tool to configure EDS as a whole or each component individually and to view the current configuration EDS requires an endpoint to connect to REST APIs on the local device, which is shown outlined in blue in the diagram. By default, the endpoint uses port 5590; however, you can configure it to use another port. Orange arrows show data flowing into EDS and blue arrows show data flowing out of EDS. For detailed information about configuring each component of EDS, see Configuration . \u003c!-- # OSIsoft Edge Data Store ======= - [Overview](xref:EdgeDataStoreOverview) - [Design considerations](xref:scalePerformance) - [Performance](xref:Performance) - [Security](xref:security) - [Quick start guides](xref:QuickStartGuides) - [OPC UA EDS adapter quick start](xref:opcUaQuickStart) - [Modbus TCP adapter quick start](xref:modbusQuickStart) - [OMF quick start](xref:omfQuickStart) - [AVEVA Data Hub egress quick start](xref:ocsEgressQuickStart) - [PI egress quick start](xref:piEgressQuickStart) - [SDS Read/Write Read Write quick start](xref:sdsQuickStart) - [Command line quick start - Linux](xref:commandLineLinuxQuickStart) - [Command line quick start - Windows](xref:commandLineWindowsQuickStart) - [Installation](xref:installationOverview) - [System requirements](xref:SystemRequirements) - [Linux and Windows platform differences](xref:linuxWindows) - [Install Edge Data Store](xref:InstallEdgeDataStore) - [Docker](xref:edgeDocker) - [Verify installation](xref:VerifyInstallation) - [Uninstall Edge Data Store](xref:UninstallEdgeDataStore) - [Configuration](xref:Configuration) - [Configuration tools](xref:ConfigurationTools) - [System configuration](xref:SystemConfiguration) - [System components configuration](xref:SystemComponentsConfiguration) - [System port configuration](xref:SystemPortConfiguration) - [Edge Data Store configuration](xref:EdgeDataStoreConfiguration) - [Data ingress configuration](xref:EDSDataIngress) - [OPC UA EDS adapter](xref:opcUaOverview) - [Supported features](xref:SupportedFeaturesOPCUA) - [Principles of operation](xref:PrinciplesOfOperationOPCUA) - [Data source configuration](xref:OPCUADataSourceConfiguration) - [Data selection configuration](xref:OPCUADataSelectionConfiguration) - [Adapter security](xref:OPCUAAdapterSecurityConfiguration) - [Modbus TCP EDS adapter](xref:modbusOverview) - [Supported features](xref:SupportedFeaturesModbus) - [Principles of operation](xref:PrinciplesOfOperationModbus) - [Data source configuration](xref:ModbusTCPDataSourceConfiguration) - [Data selection configuration](xref:ModbusTCPDataSelectionConfiguration) - [OSIsoft Message Format (OMF)](xref:omfOverview) - [Storage](xref:storage) - [Storage runtime configuration](xref:storageruntime) - [Data egress configuration](xref:egress) - [Prepare egress destinations](xref:PrepareEgressDestinations) - [Egress execution details](xref:EgressExecutionDetails) - [Diagnostics configuration](xref:EdgeDataStoreDiagnostics) - [Health endpoints configuration](xref:HealthEndpointsConfiguration) - [Logging configuration](xref:LoggingConfig) - [Administration](xref:EdgeDataStoreAdministration) - [Retrieve product version information](xref:RetrieveProductVersionInformation) - [Reset Edge Data Store](xref:ResetEdgeDataStore) - [Reset the Storage component](xref:ResetTheStorageComponent) - [Stop and start an EDS adapter](xref:StopAndStartAnEDSAdapter) - [Troubleshoot Edge Data Store](xref:troubleShooting) - [Disaster recovery](xref:disasterRecovery) - [Reference](xref:Reference) - [Sequential Data Store (SDS)](xref:sdsOverview) - [Types](xref:sdsTypes) - [Streams](xref:sdsStreams) - [Stream views](xref:sdsStreamViews) - [Indexes](xref:sdsIndexes) - [Read Data](xref:sdsReadingData) - [API calls for reading data](xref:sdsReadingDataApi) - [Filter expressions](xref:sdsFilterExpressions) - [Table format](xref:sdsTableFormat) - [Write data](xref:sdsWritingData) - [Response format](xref:ResponseFormatWriteAPIs) - [API calls for writing data](xref:sdsWritingDataApi) - [Units of measure](xref:unitsOfMeasure) - [Compression](xref:sdsCompression) - [Searching](xref:sdsSearching) - [EdgeCmd commands](xref:EdgecmdCommands) - [Release notes](xref:releaseNotes) - [Technical support and feedback](xref:Feedback) --\u003e"
                           },
    "content/installation/installation-overview.html":  {
                                                            "href":  "content/installation/installation-overview.html",
                                                            "title":  "Installation",
                                                            "keywords":  "Installation You can install Edge Data Store in one of two ways: Using an install kit. For more information, see Install Edge Data Store . Using Docker containers. For more information, see Docker . For a list of supported platforms and processors, see System requirements ."
                                                        },
    "content/installation/install-edge-data-store.html":  {
                                                              "href":  "content/installation/install-edge-data-store.html",
                                                              "title":  "Install Edge Data Store",
                                                              "keywords":  "Install Edge Data Store Install Edge Data Store using an install kit, as described in this section, or using Docker containers. For more information on using Docker, see Install Edge Data Store using Docker . For a list of supported platforms and processors, see System requirements . The installation includes the OPC UA EDS adapter, the Modbus TCP EDS adapter, and the Sequential Data Store (SDS) storage. An OPC UA EDS adapter instance and a Modbus TCP EDS adapter instance can each be added during the installation. Additional instances can be added for each adapter after installation. For more information, see Data ingress configuration . The port assignment can be changed either during or after installation. For more information on how to change the port number after installation, see System port configuration . Windows (Windows 10 x64) You must have administrator privileges on the device to install EDS. Run the installation file directly to step through a wizard or use the command line to run the installation, including silent installation. For instructions on verifying the EDS installation, see Verify installation . Download the install file To download the Windows installation file: Download the Windows EdgeDataStore_1.1.1.46-x64_.msi file from the OSIsoft Customer portal . Note: Customer login credentials are required to access the portal. Copy the EdgeDataStore_1.1.1.46-x64_.msi file to the file system of the device. Run the installation wizard To install EDS on Windows using the installation wizard: To start the installer, double-click the EdgeDataStore_1.1.1.46-x64_.msi file in Windows Explorer. In the OSIsoft Edge Data Store Setup window, select Next . (Optional) Change the installation folder and port number. Note: OSIsoft recommends you use the default installation path. Valid values for the port number are in the range of 1024 to 65535 and only an unused port number should be entered. The default port is 5590. (Optional) Add a system component for a Modbus TCP EDS adapter instance, an OPC UA EDS adapter instance, or both. Note: The Modbus TCP EDS adapter and the OPC UA EDS adapter are both installed regardless of whether system components are added. Additional system components can be added for each adapter after installation. Select Next \u003e Install . Select Finish . Run the installation from a command line To install EDS on Windows from the command line: Open a command window, by running as an administrator, and change the working directory to the location of the EdgeDataStore_1.1.1.46-x64_.msi file. Enter the msiexec command and specify parameters to run the installation, using the following example as a guide. Msiexec /quiet  quiet /i  i EdgeDataStore_1.1.1.46-x64_.msi PORT=\"\u003cnumber\u003e\" INSTALLFOLDER=\"\u003cfile_path\u003e\" WIXUI_ENABLEMODBUS=\"1\" WIXUI_ENABLEOPCUA=\"1\" Parameters: /quiet  quiet ??? The installation runs in silent mode. /i  i ??? This is the install flag. PORT ??? Specify a port other than the default of 5590. If the \"quiet\" or \"no ui\" flag for msiexec is specified and the PORT value on the command line is not valid, the install will proceed with the default 5590 value. INSTALLFOLDER ??? Specify an alternate location for the binary components other than the default location of \"%PROGRAMFILES%\\OSISoft\\EdgeDataStore\". OSIsoft recommends you use the default installation path. WIXUI_ENABLEMODBUS ??? Add a system component to create a Modbus TCP EDS adapter instance. The value must be 1 for the component to be added. WIXUI_ENABLEOPCUA - Add a system component to create a OPC UA EDS adapter instance. The value must be 1 for the component to be added. Note: If you do not specify a parameter, the default value for the parameter is used. Property names must be in all capital letters, for example, PORT . Linux You must have administrator privileges to install the software, for example root or sudo privilege and the Linux OS must be up to date for the install to succeed. For instructions on how to verify the Edge Data Store installation, see Verify installation . Download the distribution file To download the appropriate file for your device: Download the Linux distribution file from the OSIsoft Customer portal . Note: Customer login credentials are required to access the portal. Copy the Linux distribution file to the file system of the device. Install on a Linux device To install EDS on Linux: Open a terminal window and change the working directory to the location of the distribution file. Run the apt install command for the distribution file appropriate to your operating system and processor. Debian 9 or later (Intel/AMD (Intel AMD 64-bit processors) sudo apt install ./EdgeDataStore_1.1.1.46-x64_.deb . EdgeDataStore_1.1.1.46-x64_.deb Debian 9 or later (ARM32, Raspberry PI 2,3,4: Raspbian, BeagleBone) sudo apt install ./EdgeDataStore_1.1.1.46-arm_.deb . EdgeDataStore_1.1.1.46-arm_.deb Debian 9 or later (Raspberry PI 3,4: Ubuntu ARM64 Server, Google Coral Dev Board, Nvidia Nano Jetson) sudo apt install ./EdgeDataStore_1.1.1.46-arm64_.deb . EdgeDataStore_1.1.1.46-arm64_.deb A validation check for prerequisites is performed. If the install fails, run the following commands from the terminal window and try the install again: sudo apt update sudo apt upgrade (Optional) Change the port number and press Enter. The default port is 5590. Note: If you specify an invalid value for the port, the install will proceed with the default value of 5590. (Optional) Add a system component for a Modbus TCP EDS adapter instance, an OPC UA EDS adapter instance, or both, and press Enter. Note: The Modbus TCP EDS adapter and the OPC UA EDS adapter are both installed, regardless of whether system components are added. Additional system components can be added for each adapter after installation. Silent install on a Linux device To perform a silent install EDS on Linux with all default options: Open a terminal window and change the working directory to the location of the distribution file. Run the apt-get install command for the distribution file appropriate to your operating system and processor. sudo apt-get install -q -y ./Edgeinstallfile.deb . Edgeinstallfile.deb \u003c /dev/null  dev null Parameters: -q ??? Specifies a silent install. -y ??? Responds Yes to installing prerequisites. Edgeinstallfile.deb ??? The name of the distribution file. \u003c /dev/null  dev null ??? All defaults are used in the installation. Silent install on a Linux device with specified parameters Complete the following steps to perform a silent install EDS on Linux using a parameter file to customize the installation: Create a file called silent.ini with the following parameters on separate lines: \u003cport_number\u003e \u003cY/N\u003e \u003cY N\u003e - Specifies whether to create a Modbus TCP EDS component. \u003cY/N\u003e \u003cY N\u003e - Specifies whether to create an OPCUA EDS component. For example: 4567 Y N Open a terminal window and change the working directory to the location of the distribution file. Run the apt-get install command for the distribution file appropriate to your operating system and processor. sudo apt-get install -q -y ./Edgeinstallfile.deb . Edgeinstallfile.deb \u003c silent.ini Parameters: -q ??? Specifies a silent install. -y ??? Responds Yes to installing prerequisites. Edgeinstallfile.deb ??? The name of the distribution file. \u003c silent.ini ??? The file with the installation properties."
                                                          },
    "content/installation/system-requirements.html":  {
                                                          "href":  "content/installation/system-requirements.html",
                                                          "title":  "System requirements",
                                                          "keywords":  "System requirements Edge Data Store is supported on a variety of platforms and processors. Install kits are available for the platforms listed in the following table. Operating System Platform Installation Kit Processors Windows 10 Enterprise Windows 10 IoT Enterprise Windows 11 x64 EdgeDataStore_1.1.1.46-x64_.msi Intel/AMD Intel AMD 64-bit processors Debian 10, 11 Ubuntu 20.04, 22.04 x64 EdgeDataStore_1.1.1.46-x64_.deb Intel/AMD Intel AMD 64-bit processors Debian 10, 11 Ubuntu 20.04, 22.04 ARM32 EdgeDataStore_1.1.1.46-arm_.deb Arm 32-bit processors Debian 10, 11 Ubuntu 20.04, 22.04 ARM64 EdgeDataStore_1.1.1.46-arm64_.deb Arm 64-bit processors Alternatively, you can use tar.gz files with binaries to build your own custom installers or containers for Linux. For more information on installing EDS with Docker containers, see Install Edge Data Store using Docker . PI Web API compatibility This version of EDS is compatible with PI Web API 2021 and later."
                                                      },
    "content/installation/uninstall-edge-data-store.html":  {
                                                                "href":  "content/installation/uninstall-edge-data-store.html",
                                                                "title":  "Uninstall Edge Data Store",
                                                                "keywords":  "Uninstall Edge Data Store Uninstall Edge Data Store to remove the program files from a device. The data files, configuration files, and log files can also be removed. Uninstall from Windows To remove EDS from a Windows device: To remove the EDS program files from a Windows device, use the Windows Control Panel uninstall application process. The configuration, data, and log files are not removed by the uninstall process. (Optional) To remove all data stored in the Edge Storage component, all configuration files, and all log files, delete the directory C:\\ProgramData\\OSIsoft\\EdgeDataStore . Uninstall from Linux To remove EDS from a Linux device: To remove EDS software from a Linux device, open a terminal window and run the following command: sudo apt remove osisoft.edgedatastore The configuration, data, and log files are not removed by the uninstall process. (Optional) To remove all data stored in the Edge Storage component, all configuration files, and all log files, delete the directory /usr/share/OSIsoft/EdgeDataStore/  usr share OSIsoft EdgeDataStore  . Alternatively, run the following command: sudo rm -r /usr/share/OSIsoft/EdgeDataStore/  usr share OSIsoft EdgeDataStore "
                                                            },
    "content/installation/verify-installation.html":  {
                                                          "href":  "content/installation/verify-installation.html",
                                                          "title":  "Verify installation",
                                                          "keywords":  "Verify installation Depending on the device capabilities, it may take some time before the Edge Data Store is fully initialized and running for the first time. Allow time for start up before verifying that Edge Data Store is correctly installed. To verify the Edge Data Store installation, follow these steps. Open a terminal window and run the following command, replacing \u003cport_number\u003e with the port number specified during installation: curl http://localhost:\u003cport_number\u003e/api/v1/configuration http:  localhost:\u003cport_number\u003e api v1 configuration If you receive an error, wait a few seconds and try the script again. If the installation was successful, a JSON copy of the default system configuration is returned: { \"Storage\": { \"EgressEndpoints\": [], \"Schedules\": [], \"DataSelectors\": [], \"EgressConfigurations\": [], \"Runtime\": { \"streamStorageLimitMb\": 2, \"streamStorageTargetMb\": 1, \"ingressDebugExpiration\": \"0001-01-01T00:00:00\", \"checkpointRateInSec\": 30, \"transactionLogLimitMB\": 250, \"enableMetrics\": false }, \"Logging\": { \"logLevel\": \"Information\", \"logFileSizeLimitBytes\": 34636833, \"logFileCountLimit\": 31 } }, \"System\": { \"Logging\": { \"logLevel\": \"Information\", \"logFileSizeLimitBytes\": 34636833, \"logFileCountLimit\": 31 }, \"HealthEndpoints\": [], \"Components\": [ { \"componentId\": \"Storage\", \"componentType\": \"Storage\" } ], \"Buffering\": { \"bufferLocation\": \"C:/ProgramData/OSIsoft/EdgeDataStore/Buffers\", \"C: ProgramData OSIsoft EdgeDataStore Buffers\", \"maxBufferSizeMB\": 1024, \"enablePersistentBuffering\": true }, \"General\": { \"enableDiagnostics\": true, \"metadataLevel\": \"Medium\", \"healthPrefix\": null } } }"
                                                      },
    "content/linux-windows/linux-windows.html":  {
                                                     "href":  "content/linux-windows/linux-windows.html",
                                                     "title":  "Linux and Windows platform differences",
                                                     "keywords":  "Linux and Windows platform differences Edge Data Store is available on both Windows and Linux platforms and it works the same way on both platforms. There are a few differences between the platforms. Files are stored in different locations based on the platform and Linux systems have a file descriptor limit that can affect EDS function. File locations Windows Program binaries are located in the C:\\Program Files\\OSIsoft\\EdgeDataStore directory by default. For information about changing this location, see Install Edge Data Store . Configuration, log, and data files are located in C:\\ProgramData\\OSIsoft\\EdgeDataStore . This location is not configurable. If you uninstall EDS, this folder structure is not automatically removed. For information about clearing these files, see Uninstall Edge Data Store . Key material for encrypted secrets of configuration files is stored using the Windows DPAPI in a secure Windows store. This is not configurable. Linux File locations for Linux cannot be configured. Program binaries are located in the /opt/OSIsoft/EdgeDataStore  opt OSIsoft EdgeDataStore directory. Configuration, log, and data files are located in /usr/share/OSIsoft/EdgeDataStore  usr share OSIsoft EdgeDataStore . If you uninstall EDS, this folder structure is not automatically removed. For information about clearing these files, see Uninstall Edge Data Store . Key material for encrypted secrets of configuration files is stored using limited access files under /usr/share/OSIsoft/EdgeDataStore  usr share OSIsoft EdgeDataStore . When the Debian installer is used, Edge Data Store is installed using the service identity osisoft.edgedatastore.service . If you need to restart the service from the Linux command line, use the following command: sudo systemctl restart osisoft.edgedatastore.service File descriptors (handles) When installed on a Linux operating system, EDS is configured with a file descriptor limit that may be higher than the corresponding limit for most processes. The limit is controlled by the LimitNOFILE variable in the file /lib/systemd/system/osisoft.edgedatastore.service  lib systemd system osisoft.edgedatastore.service . Linux operating systems impose a limit on the number of file descriptors used in a process. The number of open file descriptors is directly related to the number of streams used in EDS, for example, data ingress. Every EDS stream uses two file descriptors. EDS will no longer function properly when it reaches the limit of available file descriptors. To ensure EDS functions properly, it is necessary to either limit the number of streams used in EDS or increase the maximum file descriptors allowed per process. File descriptor usage differs on different Linux operating systems and devices, and may differ slightly from execution to execution, so it is important to understand your system and adjust accordingly. For example, when tested on a Raspberry Pi 3 Model B+ using a Raspbian operating system, an installation of EDS with no user-defined streams had an average of 424 open file descriptors. The same installation with 250 streams had an average of 932 open file descriptors. The file descriptor limit per process for the operating system used was 1024. Windows has an object called a handle that is used in much the same way that Linux uses file descriptors; however, Windows does not have a limitation on the number of handles."
                                                 },
    "content/logging/component-logging.html":  {
                                                   "href":  "content/logging/component-logging.html",
                                                   "title":  "Component-level logging configuration",
                                                   "keywords":  "Component-level logging configuration Component-level logging collects information about how the components of Edge Data Store are performing. Each message in the log displays the message severity level, timestamp, and the message itself. By default, EDS captures Information, Warning, and Error messages in the message log. Configure logging To change the message logging behavior: Using any text editor, open the log configuration file that you want. Change the values as needed, so it looks similar to the Logging example . Use any tool capable of making HTTP requests to execute a POST command with the contents of that file to the respective endpoint. Note: Replace \u003cComponentId\u003e with the ComponentId of the adapter instance or Storage component, for example OpcUa1 . Example using curl (run this command from the same directory where the file is located): curl -d \"@componentId_Logging.json\" -H \"Content-Type: application/json\" application json\" -X PUT http://localhost:5590/api/v1/configuration/\u003cComponentId\u003e/Logging http:  localhost:5590 api v1 configuration \u003cComponentId\u003e Logging The component endpoints are the following: Edge Data Store: http://localhost:5590/api/v1/configuration/System/Logging http:  localhost:5590 api v1 configuration System Logging Storage: http://localhost:5590/api/v1/configuration/Storage/Logging http:  localhost:5590 api v1 configuration Storage Logging OSIsoft Adapter for OPC UA: http://localhost:5590/api/v1/configuration/OpcUa1/Logging http:  localhost:5590 api v1 configuration OpcUa1 Logging OSIsoft Adapter for Modbus TCP: http://localhost:5590/api/v1/configuration/Modbus1/Logging http:  localhost:5590 api v1 configuration Modbus1 Logging Logging schema The full schema definition for the logging configuration is in the component specific logging file: Edge Data Store: System_Logging.json Storage: Storage_Logging.json OSIsoft Adapter for OPC UA: OpcUa1_Logging.json OSIsoft Adapter for Modbus TCP: Modbus1_Logging.json If you have more than one adapter of the same kind configured, the default file name incrementally changes, for example, OpcUa2_Logging.json . The component specific logging files are located in the following folders: Windows: %ProgramFlies%\\OSIsoft\\EdgeDataStore\\Schemas Linux: /usr/share/OSIsoft/EdgeDataStore/Schemas  usr share OSIsoft EdgeDataStore Schemas Parameters for logging The following parameters are available for configuring logging. Parameter Required Type Nullable Description LogFileCountLimit Optional integer Yes The maximum number of log files that the service will create for the component. It must be a positive integer. Minimum value: 1 Maximum value: 2147483647 Default value: 31 LogFileSizeLimitBytes Optional integer Yes The maximum size in bytes of log files that the service will create for the component. It must be a positive integer. Minimum value: 1000 Maximum value: 9223372036854775807 Default value: 34636833 LogLevel Optional reference No The log level settings that you want. The following options are available: Verbose - Captures all messages: Verbose, Debug, Information, Warning and Error Debug - Captures most messages: Debug, Information, Warning and Error Information - Captures most messages: Information, Warning and Error Warning - Captures only Warning and Error messages Error - Captures Error messages only Log levels The logLevel sets the minimum severity for messages included in the logs. Messages with a severity below the level set are not included. The log levels in their increasing order of severity are as follows: Trace , Debug , Information , Warning , Error , Critical . The following table has general guidelines for setting the log level. Level Description Trace Logs that contain the most detailed messages. These messages may contain sensitive application data like actual received values and should not be enabled in a production environment. Debug Logs that can be used to troubleshoot data flow issues by recording metrics and detailed flow-related information. Information Logs that track the general flow of the application. Any non-repetitive general information (like version information relating to the software at startup, what external services are being used, data source connection string, number of measurements, egress URL, change of state \"Starting\", \"Stopping\", or configuration) can be useful for diagnosing potential application errors. Warning Logs that highlight an abnormal or unexpected event in the application flow, but does not otherwise cause the application execution to stop. Warning messages can indicate a data source state that is not configured, that a communication with backup failover instance has been lost, an insecure communication channel in use, or any other event that could require attention, but that does not impact data flow. Error Logs that highlight when the current flow of execution is stopped due to a failure. These should indicate a failure in the current activity, not an application-wide failure. This can indicate an invalid configuration, unavailable external endpoint, internal flow error, and so on. Critical Logs that describe an unrecoverable application or system crash, or a catastrophic failure that requires immediate attention. This can indicate application wide failures like beta timeout expired, unable to start self-hosted endpoint, unable to access vital resource (for example, Data Protection key file), and so on. Logging example { \"LogLevel\": \"Information\", \"LogFileSizeLimitBytes\": 34636833, \"LogFileCountLimit\": 31 }"
                                               },
    "content/logging/logging-configuration.html":  {
                                                       "href":  "content/logging/logging-configuration.html",
                                                       "title":  "Configure logging",
                                                       "keywords":  "Configure logging Use the logging configuration to collect information about how Edge Data Store and its components are performing. Set the severity level for the messages to capture, anywhere from critical errors only to debugging messages for troubleshooting. Edge Data Store writes daily log messages to flat text files in the following locations: Windows: %ProgramData%\\OSIsoft\\EdgeDataStore\\Logs Linux: /usr/share/OSIsoft/EdgeDataStore/Logs  usr share OSIsoft EdgeDataStore Logs Each message in the log displays the message severity level, timestamp, and the message itself. Configure logging To change the message logging behavior: Using any text editor, open the log configuration file that you want. Change the values as needed so it looks similar to the Logging example . Use any tool capable of making HTTP requests to execute a POST command with the contents of that file to the respective endpoint. Note: Replace \u003cComponentId\u003e with the ComponentId of the adapter instance or Storage component, for example OpcUa1 . Example using curl or EdgeCmd (run this command from the same directory where the file is located): curl EdgeCmd curl -d \"@componentId_Logging.json\" -H \"Content-Type: application/json\" application json\" -X PUT http://localhost:5590/api/v1/configuration/\u003cComponentId\u003e/Logging http:  localhost:5590 api v1 configuration \u003cComponentId\u003e Logging edgecmd edit logging -cid \u003ccomponentID\u003e -loglevel \u003cLogLevel\u003e The component endpoints are the following: Edge Data Store: http://localhost:5590/api/v1/configuration/System/Logging http:  localhost:5590 api v1 configuration System Logging Storage: http://localhost:5590/api/v1/configuration/Storage/Logging http:  localhost:5590 api v1 configuration Storage Logging OSIsoft Adapter for OPC UA: http://localhost:5590/api/v1/configuration/OpcUa1/Logging http:  localhost:5590 api v1 configuration OpcUa1 Logging OSIsoft Adapter for Modbus TCP: http://localhost:5590/api/v1/configuration/Modbus1/Logging http:  localhost:5590 api v1 configuration Modbus1 Logging Logging schema The full schema definition for the logging configuration is in the component specific logging file: Edge Data Store: System_Logging.json Storage: Storage_Logging.json OSIsoft Adapter for OPC UA: OpcUa1_Logging.json OSIsoft Adapter for Modbus TCP: Modbus1_Logging.json If you have more than one adapter of the same kind configured, the default file name incrementally changed, for example, OpcUa2_Logging.json . The component specific logging files are located in the following folders: Windows: %ProgramFlies%\\OSIsoft\\EdgeDataStore\\Schemas Linux: /usr/share/OSIsoft/EdgeDataStore/Schemas  usr share OSIsoft EdgeDataStore Schemas Parameters for logging The following parameters are available for configuring logging. Parameter Required Type Nullable Description LogFileCountLimit Optional integer Yes The maximum number of log files that the service will create for the component. It must be a positive integer. Minimum value: 1 Maximum value: 2147483647 Default value: 31 LogFileSizeLimitBytes Optional integer Yes The maximum size in bytes of log files that the service will create for the component. It must be a positive integer. Minimum value: 1000 Maximum value: 9223372036854775807 Default value: 34636833 LogLevel Optional reference No The log level settings that you want. The following options are available: Verbose - Captures all messages: Verbose, Debug, Information, Warning and Error Debug - Captures most messages: Debug, Information, Warning and Error Information - Captures most messages: Information, Warning and Error Warning - Captures only Warning and Error messages Error - Captures Error messages only Log levels The logLevel sets the minimum severity for messages to be included in the logs. Messages with a severity below the level set are not included. The log levels in their increasing order of severity are as follows: Trace , Debug , Information , Warning , Error , Critical . The following table has general guidelines for setting the log level. Level Description Trace Logs that contain the most detailed messages. These messages may contain sensitive application data like actual received values and should not be enabled in a production environment. Debug Logs that can be used to troubleshoot data flow issues by recording metrics and detailed flow-related information. Information Logs that track the general flow of the application. Any non-repetitive general information (like version information relating to the software at startup, what external services are being used, data source connection string, number of measurements, egress URL, change of state \"Starting\", \"Stopping\", or configuration) can be useful for diagnosing potential application errors. Warning Logs that highlight an abnormal or unexpected event in the application flow, but does not otherwise cause the application execution to stop. Warning messages can indicate a data source state that is not configured, that a communication with backup failover instance has been lost, an insecure communication channel in use, or any other event that could require attention, but that does not impact data flow. Error Logs that highlight when the current flow of execution is stopped due to a failure. These should indicate a failure in the current activity, not an application-wide failure. This can indicate an invalid configuration, unavailable external endpoint, internal flow error, and so on. Critical Logs that describe an unrecoverable application, system crash, or a catastrophic failure that requires immediate attention. This can indicate application wide failures like beta timeout expired, unable to start self-hosted endpoint, unable to access vital resource (for example, Data Protection key file), and so on. Example logging configuration By default, logging captures Information, Warning, Error, and Critical messages in the message logs. The following logging configuration is the installation default for a component: { \"LogLevel\": \"Information\", \"LogFileSizeLimitBytes\": 34636833, \"LogFileCountLimit\": 31 }"
                                                   },
    "content/logging/system-logging.html":  {
                                                "href":  "content/logging/system-logging.html",
                                                "title":  "System-level logging configuration",
                                                "keywords":  "System-level logging configuration Edge Data Store writes daily log messages to flat text files in the following locations: Windows: %ProgramData%\\OSIsoft\\EdgeDataStore\\Logs Linux: /usr/share/OSIsoft/EdgeDataStore/Logs  usr share OSIsoft EdgeDataStore Logs Each message in the log displays the message severity level, timestamp, and the message itself. If the transaction log reaches its maximum size, EDS sends a DeviceinError message. Default logging configuration and schema By default, logging captures Information, Warning, Error, and Critical messages in the message logs. The following logging configuration is the default for a component on install: { \"logLevel\": \"Information\", \"logFileSizeLimitBytes\": 34636833, \"logFileCountLimit\": 31 } The schema file specifies how to formally describe the configuration parameters for message logging. It is located in: Windows: %ProgramFiles%\\OSIsoft\\EdgeDataStore\\Schema Linux: /opt/OSIsoft/EdgeDataStore/Schema  opt OSIsoft EdgeDataStore Schema Change logging configuration To change the logging configuration: Edit the message logging configuration JSON file that you want to update the parameters as needed. For example, the System_Logging.json file: { \"logLevel\": \"Warning\", \"logFileSizeLimitBytes\": 16777216, \"logFileCountLimit\": 30 } Save the file. Use any tool capable of making HTTP requests to execute a PUT command with the contents of that file to the following endpoint: http://localhost:5590/api/v1/configuration/System/Logging http:  localhost:5590 api v1 configuration System Logging . Example using curl (run this command from the same directory where the file is located): curl -d \"@System_Logging.json\" -H \"Content-Type: application/json\" application json\" -X PUT http://localhost:5590/api/v1/configuration/System/Logging http:  localhost:5590 api v1 configuration System Logging On successful execution, the log-level change takes effect immediately during runtime. The other configurations (log file size and file count) get updated after Edge Data Store is restarted. Note: Any parameter not specified in the updated configuration file will revert to the default schema value. Log levels The logLevel sets the minimum severity for messages to be included in the logs. Messages with a severity below the level set are not included. The log levels in their increasing order of severity are as follows: Trace , Debug , Information , Warning , Error , Critical . The following table has general guidelines for setting the log level. Level Description Trace Logs that contain the most detailed messages. These messages may contain sensitive application data like actual received values and should not be enabled in a production environment. Debug Logs that can be used to troubleshoot data flow issues by recording metrics and detailed flow related information. Information Logs that track the general flow of the application. Any non-repetitive general information (like version information relating to the software at startup, what external services are being used, data source connection string, number of measurements, egress URL, change of state \"Starting\", \"Stopping\", or configuration) can be useful for diagnosing potential application errors. Warning Logs that highlight an abnormal or unexpected event in the application flow, but does not otherwise cause the application execution to stop. Warning messages can indicate a data source state that is not configured, that a communication with backup failover instance has been lost, an insecure communication channel in use, or any other event that could require attention, but that does not impact data flow. Error Logs that highlight when the current flow of execution is stopped due to a failure. These should indicate a failure in the current activity, not an application-wide failure. This can indicate an invalid configuration, unavailable external endpoint, internal flow error, and so on. Critical Logs that describe an unrecoverable application or system crash, or a catastrophic failure that requires immediate attention. This can indicate application wide failures like beta timeout expired, unable to start self-hosted endpoint, unable to access vital resource (for example, Data Protection key file), and so on. Parameters for logging The following parameters are available for configuring logging. Parameter Required Type Nullable Description LogFileCountLimit Optional integer Yes The maximum number of log files that the service will create for the component. It must be a positive integer. LogFileSizeLimitBytes Optional integer Yes The maximum size in bytes of log files that the service will create for the component. It must be a positive integer. LogLevel Optional reference No The log level settings that you want. The following options are available: Verbose - Captures all messages: Verbose, Debug, Information, Warning and Error Debug - Captures most messages: Debug, Information, Warning and Error Information - Captures most messages: Information, Warning and Error Warning - Captures only Warning and Error messages Error - Captures Error messages only"
                                            },
    "content/modbus/modbus-data-selection-configuration.html":  {
                                                                    "href":  "content/modbus/modbus-data-selection-configuration.html",
                                                                    "title":  "Data selection configuration",
                                                                    "keywords":  "Data selection configuration Once a data source is configured for a Modbus TCP instance, create a data selection configuration file to specify the data for the Modbus TCP EDS adapter instance to collect from the data source. Configure Modbus TCP data selection To configure Modbus TCP data selection: Using any text editor, create a file that contains a Modbus TCP data selection in JSON form. For content structure, see Modbus TCP data selection examples . Update the parameters as needed. For a table of all available parameters, see Parameters for Modbus TCP data selection . Save the file to the device with EDS installed with the name Modbus1DataSelection..json . Use any tool capable of making HTTP requests to execute a POST command with the contents of that file to the following endpoint: http://localhost:\u003cport_number\u003e/api/v1/configuration/\u003cEDS http:  localhost:\u003cport_number\u003e api v1 configuration \u003cEDS adapterId\u003e/DataSelection/ adapterId\u003e DataSelection  . The following example shows the HTTPS request using curl, which must be run from the same directory where the file is located, and uses the adapter instance created during installation, which is Modbus1: curl EdgeCmd curl -d \"@ModbusDataSelection..json\" -H \"Content-Type: application/json\" application json\" \"http://localhost:5590/api/v1/configuration/Modbus1/DataSelection\" \"http:  localhost:5590 api v1 configuration Modbus1 DataSelection\" edgecmd set dataSelection -cid Modbus1 -file Modbus1DataSelection..json To see the streams that have been created in EDS storage for the data specified in the configuration, run the following curl script: curl http://localhost:5590/api/v1/tenants/default/namespaces/default/streams/ http:  localhost:5590 api v1 tenants default namespaces default streams  Parameters for Modbus TCP data selection The following parameters are available for configuring Modbus TCP data selection. Parameter Required Type Nullable Description DeviceId Required string No Specifies the DataSource device that this data selection item is read from. The value must match one of the \u003cId\u003e values specified in the DataSource Devices configuration. Selected Optional Boolean No Used to select or clear a measurement. To select an item, set to true. To remove an item, leave the field empty or set to false. If not configured, the default value is true. Name Optional string Yes The optional friendly name of the data item collected from the data source. If not configured, the default value will be the stream ID. UnitId Required number No Modbus TCP slave device unit ID. This must be a value between 0 and 247, inclusively. RegisterType Required number or string No Modbus TCP register type. Supported types are Coil, Discrete, Input16, Input32, Holding16 and Holding32. Input16 and Holding16 are used to read registers that have a size of 16 bits. For registers that have a size of 32 bits, use the Input32 and Holding32 register types. To represent the types, type the register type ID or the exact name: 1 or Coil (Read Coil Status) 2 or Discrete (Read Discrete Input Status) 3 or Holding16 (Read 16-bit Holding Registers) 4 or Holding32 (Read 32-bit Holding Registers) 6 or Input16 (Read 16-bit Input Registers) 7 or Input32 (Read 32-bit Input Registers) RegisterOffset Required number No The 0 relative offset to the starting register for this measurement. For example, if the Holding registers start at base register 40001, the offset to this register is 0. For 40002, the offset to this register is 1. DataTypeCode Required number No Represents the data type that Modbus TCP EDS adapter will read starting at the register specified by the offset. Supported data types are: 1 = Boolean 10 = Int16 20 = UInt16 30 = Int32 31 = Int32ByteSwap 100 = Float32 101 = Float32ByteSwap 110 = Float64 111 = Float64ByteSwap 1001 - 1250 = String 2001 - 2250 = StringByteSwap ScanRate Required number No Defines how often this measurement is read from the device in milliseconds. Acceptable values are from 0 to 86400000. If 0 ms is specified, Modbus TCP EDS adapter will scan for data as fast as possible. BitMap Optional string Yes Bitmap used to extract and reorder bits from a word register. The format of the bitmap is uuvvwwxxyyzz, where uu, vv, ww, yy, and zz each refer to a single bit. A leading zero is required if the referenced bit is less than 10. The low-order bit is 01 and high-order bit is either 16 or 32. Up to 16 bits can be referenced for a 16-bit word (data types 10 and 20) and up to 32 bits can be referenced for a 32-bit word (data type 30 and 31). The bitmap 0307120802 will map the second bit of the original word to the first bit of the new word, the eighth bit to the second bit, the twelfth bit to the third bit, and so on. The high-order bits of the new word are padded with zeros if they are not specified. ConversionFactor Optional number Yes Used to scale the raw response received from the Modbus TCP device. If this is specified, regardless of the specified data type, the value will be promoted to a float32 (single) when stored. [Result = (Value /   Conversion Factor)] ConversionOffset Optional number Yes Used to apply an offset to the response received from the Modbus TCP device. If this is specified, regardless of the specified data type, the value will be promoted to a float32 (single) when stored. [Result = (Value - Conversion Offset)] StreamID Optional string Yes The custom stream ID that will be used to create the streams. If not specified, the Modbus TCP EDS adapter will generate a default stream ID based on the measurement configuration. A properly configured custom stream ID follows these rules: Is not case-sensitive. Can contain spaces. Cannot start with two underscores (\"__\"). Can contain a maximum of 100 characters. Cannot use the following characters: /   : ? # [ ] @ ! $ \u0026 \u0027 ( ) \\ * + , ; = % \u003c \u003e | Cannot start or end with a period. Cannot contain consecutive periods. Cannot consist of only periods. Each JSON object in the file represents a measurement. Add additional JSON objects in the file for each measurement to collect. Modify the fields in each object to configure the measurement parameters. Modbus TCP data selection examples The following are examples of valid Modbus TCP data selection configurations. Minimum data selection configuration: [ { \"DeviceId\": \"Device1\", \"UnitId\": 1, \"RegisterType\": 3, \"RegisterOffset\": 122, \"DataTypeCode\": 20, \"ScanRate\": 1000 } ] Maximum data selection configuration: [ { \"DeviceId\": \"Device1\", \"Selected\": true, \"Name\": \"MyDataItem\", \"UnitId\": 1, \"RegisterType\": 3, \"RegisterOffset\": 123, \"DataTypeCode\": 20, \"ScanRate\": 300, \"StreamId\": \"stream.1\", \"BitMap\": \"020301\", \"ConversionFactor\": 12.3, \"ConversionOffset\": 14.5 } ]"
                                                                },
    "content/modbus/modbus-overview.html":  {
                                                "href":  "content/modbus/modbus-overview.html",
                                                "title":  "Modbus TCP EDS adapter",
                                                "keywords":  "Modbus TCP EDS adapter The Modbus TCP EDS adapter is a component of Edge Data Store that transfers time-series data from a device to EDS. Modbus TCP is a commonly available communication protocol used for connecting and transmitting information between industrial electronic devices. The Modbus TCP EDS adapter can communicate with any device conforming to the Modbus TCP/IP TCP IP protocol through a gateway or router; devices and routers do not need to be on the same subnet as Edge Data Store. The following diagram depicts the data flow of a single instance of Modbus TCP EDS adapter: The adapter instance requests data from the Modbus TCP device and then the device sends its data. The adapter sends the collected data to the storage component where it is held until it can be egressed to permanent storage in PI Server or AVEVA Data Hub. You can configure the adapter instance from the device where EDS is installed and EDS collects health information about the adapter that can be egressed. The Modbus TCP EDS adapter can connect to multiple devices by defining one instance of the adapter for each device. The EDS installation includes the Modbus TCP EDS adapter and the option to add a single Modbus TCP EDS adapter instance. Add additional instances after installation using system components configuration. Once an adapter instance is defined, manually configure it with JSON documents that specify the following: Data source configuration - identifies the device from which the data originates, specifies the security for the connection, and controls how the data streams from that device identified. Each adapter component instance requires a data source configuration. Data selection configuration - specifies what data is collected from the device and how it is identified. Each adapter component instance requires a data selection configuration. With these configurations completed, the Modbus TCP EDS adapter polls devices to capture data, at the rate specified in the configuration and sends it to EDS, where it is stored locally until it can be sent to a PI System or AVEVA Data Hub for long-term storage and analysis."
                                            },
    "content/modbus/operational-overview.html":  {
                                                     "href":  "content/modbus/operational-overview.html",
                                                     "title":  "Operational overview",
                                                     "keywords":  "Operational overview Once an instance of the Modbus TCP EDS adapter is defined in the system components configuration, it must be configured for it to create streams and collect data. Adapter configuration For an Modbus TCP EDS adapter instance to start data collection, configure the adapter by defining the following: Data source - Provide the connection information for the Modbus data source. Data selection - Specify the Modbus TCP items to which the adapter instance should subscribe for data. Logging - Set up the logging behavior for the adapter instance. For more details, see Data source configuration and Data selection configuration . For more information on how to configure logging, see Component-level logging configuration . Connection The Modbus TCP EDS adapter communicates with the Modbus TCP devices through the TCP/IP TCP IP network by sending request packets that are constructed based on the data selection configurations and collects the response packets returned by the devices. Stream creation From the parsed data selection configurations, the Modbus TCP EDS adapter creates types, streams, and data based on the information provided. For each measurement in the data selection configuration, a stream is created in EDS to store time series data. Data collection The Modbus TCP EDS adapter collects data from the Modbus TCP devices at the polling rates specified in the configuration. The rates are set in each of the data selection configurations and can range from 0 milliseconds (as fast as possible) up to 1 day per polling. The adapter automatically optimizes the data collection process by grouping the requests to reduce the I/O I O load imposed on the Modbus TCP networks. Streams by Modbus TCP EDS adapter For each data selection configuration, the Modbus TCP EDS adapter creates a stream with two properties. The properties are described in the following table. Property name Data type Description Timestamp String The response time of the stream data from the Modbus TCP device. Value Specified by the data selection The value of the stream data from the Modbus TCP device Stream ID is a unique identifier for each stream created by the adapter for the selected measurement. If you specify a custom stream ID for the measurement in the data selection configuration, the Modbus TCP EDS adapter will use that stream ID to create the stream. Otherwise, the adapter constructs the stream ID using the following format: \u003cAdapter Component ID\u003e.\u003cUnit ID\u003e.\u003cRegister Type\u003e.\u003cRegister Offset\u003e Note: Naming convention is affected by StreamIdPrefix and ApplyPrefixToStreamID settings in data source configuration. For more information, see Data source configuration . Buffering Because the Modbus TCP EDS adapter sends data directly to EDS, buffering capability is not provided. EDS acts as a buffer before the data is egressed to either a PI Server or AVEVA Data Hub. The amount of data stored in EDS is controlled by the following storage parameters: StreamStorageLimitMb StreamStorageTargetMb For more information about configuring data storage in EDS, see Storage runtime configuration ."
                                                 },
    "content/modbus/README.html":  {
                                       "href":  "content/modbus/README.html",
                                       "title":  "content/modbus/shared-content subtree",
                                       "keywords":  "content/modbus/shared-content content modbus shared-content subtree All content in this directory is consumed from the PI Adapter Modbus Docs repository as a subtree. This repo contains the unique Modbus documents used in EDS. Note that the Modbus document also makes use of the PI Adapter framework documents stored in content/shared-content content shared-content . Subtree wiki article To update the subtree To update content in content/modbus/shared-content content modbus shared-content , perform the following actions from a command prompt session: Enter the following command. git subtree pull --prefix content/modbus/shared-content content modbus shared-content https://github.com/osisoft/PI-Adapter-Modbus-Docs https:  github.com osisoft PI-Adapter-Modbus-Docs main --squash Resolve any conflicts and complete the merge. Remember: The Modbus docs include the PI Adapter framework. Delete any files that are not unique to Modbus and are redundant to the framework. Which branch of the PI Adapter Modbus docs is EDS consuming? It\u0027s currently consuming main . It should continue to consume main until another use case arises. What if I want to consume a different branch of the PI Adapter Modbus docs? If you want to swap out the most recent version of the PI Adapter Modbus docs for a specific version, update the branch parameter in the git subtree pull command: git subtree pull --prefix content/modbus/shared-content content modbus shared-content https://github.com/osisoft/PI-Adapter-Modbus-Docs https:  github.com osisoft PI-Adapter-Modbus-Docs \u003cCUSTOM_BRANCH\u003e --squash"
                                   },
    "content/modbus/shared-content/content/configuration/data-source.html":  {
                                                                                 "href":  "content/modbus/shared-content/content/configuration/data-source.html",
                                                                                 "title":  "Data source configuration",
                                                                                 "keywords":  "Data source configuration For each instance of the Modbus TCP EDS adapter defined in system configuration, you must configure the data source from which it polls data. Configure Modbus TCP data source You cannot manually modify Modbus TCP data source configurations. You must use the REST endpoints to add or edit the configuration. To configure the Modbus TCP data source: Using any text editor, create a file that contains a Modbus TCP data source in JSON format. For content structure, see Modbus TCP data source examples . Modify the parameters in the example to match your environment. For a table of all available parameters, see Parameters for Modbus TCP data source . Save the file to the device with EDS installed using a file name based on the adapter instance name. For example, to use the adapter instance created during installation, which is Modbus1 , name the file Modbus1Datasource.json . Use any tool capable of making HTTP requests to execute a POST command with the contents of that file to the following endpoint: http://localhost:\u003cport_number\u003e/api/v1/configuration/\u003cEDS http:  localhost:\u003cport_number\u003e api v1 configuration \u003cEDS adapterId\u003e/DataSource/ adapterId\u003e DataSource  . The following examples show the HTTPS request using curl and EdgeCmd, which must be run from the same directory where the file is located, and uses the adapter instance created during installation, which is Modbus1 : curl EdgeCmd curl -d \"@Modbus1DataSource..json\" -H \"Content-Type: application/json\" application json\" \"http://localhost:5590/api/v1/configuration/Modbus1/DataSource\" \"http:  localhost:5590 api v1 configuration Modbus1 DataSource\" edgecmd set dataSource -cid Modbus1 -file Modbus1DataSource..json Parameters for Modbus TCP data source The following parameters are available for configuring a Modbus TCP data source. Parameter Required Type Description Devices Required Array of objects List of Modbus devices that this adapter instance reads. All devices read by the adapter share the common configuration defined in this table. For the properties that a device is comprised of, see the Devices table. StreamIdPrefix Optional string Specifies what prefix is used for Stream IDs. The naming convention is {StreamIdPrefix}{StreamId} . An empty string means no prefix will be added to the Stream IDs and names. A null value defaults to ComponentID followed by a period. Example: Modbus1.{DeviceId}.{UnitId}.{RegisterType}.{RegisterOffset} Note: Every time you change the StreamIdPrefix of a configured adapter, for example when you delete and add a data source, you need to restart the adapter for the changes to take place. New streams are created on adapter restart and pre-existing streams are no longer updated. Allowed value: any string Default value: null DefaultStreamIdPattern Optional string Specifies the default stream Id pattern to use. Allowed value: any string Default value: {DeviceId}.{UnitId}.{RegisterType}.{RegisterOffset} . ConnectTimeout Optional string The length of time to wait when the adapter is trying to connect to the data source. * Minimum value: 00:00:01 Maximum value: 00:00:30 Default value: 00:00:05 ReconnectInterval Optional string The length of time to wait before retrying to connect to the data source when the data source is offline. * Minimum value: 00:00:00.1 Maximum value: 00:00:30 Default value: 00:00:01 RequestTimeout Optional string The length of time that the adapter waits for a pending request before marking it as timeout and dropping the request. * Minimum value: must be positive Maximum value: 48 hours or 02:00:00:00 Default value: 00:00:10 DelayBetweenRequests Optional string The minimum length of time between two successive requests sent to the data source. * Minimum value: 00:00:00 Maximum value: 00:00:01 Default value: 00:00:00 MaxResponseDataLength Optional number The maximum length (in bytes) of data that can be read within one transaction. This feature is provided to support devices that limit the number of bytes that can be returned. If there is no device limitation, the request length should be the maximum length of 250 bytes. Minimum value: 2 Maximum value: 250 Default value: 250 SimultaneousRequests Optional number The number of simultaneous reads from a single IP address and port combination to prevent scan overruns when a lot of data is being read from a single device. Minimum value: 1 Maximum value: 16 Default value: 1 Note: You can also specify timespans as numbers in seconds. For example, \"RequestTimeout\": 25 specifies 25 seconds, or \"RequestTimeout\": 125.5 specifies 2 minutes and 5.5 seconds. Devices The following parameters are available for configuring the \u0027Devices\u0027 parameter of a Modbus TCP data source. Parameter Required Type Description Id Required string The ID of the device that is used in data selection to associate a register with a device. IpAddress Required string The IP address of the device from which the data is collected using the Modbus TCP protocol. Host name is not supported. Port Optional number The TCP port of the target device that listens for and responds to Modbus TCP requests. The value ranges from 0 to 65535 . If you do not configure it, the default TCP port is 502 , which is the default port for Modbus TCP protocol. Modbus TCP data source examples The following are examples of valid Modbus TCP data source configurations: Minimal data source configuration { \"Devices\": [ { \"Id\": \"Device1\", \"IpAddress\": \"127.0.0.1\" } ] } Complete data source configuration { \"Devices\": [ { \"Id\": \"Device1\", \"IpAddress\": \"127.0.0.1\", \"Port\": 502 }, { \"Id\": \"Device2\", \"IpAddress\": \"127.0.0.2\", \"Port\": 502 }, { \"Id\": \"Device3\", \"IpAddress\": \"127.0.0.3\", \"Port\": 502 } ], \"StreamIdPrefix\": \"my.prefix\", \"DefaultStreamIdPattern\": \"{DeviceId}.{UnitId}.{RegisterType}.{RegisterOffset}\", \"ConnectTimeout\": \"00:00:05\", \"ReconnectInterval\": \"00:00:01\", \"RequestTimeout\": \"00:00:10\", \"DelayBetweenRequests\": \"00:00:00.5\", \"MaxResponseDataLength\": 125, \"SimultaneousRequests\": 1 } REST URLs Relative URL HTTP verb Action api/v1/configuration/\u003cComponentId\u003e/DataSource api v1 configuration \u003cComponentId\u003e DataSource GET Retrieves the data source configuration. api/v1/configuration/\u003cComponentId\u003e/DataSource api v1 configuration \u003cComponentId\u003e DataSource POST Creates the data source configuration. The adapter starts collecting data after the following conditions are met: ??? The data source configuration POST request is received. ??? A data selection configuration is active. api/v1/configuration/\u003cComponentId\u003e/DataSource api v1 configuration \u003cComponentId\u003e DataSource PUT Configures or updates the data source configuration. Overwrites any active data source configuration. If no configuration is active, the adapter starts collecting data after the following conditions are met: ??? The data source configuration PUT request is received. ??? A data selection configuration is active. api/v1/configuration/\u003cComponentId\u003e/DataSource api v1 configuration \u003cComponentId\u003e DataSource DELETE Deletes the data source configuration. After the request is received, the adapter stops collecting data. Note: Replace ComponentId with the Id of your Modbus TCP component. For example, Modbus1 ."
                                                                             },
    "content/modbus/shared-content/content/configuration/schedules.html":  {
                                                                               "href":  "content/modbus/shared-content/content/configuration/schedules.html",
                                                                               "title":  "Schedules",
                                                                               "keywords":  "Schedules You can configure the adapter to run scans based on a schedule. Each data item can be assigned to a schedule in the data selection configuration. The adapter samples data for those data items at the scheduled time. Note: You start an ingress component without a schedule configuration. A default schedule configuration is added to use as an example. Note: When the adapter framework scheduler misses or skips a scan for any reason, either one of the following messages is printed: Scan skipped for schedule id \u003cId\u003e or Scan missed for schedule \u003cid\u003e . Configure schedules Complete the following steps to change the schedules configuration: Using any text editor, create a file that contains the schedules configuration in the JSON format. For content structure, see the example schedule configuration . For all available parameters, see the schedules parameters . Save the file. For example, ConfigureSchedules.json . Use any of the Configuration tools capable of making HTTP requests to run a PUT command with the contents of the file to the following endpoint: http://localhost:5590/api/v1/configuration/\u003cComponentId\u003e/Schedules http:  localhost:5590 api v1 configuration \u003cComponentId\u003e Schedules . Note: Replace \u003cComponentId\u003e with the ComponentId of the adapter. 5590 is the default port number. If you selected a different port number, replace it with that value. Example using curl or EdgeCmd: Note: Run this curl command or EdgeCmd from the same directory where the file is located. curl EdgeCmd curl -d \"@Modbus1ConfigureSchedules.json\" -H \"Content-Type: application/json\" application json\" -X PUT \"http://localhost:5590/api/v1/configuration/\u003cComponentId\u003e/Schedules\" \"http:  localhost:5590 api v1 configuration \u003cComponentId\u003e Schedules\" edgecmd set schedules -cid Modbus1 -file Modbus1ConfigureSchedules.json On successful execution, the schedules change takes effect immediately during runtime. Schedules parameters The following parameters are available for configuring schedules: Parameter Required Type Description Id Required string Unique identifier for the schedule Allowed value: any string identifier Period Required string The data sampling rate of the schedule. The expected format is HH:MM:SS.###. Invalid inputs: null , negative timespan, or zero A default value must be specified. Offset Optional string The offset from the midnight when the schedule starts. The expected format is HH:MM:SS.### Invalid input: negative timespan A default value must be specified. Note: You can also specify timespans as numbers in seconds. For example, \"Period\": 25 specifies 25 seconds, or \"Period\": 125 specifies 2 minutes and 5 seconds. Example schedule configuration The following is an example of a complete schedule configuration: [ { \"Id\": \"schedule1\", \"Period\": \"00:00:01.500\", \"Offset\": \"00:02:03\" } ] Default schedule configuration If no schedule is configured, the adapter uses the following default schedule configuration: [ { \"Id\": \"1\", \"Period\": \"0:00:05\", \"Offset\": \"0:00:00\" } ] REST URLs Relative URL HTTP verb Action api/v1/configuration/ api v1 configuration  ComponentId /Schedules  Schedules GET Gets all configured schedules api/v1/configuration/ api v1 configuration  ComponentId /Schedules  Schedules DELETE Deletes all configured schedules api/v1/configuration/ api v1 configuration  ComponentId /Schedules  Schedules POST Adds an array of schedules or a single schedule. Fails if any schedule already exists api/v1/configuration/ api v1 configuration  ComponentId /Schedules  Schedules PUT Replaces all schedules api/v1/configuration/ api v1 configuration  ComponentId /Schedules/  Schedules  id GET Gets configured schedule by Id api/v1/configuration/ api v1 configuration  ComponentId /Schedules/  Schedules  id DELETE Deletes configured schedule by Id api/v1/configuration/ api v1 configuration  ComponentId /Schedules/  Schedules  id PUT Replaces schedule by Id . Fails if schedule does not exist api/v1/configuration/ api v1 configuration  ComponentId /Schedules/  Schedules  id PATCH Allows partial updating of configured schedule by Id Note: Replace ComponentId with the Id of your adapter component."
                                                                           },
    "content/modbus/shared-content/content/configuration/security.html":  {
                                                                              "href":  "content/modbus/shared-content/content/configuration/security.html",
                                                                              "title":  "Security",
                                                                              "keywords":  "Security When determining Modbus TCP security practices with regard to REST APIs, only administrators should have access to machines where the adapter is installed to keep the adapter secure. REST APIs are bound to localhost, which means that only requests originating from within the machine are accepted. Modbus protocol The Modbus TCP adapter does not currently support transport layer security between the adapter and the data source, which means that Modbus traffic is unprotected. If needed, use other measures to protect this traffic, such as a VPN connection, air-gapped control network, or SSH tunnel."
                                                                          },
    "content/modbus/shared-content/content/data-filters.html":  {
                                                                    "href":  "content/modbus/shared-content/content/data-filters.html",
                                                                    "title":  "Data filters",
                                                                    "keywords":  "Data filters You can configure PI adapters to perform data filtering to save network bandwidth. Every data item in the data selection configuration can be assigned the Id of a data filter. The adapter filters data for those data items based on the data filter configuration. Note: If you enable data filters and data quality changes, both the old and current data quality values are passed on. Configure data filters Complete the following steps to configure data filters. Use the PUT method in conjunction with the http://localhost:5590/api/v1/configuration/\u003cComponentId\u003e/DataFilters http:  localhost:5590 api v1 configuration \u003cComponentId\u003e DataFilters REST endpoint to initialize the configuration. Using a text editor, create an empty text file. Copy and paste an example configuration for data filters into the file. For sample JSON, see Data filters example . Update the example JSON parameters for your environment. For a table of all available parameters, see Data filters parameters . Save the file. For example, as ConfigureDataFilters.json . Open a command line session. Change directory to the location of ConfigureDataFilters.json . Enter the following curl command or EdgeCmd to initialize the data filters configuration. curl EdgeCmd curl -d \"@ConfigureDataFilters.json\" -H \"Content-Type: application/json\" application json\" -X PUT \"http://localhost:5590/api/v1/configuration/Modbus1/DataFilters\" \"http:  localhost:5590 api v1 configuration Modbus1 DataFilters\" edgecmd set dataFilters -cid Modbus1 -file ConfigureDataFilters.json Note: If you installed the adapter to listen on a non-default port, update 5590 to the port number in use. For a list of other REST operations you can perform, like updating or deleting a data filters configuration, see REST URLs . On successful execution, the change that you have made to data filters takes effect immediately during runtime. Data filters parameters The following parameters are available for configuring data filters: Parameter Required Type Description Id Required string Unique identifier for the data filter. Allowed value: any string identifier AbsoluteDeadband Optional double Specifies the absolute change in data value that should cause the current value to pass the filter test. Note: You must specify AbsoluteDeadband or PercentChange . Allowed value: double value representing absolute deadband number Default value: null PercentChange Optional double Specifies the percent change from previous value that should cause the current value to pass the filter test. Note: You must specify AbsoluteDeadband or PercentChange . Allowed value: double value representing percent change Default value: null ExpirationPeriod Optional timespan The length in time that can elapse after an event before automatically sending the next event, regardless of whether the next event passes the filter or not. The expected format is HH:MM:SS.### or SSS.* Allowed value: any timespan Default value: null * Note: For example, \"ExpirationPeriod\": 5:00 and \"ExpirationPeriod\": 300 both specify an expiration period of 5 minutes and 0 seconds. Data filters example [ { \"Id\": \"DuplicateData\", \"AbsoluteDeadband\": 0, \"PercentChange\": null, \"ExpirationPeriod\": \"01:00:00\" } ] REST URLs Relative URL HTTP verb Action api/v1/configuration/ api v1 configuration  ComponentId /DataFilters  DataFilters GET Gets all configured data filters. api/v1/configuration/ api v1 configuration  ComponentId /DataFilters  DataFilters DELETE Deletes all configured data filters. api/v1/configuration/ api v1 configuration  ComponentId /DataFilters  DataFilters POST Adds an array of data filters or a single data filter. Fails if any data filter already exists. api/v1/configuration/ api v1 configuration  ComponentId /DataFilters  DataFilters PUT Replaces all data. api/v1/configuration/ api v1 configuration  ComponentId /DataFilters  DataFilters PATCH Allows partial updating of configured data filter. api/v1/configuration/ api v1 configuration  ComponentId /DataFilters/  DataFilters  id GET Gets configured data filter by Id . api/v1/configuration/ api v1 configuration  ComponentId /DataFilters/  DataFilters  id DELETE Deletes configured data filter by Id . api/v1/configuration/ api v1 configuration  ComponentId /DataFilters/  DataFilters  id PUT Replaces data filter by Id . Fails if data filter does not exist. Note: Replace ComponentId with the Id of your adapter component."
                                                                },
    "content/modbus/supported-features-modbus.html":  {
                                                          "href":  "content/modbus/supported-features-modbus.html",
                                                          "title":  "Supported features",
                                                          "keywords":  "Supported features The Modbus TCP EDS adapter collects data using register types and then converts the registers into data types. It can apply bitmaps and data conversion to values converted from reading the Modbus TCP devices. Register types The Modbus TCP EDS adapter supports 6 register types, corresponding to 4 function codes (1-4). Since one function code can return two types of registers, either 16-bit or 32-bit depending on the device, either the register type or the register type code is required when configuring the data selection for the adapter. The following table lists all the register types supported in the Modbus TCP EDS adapter. Register Type Register Type Code Description Function Code Coil 1 Read Coil Status 1 Discrete 2 Read Discrete Input Status 2 Holding16 3 Read 16-bit Holding Registers 3 Holding32 4 Read 32-bit Holding Registers 3 Input16 6 Read 16-bit Input Registers 4 Input32 7 Read 32-bit Input Registers 4 When reading from function codes 1 and 2 , the adapter expects these to be returned as single bits. For function codes 3 and 4 , the adapter expects 16 bits to be returned from devices that contain 16-bit registers and 32 bits to be returned from devices that contain 32-bit registers. Data types The Modbus TCP EDS adapter converts readings from single or multiple registers into the data types specified by the data type code and populates the value into streams created in the Edge Data Store. The following table lists all data types with their corresponding type codes supported by the Modbus TCP EDS adapter. Data type code Data type name Value type Register type Description 1 Boolean Boolean Bool 0 = false 1 = true 10 Int16 Int16 Bool/16-bit Bool 16-bit Read 1 Modbus TCP register and interpret as a 16-bit integer. Bytes [BA] read from the device are stored as [AB]. 20 UInt16 UInt16 Bool/16-bit Bool 16-bit Read 1 Modbus TCP register and interpret as an unsigned 16-bit integer. Bytes [BA] read from the device are stored as [AB]. 30 Int32 Int32 16-bit/32-bit 16-bit 32-bit Read 32 bits from the Modbus TCP device and interpret as a 32-bit integer. Bytes [DCBA] read from the device are stored as [ABCD]. 31 Int32ByteSwap Int32 16-bit/32-bit 16-bit 32-bit Read 32 bits from the Modbus TCP device and interpret as a 32-bit integer. Bytes [BADC] read from the device are stored as [ABCD]. 100 Float32 Float32 16-bit/32-bit 16-bit 32-bit Read 32 bits from the Modbus TCP device and interpret as a 32-bit float. Bytes [DCBA] read from the device are stored as [ABCD]. 101 Float32ByteSwap Float32 16-bit/32-bit 16-bit 32-bit Read 32 bits from the Modbus TCP device and interpret as a 32-bit float. Bytes [BADC] read from the device are stored as [ABCD]. 110 Float64 Float64 16-bit/32-bit 16-bit 32-bit Read 64 bits from the Modbus TCP device and interpret as a 64-bit float. Bytes [HGFEDCBA] read from the device are stored as [ABCDEFGH]. 111 Float64ByteSwap Float64 16-bit/32-bit 16-bit 32-bit Read 64 bits from the Modbus TCP device and interpret as a 64-bit float. Bytes [BADCFEHG] read from the device are stored as [ABCDEFGH]. 1001 - 1250 String String 16-bit/32-bit 16-bit 32-bit 1001 reads a one-character string, 1002 reads a two-character string, and 1003 reads a three-character string and so on. Bytes [AB] are interpreted as \"AB\". 2001 - 2250 StringByteSwap String 16-bit/32-bit 16-bit 32-bit 2001 reads a one-character string, 2002 reads a two-character string, and 2003 reads a three-character string and so on. Bytes [BA] are interpreted as \"AB\". Apply bitmap The Modbus TCP EDS adapter supports applying bitmaps to the value converted from the readings from the Modbus TCP devices. A bitmap is a series of numbers used to extract and reorder bits from a word register. The format of the bitmap is uuvvwwxxyyzz , where uu , vv , ww , yy , and zz each refer to a single bit. A leading zero is required if the referenced bit is less than 10. The low-order bit is 01 and high-order bit is either 16 or 32. Up to 16 bits can be referenced for a 16-bit word (data types 10 and 20) and up to 32 bits can be referenced for a 32-bit word (data type 30 and 31). For example, the bitmap 0307120802 maps the second bit of the original word to the first bit of the new word, the eighth bit to the second bit, the twelfth bit to the third bit, and so on. The high-order bits of the new word are padded with zeros if they are not specified. Not all data types support applying bitmap. The data types supporting bitmap are: Int16 (Data type code 10) UInt16 (Data type code 20) Int32 (Data type code 30 and 31) Apply data conversion The Modbus TCP EDS adapter supports applying data conversion to the value converted from reading the Modbus TCP devices. A conversion factor and conversion offset can be specified. The conversion factor is used for scaling the value up or down, and the conversion offset is used for shifting the value. The mathematical equation used in conversion is the following: \u003cAfter Conversion\u003e = \u003cBefore Conversion\u003e /   Factor - Offset Not all data types support applying data conversion. Data types that support data conversion are: Int16 (Data type code 10) UInt16 (Data type code 20) Int32 (Data type code 30 and 31) Float32 (Data type code 100 and 101) The value with data conversion applied will always be converted to the 32-bit float type to maintain the precision of the conversion factor and conversion offset."
                                                      },
    "content/omf/omf-messages.html":  {
                                          "href":  "content/omf/omf-messages.html",
                                          "title":  "OMF messages",
                                          "keywords":  "OMF messages The OSIsoft Message Format (OMF) specification is generic in that it does not specify a particular back-end system. This topic is a companion to the OMF specification which describes how OMF is interpreted by Edge Data Store. When creating an OMF application for EDS, you also need to consider the final destination of the data and review the associated documentation to determine what is supported. Headers Message headers allow you to pass additional information with the message. The message header is where you specify the action for the message, such as CREATE . For a description of each of the headers, see the OMF specification . The omfversion header must match the version of the OMF spec used to construct the message. EDS supports versions 1.0, 1.1, and 1.2 of the OMF specification. Message types OMF message types fall into three categories: Type, Container, and Data, which are described below. Each message type creates a different type of data and contains keywords that define characteristics of the data. Most of the message types are used to create the structure of the data and give it meaning. Data messages contain time-series data for which the PI System is known. The message types and the data they create are described in detail in this section. For details about the keywords supported by AVEVA Data Hub, see the AVEVA Data Hub documentation, AVEVA Data Hub . All messages should only be sent from the OMF application one time, but resending the same definition again does not cause an error. Type messages An OMF type message describes the format of the data to be stored. A type message is interpreted by AVEVA Data Hub as an SdsType in the Sequential Data Store. Because SdsTypes are immutable, update operations are not supported. Create an OMF type The first step in OMF data ingress is to create an OMF type that describes the format of the data to be stored. In this example, the data to be written is a timestamp and a numeric value. To create an OMF type: Create an OMF JSON file that defines the type as follows: [{ \"id\": \"MyCustomType\", \"classification\": \"dynamic\", \"type\": \"object\", \"properties\": { \"Timestamp\": { \"type\": \"string\", \"format\": \"date-time\", \"isindex\": true }, \"Value\": { \"type\": \"number\", \"format\": \"float32\" } } }] The value is indexed by a timestamp, and the numeric value that will be stored is a 32-bit floating point value. To create the OMF type in Edge Storage, store the JSON file with the name OmfCreateType.json on the local device. Run the following curl command: curl -d \"@OmfCreateType.json\" -H \"Content-Type: application/json\" application json\" -H \"producertoken: x \" -H \"omfversion: 1.1\" -H \"action: create\" -H \"messageformat: json\" -H \"messagetype: type\" -X POST http://localhost:5590/api/v1/tenants/default/namespaces/default/omf/ http:  localhost:5590 api v1 tenants default namespaces default omf  When this command completes successfully, an OMF type with the same name is created on the server. Any number of containers can be created from the type, as long as they use a timestamp as an index and have a 32-bit floating point value. The create type message needs to be sent before container and data messages. Container messages An OMF container message uses an OMF type as a template to create a way to collect and group data events. A container message is interpreted as an SdsStream in the Sequential Data Store. Create an OMF container The next step in writing OMF data is to create an OMF container. To create an OMF container: Create an OMF JSON file that defines the container as follows: [{ \"id\": \"MyCustomContainer\", \"typeid\": \"MyCustomType\" }] This container references the OMF type that was created earlier, and an error occurs if the type does not exist when the container is created. To create the OMF container in Edge Storage, store the JSON file with the name OmfCreateContainer.json on the local device. To create the SDS stream to store data defined by the type, run the following curl command: curl -d \"@OmfCreateContainer.json\" -H \"Content-Type: application/json\" application json\" -H \"producertoken: x \" -H \"omfversion: 1.1\" -H \"action: create\" -H \"messageformat: json\" -H \"messagetype: container\" -X POST http://localhost:5590/api/v1/tenants/default/namespaces/default/omf/ http:  localhost:5590 api v1 tenants default namespaces default omf  Data messages An OMF data message sends actual data events, like time-series data, to be stored. A data message is mapped to generic SDS values in the Sequential Data Store. Write data events to the OMF container Once a type and container are defined, you can send data messages to write data to the container. To writer data to the container: Create an OMF JSON file to define data events to be stored in the SdsStreams created in the previous steps. For best performance, batch OMF messages together, as in the following example: [{ \"containerid\": \"MyCustomContainer\", \"values\": [{ \"Timestamp\": \"2019-07-16T15:18:24.9870136Z\", \"Value\": 12345.6789 }, { \"Timestamp\": \"2019-07-16T15:18:25.9870136Z\", \"Value\": 12346.6789 } ] }] Save the JSON file with the name OmfCreateDataEvents.json on the local device. To write data values to the SDS stream, run the following curl command: curl -d \"@OmfCreateDataEvents.json\" -H \"Content-Type: application/json\" application json\" -H \"producertoken: x \" -H \"omfversion: 1.1\" -H \"action: create\" -H \"messageformat: json\" -H \"messagetype: data\" -X POST http://localhost:5590/api/v1/tenants/default/namespaces/default/omf/ http:  localhost:5590 api v1 tenants default namespaces default omf  HTTPS status codes Edge Data Store returns the following status codes to provide feedback when an OMF ingress message is received. If an error occurs because of an issue with the server, such as 408 Request Timeout or 503 Service Unavailable , the application can retry the request. It is up to the OMF application developer to determine how many times to retry a request. Status Code Description Common Causes 204 No Content The OMF message was successfully processed, but there is no additional information to return. 400 Bad Request The OMF message was malformed or not understood. The client should not retry sending the message without modifications. The body or headers of the OMF message were incorrect. Verify the validity of the request contents and headers. 408 Request Timeout The server did not reply to a request within the time that the client was prepared to wait. The client may repeat the request without modifications at any later time. Server busy or over-loaded with other requests. 409 Conflict The request could not be completed due to a conflict with the current state of the server. The information in the OMF message contradicts the data currently stored in EDS. 413 Payload Too Large Payload size exceeds OMF body size limit. The body of an OMF message has a maximum size of 192KB. A request with a body size exceeding the maximum will be rejected with this error code. 500 Internal Server Error The server encountered an unexpected condition. Review EDS logs for more insight. 503 Service Unavailable The server is currently unavailable, retry later. The EDS server is performing maintenance on one or more streams."
                                      },
    "content/omf/omf-overview.html":  {
                                          "href":  "content/omf/omf-overview.html",
                                          "title":  "OSIsoft Message Format",
                                          "keywords":  "OSIsoft Message Format Create a custom application using OSIsoft Message Format (OMF) to send data to EDS from sources that cannot use Modbus or OPC UA protocols. The following diagram depicts the data flow from an OMF data collection application into EDS: The OMF application collects data from a data source and sends it to the Edge Data Store endpoint. The EDS endpoint sends the data to the storage component where it is held until it can be egressed to permanent storage in PI Server or AVEVA Data Hub. The OMF application must run on the same device as Edge Data Store and no authentication is needed. OMF endpoint The route to the OMF endpoint to the Edge Storage component is shown below. Replace \u003cport_number\u003e with the port configured for your EDS system: Method: POST Endpoint: http://localhost:\u003cport_number\u003e/api/v1/tenants/default/namespaces/default/omf http:  localhost:\u003cport_number\u003e api v1 tenants default namespaces default omf This endpoint can only be accessed locally, so the OMF application must run on the same device as EDS. Supported functionality Edge Data Store supports OMF versions 1.0, 1.1, and 1.2 for data ingress. For details on the difference versions of OMF, see the OMF specification, available here: OSIsoft Message Format . The OMF ingress functionality is the same technology that is used in AVEVA Data Hub and writing an OMF application for EDS is very similar to writing an OMF application for AVEVA Data Hub. The OMF endpoint for the Edge Storage component only supports the create action; it does not support the update action. If a create data message is sent with the same time index, the values will be replaced at that index value. Dynamic messages are supported, but static messages (usually used for creating PI AF assets) are not supported by EDS. Any static OMF messages sent to the EDS OMF REST endpoint will be ignored. For efficiency reasons, OSIsoft recommends batching OMF messages that are sent to the EDS endpoint. Sending single messages or a small number of messages to the OMF endpoint can be successful, but it is inefficient. When a single message or a small number of messages are sent at a time, the HTTP overhead of creating the request and processing the response on a small device requires more system resources than the processing of the OMF message itself. While a large number of OMF messages per second can be processed by EDS platforms, OSIsoft recommends keeping the number of HTTP calls per second low to increase efficiency."
                                      },
    "content/opc-ua/opc-ua-adapter-security-configuration.html":  {
                                                                      "href":  "content/opc-ua/opc-ua-adapter-security-configuration.html",
                                                                      "title":  "Adapter security",
                                                                      "keywords":  "Adapter security The OPC UA security standard is concerned with the authentication of client and server applications, the authentication of users, and confidentiality of their communication. As the security model relies heavily on Transport Level Security (TLS) to establish a secure communication link with an OPC UA server, each client, including the OPC UA EDS adapter, must have a digital certificate deployed and configured. Certificates uniquely identify client applications and machines on servers, and allow for creation of a secure communication link when trusted on both sides. The OPC UA EDS adapter instance generates a self-signed certificate when the first secure connection attempt is made. Each OPC UA EDS adapter instance creates a certificate store to persist both its own certificates and those received from the server. Configure OPC UA EDS adapter security To configure OPC UA EDS adapter security: In the data source configuration, set UseSecureConnection to true . For more information, see Data source configuration . The adapter instance verifies whether the server certificate is present in the adapter trusted certificates folder and is therefore trusted. If the certificates were not exchanged before the first attempted connection, the adapter instance saves the server certificate within the adapter rejected certificates folder and returns the following warning message about the rejected server certificate: ~~2019-09-08 11:45:48.093 +01:00~~ [Warning] Rejected Certificate: \"DC=MyServer.MyDomain.int, O=Prosys OPC, CN=Simulation Manually move the server certificate from the RejectedCertificates\\certs folder to the Trusted\\certs folder using a file explorer or command-line interpreter. Linux example using command-line: mv /usr/share/OSIsoft/EdgeDataStore/OpcUa1/Certificates/RejectedCertificates/certs/SimulationServer\\  usr share OSIsoft EdgeDataStore OpcUa1 Certificates RejectedCertificates certs SimulationServer\\ \\[F9823DCF607063DBCECCF6F8F39FD2584F46AEBB\\].der /usr/share/OSIsoft/EdgeDataStore/OpcUa1/Certificates/Trusted/certs/  usr share OSIsoft EdgeDataStore OpcUa1 Certificates Trusted certs  Note: Administrator or root privileges are required to perform this operation. Once the certificate is in the adapter instance trusted certificates folder, the adapter instance trusts the server and the connection attempt makes the connection call to the configured server. Add the adapter certificate to the server\u0027s trust store. The connection succeeds only when the adapter certificate is trusted on the server side. For more details on how to make a client certificate trusted, see your OPC UA server documentation. In general, OPC UA servers work in a similar fashion as the clients, a similar approach may work to make the server certificate trusted on the client side. When certificates are mutually trusted, the connection attempt succeeds and the adapter instance is connected to the most secure endpoint provided by the server. Certificate locations For all of the following locations, {ComponentID} identifies the adapter instance. Adapter rejected certificates Windows: %programdata%\\OSIsoft\\EdgeDataStore\\{ComponentId}\\Certificates\\RejectedCertificates\\certs Linux: /usr/share/OSIsoft/EdgeDataStore/{ComponentId}/Certificates/RejectedCertificates/certs  usr share OSIsoft EdgeDataStore {ComponentId} Certificates RejectedCertificates certs Adapter trusted certificates Windows: %programdata%\\OSIsoft\\EdgeDataStore\\{ComponentId}\\Certificates\\Trusted\\certs Linux: /usr/share/OSIsoft/EdgeDataStore/{ComponentId}/Certificates/Trusted/certs  usr share OSIsoft EdgeDataStore {ComponentId} Certificates Trusted certs Adapter\u0027s certificate Windows: %programdata%\\OSIsoft\\EdgeDataStore\\{ComponentId}\\Certificates\\My\\certs Linux: /usr/share/OSIsoft/EdgeDataStore/{ComponentId}/Certificates/My/certs  usr share OSIsoft EdgeDataStore {ComponentId} Certificates My certs"
                                                                  },
    "content/opc-ua/opc-ua-data-selection.html":  {
                                                      "href":  "content/opc-ua/opc-ua-data-selection.html",
                                                      "title":  "Generate default OPC UA data selection configuration file",
                                                      "keywords":  "Generate default OPC UA data selection configuration file When you add a data source, the OPC UA EDS adapter browses the entire OPC UA server address space and exports the available OPC UA variables into a JSON file for data selection. Data is collected automatically based upon user demands. OPC UA data from OPC UA variables is read through subscriptions (unsolicited reads). A default OPC UA data selection file will be created if there is no OPC UA data selection configuration, but a valid OPC UA data source exists. Note: Generating the default data selection file can tax system resources. To avoid possibly expensive browse operations, OSIsoft recommends that you manually create a data selection file instead of generating the default data selection file. For more information, see Data selection configuration . To generate the default data selection file: Add an OPC UA EDS adapter with a unique ComponentId. During the installation of Edge Data Store, enabling the OPC UA EDS adapter results in addition of a unique component that also satisfies this condition. Configure a valid OPC UA data source . Once you complete these steps, a default OPC UA data selection configuration file will be generated in the configuration directory for the corresponding platform. The following are example locations of the file created. In this example, it is assumed that the ComponentId of the OPC UA component is the default OpcUa1: Windows: %programdata%\\OSIsoft\\EdgeDataStore\\Configuration\\OpcUa1_DataSelection.json Linux: /usr/share/OSIsoft/EdgeDataStore/Configuration/OpcUa1_DataSelection.json  usr share OSIsoft EdgeDataStore Configuration OpcUa1_DataSelection.json Copy the file to a different directory. The contents of the file will look something like: [ { \"Selected\": false, \"Name\": \"Cold Side Inlet Temperature\", \"NodeId\": \"ns=2;s=Line1.HeatExchanger1001.ColdSideInletTemperature\", \"StreamId\": null }, { \"Selected\": false, \"Name\": \"Cold Side Outlet Temperature\", \"NodeId\": \"ns=2;s=Line1.HeatExchanger1001.ColdSideOutletTemperature\", \"StreamId\": null } ] In a text editor, edit the file and change the value of any Selected key from false to true in order to transfer the OPC UA data to be stored in Edge Data Store. In the same directory where you edited the file, run the following curl command: curl -i -d \"@OpcUa1_DataSelection.json\" -H \"Content-Type: application/json\" application json\" -X PUT http://localhost:5590/api/v1/configuration/OpcUa1/Dataselection http:  localhost:5590 api v1 configuration OpcUa1 Dataselection"
                                                  },
    "content/opc-ua/opc-ua-data-selection-configuration.html":  {
                                                                    "href":  "content/opc-ua/opc-ua-data-selection-configuration.html",
                                                                    "title":  "Data selection configuration",
                                                                    "keywords":  "Data selection configuration After configuring the data source, create a data selection configuration file to specify the data for the OPC UA EDS adapter instance to collect from the data source. When you add a data source, the OPC UA EDS adapter browses the OPC UA server address space and exports the available OPC UA variables into a JSON file for data selection. If RootNodeIds are specified in the data source configuration, only those nodeIds are browsed. Data is collected automatically based upon user demands. OPC UA data from OPC UA variables is read through subscriptions (unsolicited reads). You can either have the data selection configuration file generated for you or you can create it manually yourself. Configure OPC UA data selection using a generated file A default OPC UA data selection file will be created if there is no OPC UA data selection configuration, but a valid OPC UA data source exists. Note: To avoid resource-intensive browse operations, OSIsoft recommends that you manually create a data selection file instead of generating the default data selection file. To generate the default data selection file and use it to configure data selection: Add an OPC UA EDS adapter instance with a unique ComponentId either manually or during the EDS installation. For details, see Edge Data Store configuration . Configure a valid OPC UA data source. For details, see Data source configuration . Once you complete these steps, a default OPC UA data selection configuration file is generated in the configuration directory with the file name based on the ComponentId. The following are example locations of the file created using the adapter instance created during installation, which is OpcUa1: Windows: %programdata%\\OSIsoft\\EdgeDataStore\\Configuration\\OpcUa1DataSelection.json.json Linux: /usr/share/OSIsoft/EdgeDataStore/Configuration/OpcUa1DataSelection.json.json  usr share OSIsoft EdgeDataStore Configuration OpcUa1DataSelection.json.json Copy the file to a different directory and open it using any text editor. It will look similar to the following example: [ { \"Selected\": false, \"Name\": \"Cold Side Inlet Temperature\", \"NodeId\": \"ns=2;s=Line1.HeatExchanger1001.ColdSideInletTemperature\", \"StreamId\": null }, { \"Selected\": false, \"Name\": \"Cold Side Outlet Temperature\", \"NodeId\": \"ns=2;s=Line1.HeatExchanger1001.ColdSideOutletTemperature\", \"StreamId\": null } ] To ingress a stream to Edge Data Store, change the value of the Selected key from false to true . All streams in the auto generated data selection file are initially set to false . Save the file. Run the following curl command or EdgeCmd from the directory where the file is located, updating the file name and destination in the script if needed: curl EdgeCmd curl -d \"@OpcUa1DataSelection.json\" -H \"Content-Type: application/json\" application json\" -X PUT http://localhost:5590/api/v1/configuration/OpcUa1/Dataselection http:  localhost:5590 api v1 configuration OpcUa1 Dataselection edgecmd set dataSelection -cid OpcUa1 -file OpcUa1DataSelection.json Configure OPC UA data selection by manually creating the file Note: OPC UA data selection configurations cannot be modified manually. Use the REST endpoints to add or edit the configuration. To configure the OPC UA data selection: Using any text editor, create a file that contains an OPC UA data selection in JSON form. For content structure, see OPC UA data selection example . Update the parameters as needed. For a table of all available parameters, see Parameters for OPC UA data selection . Save the file to the device with Edge Data Store installed with the name OpcUa1DataSelection.json . Use any tool capable of making HTTP requests to execute a POST command with the contents of that file to the following endpoint: http://localhost:\u003cport_number\u003e/api/v1/configuration/\u003cEDS http:  localhost:\u003cport_number\u003e api v1 configuration \u003cEDS adapterId\u003e/DataSelection/ adapterId\u003e DataSelection  The following examples show the HTTPS request using curl and EdgeCmd, which must be run from the same directory where the file is located, and uses the adapter instance created during installation, which is OpcUa1: curl EdgeCmd curl -d \"@OpcUa1DataSelection.json\" -H \"Content-Type: application/json\" application json\" -X PUT http://localhost:5590/api/v1/configuration/OpcUa1/Dataselection http:  localhost:5590 api v1 configuration OpcUa1 Dataselection edgecmd set dataSelection -cid OpcUa1 -file OpcUa1DataSelection.json Parameters for OPC UA data selection The following parameters can be used to configure OPC UA data selection. Parameter Required Type Nullable Description Selected Optional Boolean No Use this field to select a measurement to be collected. To select an item, set to true. To remove an item and not collect its data, leave the field empty or set to false. If not configured, the default value is false. Name Required string Yes The friendly name of the data item collected from the data source. The field is populated with the DisplayName value from the OPC UA server when the data selection configuration is populated by the adapter. NodeId Required string Yes The NodeId of the variable. StreamID Optional string Yes The custom stream ID used to create the streams. If not specified, the OPC UA EDS adapter will generate a default stream ID based on the measurement configuration. A properly configured custom stream ID follows these rules: Is not case-sensitive. Can contain spaces. Cannot start with two underscores (\"__\"). Can contain a maximum of 100 characters. Cannot use the following characters: /   : ? # [ ] @ ! $ \u0026 \u0027 ( ) \\ * + , ; = % \u003c \u003e | Cannot start or end with a period. Cannot contain consecutive periods. Cannot consist of only periods. OPC UA data selection example The following is an example of valid OPC UA data selection configuration: [ { \"Selected\": true, \"Name\": \"Random1\", \"NodeId\": \"ns=5;s=Random1\", \"StreamId\": \"CustomStreamName\" }, { \"Selected\": false, \"Name\": \"Sawtooth1\", \"NodeId\": \"ns=5;s=Sawtooth1\", \"StreamId\": null }, { \"Selected\": true, \"Name\": \"Sinusoid1\", \"NodeId\": \"ns=5;s=Sinusoid1\", \"StreamId\": null } ]"
                                                                },
    "content/opc-ua/opc-ua-overview.html":  {
                                                "href":  "content/opc-ua/opc-ua-overview.html",
                                                "title":  "OPC UA EDS adapter",
                                                "keywords":  "OPC UA EDS adapter Overview The OPC UA EDS adapter is a component of Edge Data Store that transfers time-series data from an OPC UA capable device to EDS. OPC Unified Architecture (OPC UA) is a machine to machine communication protocol for industrial automation developed by the OPC Foundation. The OPC UA EDS adapter can connect to any device that uses the OPC UA communication protocol, and it uses subscriptions to reduce the amount of data transferred by only reading data changes and events. The following diagram depicts the data flow for a single instance of OPC UA EDS adapter instance: The adapter instance polls the OPC UA device and then collects data from the device. The adapter then sends the data to the storage component where it is held until it can be egressed to permanent storage in PI Server or AVEVA Data Hub. You can configure the adapter instance from the device where EDS is installed. EDS also collects health information about the adapter that can be egressed. The OPC UA EDS adapter can connect to multiple devices by defining one instance of the adapter for each device. The EDS installation includes the OPC UA EDS adapter and the option to add a single OPC UA EDS adapter instance. Add additional instances after installation using system components configuration. Once an adapter instance is defined, manually configure it with JSON documents that specify the following: Data source configuration - identifies the device from which the data originates, specifies the security for the connection, and controls how the data streams from that device identified. Each adapter component instance requires a data source configuration. Data selection configuration - specifies what data is collected from the device and how it is identified. Each adapter component instance requires a data selection configuration. With these configurations completed, the OPC UA EDS adapter instance collects data from the specified device and sends it to EDS, where it is stored locally until it can be sent to a PI System or AVEVA Data Hub for long-term storage and analysis."
                                            },
    "content/opc-ua/operational-overview-opc-ua.html":  {
                                                            "href":  "content/opc-ua/operational-overview-opc-ua.html",
                                                            "title":  "Operational overview",
                                                            "keywords":  "Operational overview The OPC UA EDS adapter conforms to the OPC UA specification for operation. Once an instance of the adapter is defined in the system components configuration, it must be configured for it to create streams and collect data. Adapter configuration For an OPC UA EDS adapter instance to start data collection, configure the adapter by defining the following: Data source - Provide the connection information for the OPC UA data source. Data selection - Specify the OPC UA items to which the adapter instance should subscribe for data. Logging - Set up the logging behavior for the adapter instance. For more information, see Data source configuration and Data selection configuration . For more information on how to configure logging, see Component-level logging configuration . Connection The OPC UA EDS adapter uses the binary opc.tcp protocol to communicate with the OPC UA servers. A secured connection is enabled by default where the X.509-type client and server certificates are exchanged and verified and the connection between the OPC UA EDS adapter and the configured OPC UA server is established. Stream creation The OPC UA EDS adapter creates types upon receiving the value update for a stream. One stream is created in Edge Data Store for every selected OPC UA item in the data selection configuration. Data collection The OPC UA EDS adapter collects time-series data from selected OPC UA dynamic variables through OPC UA subscriptions (unsolicited reads). The adapter supports the Data Access (DA) part of OPC UA specification. Stream properties The OPC UA EDS adapter creates a stream with two properties per selected OPC UA item. The properties are described in the following table. Property name Data type Description Timestamp DateTime Timestamp of the given OPC UA item value update. Value Based on type of incoming OPC UA value Value of the given OPC UA item update. Stream ID is a unique identifier for each stream created by the adapter for a given OPC UA item. If the Custom Stream ID is specified for the OPC UA item in data selection configuration, the OPC UA EDS adapter uses that as a stream ID for the stream. Otherwise, the adapter constructs the stream ID using the following format constructed from the OPC UA item node ID: \u003cAdapter Component ID\u003e.\u003cNamespace\u003e.\u003cIdentifier\u003e Note: The naming convention is affected by StreamIdPrefix and ApplyPrefixToStreamID settings in data source configuration. For more information, see Data source configuration . Export operation The OPC UA EDS adapter is able to export available OPC UA dynamic variables by browsing the OPC UA hierarchies or sub-hierarchies as part of the data source configuration process. For more information, see Data source configuration . Buffering Because the OPC UA EDS adapter sends data directly to EDS, buffering capability is not provided. EDS acts as a buffer before the data is egressed to either a PI Server or AVEVA Data Hub. The amount of data stored in EDS is controlled by the following storage parameters: StreamStorageLimitMb StreamStorageTargetMb For more information about configuring data storage in EDS, see Storage runtime configuration ."
                                                        },
    "content/opc-ua/README.html":  {
                                       "href":  "content/opc-ua/README.html",
                                       "title":  "content/opc-ua/shared-content subtree",
                                       "keywords":  "content/opc-ua/shared-content content opc-ua shared-content subtree All content in this directory is consumed from the PI Adapter OPC-UA Docs repository as a subtree. This repo contains the unique OPC-UA documents used in EDS. Note that the OPC-UA document also makes use of the PI Adapter framework documents stored in content/shared-content content shared-content . Subtree wiki article To update the subtree To update content in content/opc-ua/shared-content content opc-ua shared-content , perform the following actions from a command prompt session: Enter the following command. git subtree pull --prefix content/opc-ua/shared-content content opc-ua shared-content https://github.com/osisoft/pi-adapter-opc-ua-Docs https:  github.com osisoft pi-adapter-opc-ua-Docs main --squash Resolve any conflicts and complete the merge. Remember: The OPC-UA docs include the PI Adapter framework. Delete any files that are not unique to OPC-UA and are redundant to the framework. Which branch of the PI Adapter OPC-UA docs is EDS consuming? It\u0027s currently consuming main . It should continue to consume main until another use case arises. What if I want to consume a different branch of the PI Adapter OPC-UA docs? If you want to swap out the most recent version of the PI Adapter OPC-UA docs for a specific version, update the branch parameter in the git subtree pull command: git subtree pull --prefix content/opc-ua/shared-content content opc-ua shared-content https://github.com/osisoft/pi-adapter-opc-ua-Docs https:  github.com osisoft pi-adapter-opc-ua-Docs \u003cCUSTOM_BRANCH\u003e --squash"
                                   },
    "content/opc-ua/shared-content/content/configuration/data-filters.html":  {
                                                                                  "href":  "content/opc-ua/shared-content/content/configuration/data-filters.html",
                                                                                  "title":  "Data filters",
                                                                                  "keywords":  "Data filters You can configure PI adapters to perform data filtering to save network bandwidth. Every data item in the data selection configuration can be assigned the Id of a data filter. The adapter filters data for those data items based on the data filter configuration. Note: If you enable data filters and data quality changes, both the old and current data quality values are passed on. Configure data filters Complete the following steps to configure data filters. Use the PUT method in conjunction with the http://localhost:5590/api/v1/configuration/\u003cComponentId\u003e/DataFilters http:  localhost:5590 api v1 configuration \u003cComponentId\u003e DataFilters REST endpoint to initialize the configuration. Using a text editor, create an empty text file. Copy and paste an example configuration for data filters into the file. For sample JSON, see Data filters example . Update the example JSON parameters for your environment. For a table of all available parameters, see Data filters parameters . Save the file. For example, as ConfigureDataFilters.json . Open a command line session. Change directory to the location of ConfigureDataFilters.json . Enter the following curl command or EdgeCmd to initialize the data filters configuration. curl EdgeCmd curl -d \"@ConfigureDataFilters.json\" -H \"Content-Type: application/json\" application json\" -X PUT \"http://localhost:5590/api/v1/configuration/OpcUa1/DataFilters\" \"http:  localhost:5590 api v1 configuration OpcUa1 DataFilters\" edgecmd set dataFilters -cid OpcUa1 -file ConfigureDataFilters.json Note: If you installed the adapter to listen on a non-default port, update 5590 to the port number in use. For a list of other REST operations you can perform, like updating or deleting a data filters configuration, see REST URLs . On successful execution, the change that you have made to data filters takes effect immediately during runtime. Data filters parameters The following parameters are available for configuring data filters: Parameter Required Type Description Id Required string Unique identifier for the data filter. Allowed value: any string identifier AbsoluteDeadband Optional double Specifies the absolute change in data value that should cause the current value to pass the filter test. Note: You must specify AbsoluteDeadband or PercentChange . Allowed value: double value representing absolute deadband number Default value: null PercentChange Optional double Specifies the percent change from previous value that should cause the current value to pass the filter test. Note: You must specify AbsoluteDeadband or PercentChange . Allowed value: double value representing percent change Default value: null ExpirationPeriod Optional timespan The length in time that can elapse after an event before automatically sending the next event, regardless of whether the next event passes the filter or not. The expected format is HH:MM:SS.### or SSS.* Allowed value: any timespan Default value: null * Note: For example, \"ExpirationPeriod\": 5:00 and \"ExpirationPeriod\": 300 both specify an expiration period of 5 minutes and 0 seconds. Data filters example [ { \"Id\": \"DuplicateData\", \"AbsoluteDeadband\": 0, \"PercentChange\": null, \"ExpirationPeriod\": \"01:00:00\" } ] REST URLs Relative URL HTTP verb Action api/v1/configuration/ api v1 configuration  ComponentId /DataFilters  DataFilters GET Gets all configured data filters. api/v1/configuration/ api v1 configuration  ComponentId /DataFilters  DataFilters DELETE Deletes all configured data filters. api/v1/configuration/ api v1 configuration  ComponentId /DataFilters  DataFilters POST Adds an array of data filters or a single data filter. Fails if any data filter already exists. api/v1/configuration/ api v1 configuration  ComponentId /DataFilters  DataFilters PUT Replaces all data. api/v1/configuration/ api v1 configuration  ComponentId /DataFilters  DataFilters PATCH Allows partial updating of configured data filter. api/v1/configuration/ api v1 configuration  ComponentId /DataFilters/  DataFilters  id GET Gets configured data filter by Id . api/v1/configuration/ api v1 configuration  ComponentId /DataFilters/  DataFilters  id DELETE Deletes configured data filter by Id . api/v1/configuration/ api v1 configuration  ComponentId /DataFilters/  DataFilters  id PUT Replaces data filter by Id . Fails if data filter does not exist. Note: Replace ComponentId with the Id of your adapter component."
                                                                              },
    "content/opc-ua/shared-content/content/configuration/data-source.html":  {
                                                                                 "href":  "content/opc-ua/shared-content/content/configuration/data-source.html",
                                                                                 "title":  "Data source configuration",
                                                                                 "keywords":  "Data source configuration For each instance of the OPC UA EDS adapter defined in system configuration, you must configure the data source from which it will receive data. Configure OPC UA data source Note: You cannot manually configure OPC UA data source configurations. You must use the REST endpoints to add or edit the configuration. To configure the OPC UA data source: Using any text editor, create a file that contains an OPC UA data source in JSON format. For content structure, see OPC UA data source examples . Modify the parameters in the example to match your environment. For a table of all available parameters, see Parameters for OPC UA data source . Save the file to the device with EDS installed using a file name based on the adapter instance name. For example, to use the adapter instance created during installation, which is OpcUa1, name the file OpcUa1Datasource.json . Use any tool capable of making HTTP requests to execute a POST command with the contents of that file to the following endpoint: http://localhost:\u003cport_number\u003e/api/v1/configuration/\u003cEDS_adapterId\u003e/DataSource/ http:  localhost:\u003cport_number\u003e api v1 configuration \u003cEDS_adapterId\u003e DataSource  . The following examples show the HTTPS request using curl and EdgeCmd using the adapter instance created during installation, which is OpcUa1 : curl EdgeCmd curl -d \"@OpcUa1DataSource..json\" -H \"Content-Type: application/json\" application json\" \"http://localhost:5590/api/v1/configuration/OpcUa1/DataSource\" \"http:  localhost:5590 api v1 configuration OpcUa1 DataSource\" edgecmd set dataSource -cid OpcUa1 -file OpcUa1DataSource..json Note: After completing data source configuration, you need to configure data selection next. You can either generate a default data selection file or create the data selection file manually. For more information, see Data selection configuration . Export OPC UA dynamic variables The OPC UA EDS adapter is able to export available OPC UA dynamic variables by browsing the OPC UA hierarchies or sub-hierarchies as part of the data source configuration process. To limit browsing, specify a comma-separated collection of nodeIds in data source configuration file using the RootNodeIds parameter. Note: The nodeIds are treated as roots from which the adapter starts the browse operation. The adapter triggers an export operation after a successful connection to the OPC UA server when the data selection file does not exist in configuration directory. Copy the exported data selection JSON file from the directory or retrieve it using a REST API call. (Optional) To avoid a potentially long and resource-intensive browse operation, create the data selection file manually. Configure it before you configure the data source or push both in one configuration call together. Parameters for OPC UA data source The following parameters are available for configuring an OPC UA data source: Parameter Required Type Description endpointURL Required string The endpoint URL of the OPC UA server in opc.tcp format. The following is an example of the URL format: opc.tcp://OPCServerHost:Port/OpcUa/SimulationServer opc.tcp:  OPCServerHost:Port OpcUa SimulationServer Note: If you change the EndpointURL on a configured adapter that has ComponentID_DataSelection.json file exported, you need to remove the ComponentID_DataSelection.json file from the configuration directory to trigger a new browse (export). Allowed value: well-formed opc.tcp address useSecureConnection Optional boolean When set to true, the adapter connects to a secure endpoint using OPC UA certificate exchange operation. The default is true. When set to false, the adapter connects to an unsecured endpoint of the server and certificate exchange operation is not required. Note: We recommend setting this option to false for testing purposes only. Allowed value: true or false Default value: true userName Optional string User name for accessing the OPC UA server. Allowed value: any string Default value: null password Optional string Password for accessing the OPC UA server. Note: We recommend using REST to configure the data source when the password must be specified because REST will encrypt the password. If you do not use REST, the plain text password is stored on-disk. Allowed value: any string Default value: null incomingTimestamp Optional string Specifies whether the incoming timestamp is taken from the source, from the OPC UA server, or should be created by the adapter instance. - Source - Default and recommended setting. The timestamp is taken from the source timestamp field. The source is what provides data for the item to the OPC UA server, such as a field device. - Server - In case the OPC UA item has an invalid source timestamp field, the Server timestamp can be used. - Adapter - The adapter generates a timestamp for the item upon receiving it from the OPC UA server. Allowed value: Source , Server , or Adapter Default value: Source streamIdPrefix Optional string Specifies what prefix is used for Stream IDs. The naming convention is {StreamIdPrefix}{StreamId} . An empty string means no prefix will be added to the Stream IDs and names. A null value defaults to ComponentID followed by a period. Example: OpcUa1.{NamespaceIndex}.{Identifier} Note: Every time you change the StreamIdPrefix of a configured adapter, for example when you delete and add a data source, you need to restart the adapter for the changes to take place. New streams are created on adapter restart and pre-existing streams are no longer updated. Allowed value: any string Default value: null defaultStreamIdPattern Optional string Specifies the default stream Id pattern to use. Possible parameters: {NamespaceIndex} 1 , {Identifier} . Allowed value: any string Default value: {NamespaceIndex}.{Identifier} dataCollectionMode Optional string Specifies the data collection mode the adapter is in. The following data collection modes are available: HistoryOnly 2 : The adapter component does not get started and history recovery on-demand is enabled. For more information, see On-demand history recovery configuration . CurrentOnly : The adapter component operates normally and on-demand history recovery is disabled. CurrentWithBackfill 2 - The adapter component operates normally, but disconnections are recorded based on device status. History recovery backfills data once device status is good . On-demand history recovery is disabled. For more information, see Automatic history recovery . Changing the mode requires a restart of the adapter component. 1 NamespaceIndex refers to the number specified in the ns keyword in the RootNodeIds parameter. 2 Historical values are collected only for OPC UA items that have the AccessLevel attribute set to HistoryRead . OPC UA data source examples The following are examples of valid OPC UA data source configurations: Minimal data source configuration { \"endpointUrl\": \"opc.tcp://\u003cIP-Address\u003e:\u003cPort\u003e/\u003cTestOPCUAServer\u003e\" \"opc.tcp:  \u003cIP-Address\u003e:\u003cPort\u003e \u003cTestOPCUAServer\u003e\" } Complete data source configuration { \"endpointUrl\": \"opc.tcp://\u003cIP-Address\u003e:\u003cPort\u003e/\u003cTestOPCUAServer\u003e\", \"opc.tcp:  \u003cIP-Address\u003e:\u003cPort\u003e \u003cTestOPCUAServer\u003e\", \"useSecureConnection\": true, \"userName\": null, \"password\": null, \"incomingTimestamp\": \"Source\", \"streamIdPrefix\": null, \"defaultStreamIdPattern\": \"{NamespaceIndex}.{Identifier}\", \"dataCollectionMode\": \"CurrentWithBackfill\" } REST URLs Relative URL HTTP verb Action api/v1/configuration/\u003cComponentId\u003e/DataSource api v1 configuration \u003cComponentId\u003e DataSource GET Retrieves the data source configuration. api/v1/configuration/\u003cComponentId\u003e/DataSource api v1 configuration \u003cComponentId\u003e DataSource POST Creates the data source configuration. The adapter starts collecting data after the following conditions are met: ??? The data source configuration POST request is received. ??? A data selection configuration is active. api/v1/configuration/\u003cComponentId\u003e/DataSource api v1 configuration \u003cComponentId\u003e DataSource PUT Configures or updates the data source configuration. Overwrites any active data source configuration. If no configuration is active, the adapter starts collecting data after the following conditions are met: ??? The data source configuration PUT request is received. ??? A data selection configuration is active. api/v1/configuration/\u003cComponentId\u003e/DataSource api v1 configuration \u003cComponentId\u003e DataSource DELETE Deletes the data source configuration. After the request is received, the adapter stops collecting data. Note: Replace ComponentId with the Id of your OPC UA component, for example OpcUa1 ."
                                                                             },
    "content/opc-ua/shared-content/content/main/shared-content/_includes/inline/product-protocol.html":  {
                                                                                                             "href":  "content/opc-ua/shared-content/content/main/shared-content/_includes/inline/product-protocol.html",
                                                                                                             "title":  "",
                                                                                                             "keywords":  ""
                                                                                                         },
    "content/opc-ua/shared-content/content/main/shared-content/introduction/intro-to-pi-adapters.html":  {
                                                                                                             "href":  "content/opc-ua/shared-content/content/main/shared-content/introduction/intro-to-pi-adapters.html",
                                                                                                             "title":  "\u003c!-- BEGIN ERROR INCLUDE: Unable to resolve [!include[product-name](../_includes/inline/product-name.md)]: Couldn\u0027t find file content/opc-ua/shared-content/content/main/shared-content/_includes/inline/product-name.md. --\u003e[!include[product-name](../_includes/inline/product-name.md)]\u003c!--END ERROR INCLUDE --\u003e \u003c!-- BEGIN ERROR INCLUDE: Unable to resolve [!include[product-version](../_includes/inline/product-version.md)]: Couldn\u0027t find file content/opc-ua/shared-content/content/main/shared-content/_includes/inline/product-version.md. --\u003e[!include[product-version](../_includes/inline/product-version.md)]\u003c!--END ERROR INCLUDE --\u003e",
                                                                                                             "keywords":  "\u003c!-- BEGIN ERROR INCLUDE: Unable to resolve [!include[product-name](../_includes/inline/product-name.md)]: [!include[product-name](.. _includes inline product-name.md)]: Couldn\u0027t find file content/opc-ua/shared-content/content/main/shared-content/_includes/inline/product-name.md. content opc-ua shared-content content main shared-content _includes inline product-name.md. --\u003e [!include[product-name](../_includes/inline/product-name.md)] [!include[product-name](.. _includes inline product-name.md)] \u003c!--END ERROR INCLUDE --\u003e \u003c!-- BEGIN ERROR INCLUDE: Unable to resolve [!include[product-version](../_includes/inline/product-version.md)]: [!include[product-version](.. _includes inline product-version.md)]: Couldn\u0027t find file content/opc-ua/shared-content/content/main/shared-content/_includes/inline/product-version.md. content opc-ua shared-content content main shared-content _includes inline product-version.md. --\u003e [!include[product-version](../_includes/inline/product-version.md)] [!include[product-version](.. _includes inline product-version.md)] \u003c!--END ERROR INCLUDE --\u003e \u003c!-- BEGIN ERROR INCLUDE: Unable to resolve [!include[product-name](../_includes/inline/product-name.md)]: [!include[product-name](.. _includes inline product-name.md)]: Couldn\u0027t find file content/opc-ua/shared-content/content/main/shared-content/_includes/inline/product-name.md. content opc-ua shared-content content main shared-content _includes inline product-name.md. --\u003e [!include[product-name](../_includes/inline/product-name.md)] [!include[product-name](.. _includes inline product-name.md)] \u003c!--END ERROR INCLUDE --\u003e is a data collection technology that collects time-series operations data from a data source over the protocol and then sends it to a supported storage location in the Open Message Format (OMF). \u003c!-- Add content about the protocol here --\u003e \u003c!-- BEGIN ERROR INCLUDE: Unable to resolve [!include[product-name](../_includes/inline/product-name.md)]: [!include[product-name](.. _includes inline product-name.md)]: Couldn\u0027t find file content/opc-ua/shared-content/content/main/shared-content/_includes/inline/product-name.md. content opc-ua shared-content content main shared-content _includes inline product-name.md. --\u003e [!include[product-name](../_includes/inline/product-name.md)] [!include[product-name](.. _includes inline product-name.md)] \u003c!--END ERROR INCLUDE --\u003e data flow The following diagram depicts the collection and processing of data for an operational \u003c!-- BEGIN ERROR INCLUDE: Unable to resolve [!include[product-name](../_includes/inline/product-name.md)]: [!include[product-name](.. _includes inline product-name.md)]: Couldn\u0027t find file content/opc-ua/shared-content/content/main/shared-content/_includes/inline/product-name.md. content opc-ua shared-content content main shared-content _includes inline product-name.md. --\u003e [!include[product-name](../_includes/inline/product-name.md)] [!include[product-name](.. _includes inline product-name.md)] \u003c!--END ERROR INCLUDE --\u003e , collecting and processing data. Refer to the list below the diagram for more information on each depicted callout . \u003c!-- Mark Bishop 3/3/22: 3 3 22: The SVG file referenced below can be opened and edited using https://app.diagrams.net/ https:  app.diagrams.net  --\u003e The user installs and configures \u003c!-- BEGIN ERROR INCLUDE: Unable to resolve [!include[product-name](../_includes/inline/product-name.md)]: [!include[product-name](.. _includes inline product-name.md)]: Couldn\u0027t find file content/opc-ua/shared-content/content/main/shared-content/_includes/inline/product-name.md. content opc-ua shared-content content main shared-content _includes inline product-name.md. --\u003e [!include[product-name](../_includes/inline/product-name.md)] [!include[product-name](.. _includes inline product-name.md)] \u003c!--END ERROR INCLUDE --\u003e on a host system. You can configure the adapter using either a REST interface or EdgeCmd, a command line utility specifically designed for interfacing with edge systems. The adapter collects data from assets over the protocol, a process known as data ingress . The adapter converts ingress data to the Open Message Format (OMF), a format that supported storage locations understand. The adapter sends OMF data to a supported storage location in a process known as data egress . Supported egress endpoints include: PI Server AVEVA Data Hub"
                                                                                                         },
    "content/opc-ua/supported-features-opc-ua.html":  {
                                                          "href":  "content/opc-ua/supported-features-opc-ua.html",
                                                          "title":  "Data type mapping",
                                                          "keywords":  "Data type mapping The following table lists OPC UA data types from which the OPC UA EDS adapter supports data collection and corresponding stream data types that will be created in Edge Data Store. OPC UA data type Stream data type Boolean Boolean Byte Int16 SByte Int16 Int16 Int16 UInt16 UInt16 Int32 Int32 UInt32 UInt32 Int64 Int64 UInt64 UInt64 Float Float32 Double Float64 DateTime DateTime String String"
                                                      },
    "content/overview/adh-egress-quick-start.html":  {
                                                         "href":  "content/overview/adh-egress-quick-start.html",
                                                         "title":  "AVEVA Data Hub egress quick start",
                                                         "keywords":  "AVEVA Data Hub egress quick start Data egress provides a mechanism to transfer data to AVEVA Data Hub using OMF messages. To get started sending data stored in Edge Data Store to AVEVA Data Hub, prepare the destination and configure the EgressEndpoints facet to use that destination. AVEVA Data Hub destinations To prepare AVEVA Data Hub to receive OMF messages from EDS, create an OMF connection in AVEVA Data Hub. Creating an OMF connection results in an available OMF endpoint that EDS can use for the egress mechanism. To create an OMF connection to AVEVA Data Hub: In AVEVA Data Hub, create a client. For details, see Clients in the AVEVA Data Hub documentation. The Client Id and Client Secret are used for the corresponding properties in the EgressEndpoints configuration. In AVEVA Data Hub, create an OMF connection. For details, see Configure an OMF connection in the AVEVA Data Hub documentation. The OMF Endpoint URL for the connection is used as the EgressEndpoints configuration Endpoint parameter. Create an egress configuration To configure Edge Data Store periodic egress to AVEVA Data Hub, follow the steps in Configure periodic data egress . To configure Edge Data Store manual egress to AVEVA Data Hub, follow the steps in Configure manual data egress ."
                                                     },
    "content/overview/analytics-quick-start.html":  {
                                                        "href":  "content/overview/analytics-quick-start.html",
                                                        "title":  "Edge Data Store analytics quick start",
                                                        "keywords":  "Edge Data Store analytics quick start This topic provides a quick start for a very simple analytic you can write using Edge Data Store. The intended input device is an Modbus TCP EDS adapter or other sensor that outputs 4 Boolean values. The normal range of operation is that the values are neither all true or all false. If all values are true, the exception condition High is triggered. If all values are false, the exception condition Low is triggered. Any other combination of Boolean values is Normal. Three analytic streams are created to track these changes. The ValueRangeHigh stream is 1 when High and 0 when anything else. The ValueRangeLow stream is -1 when Low and 0 when anything else. The ValueRangeOut stream is -1 when Low, 0 when Normal, and 1 when High. This example assumes Edge Data Store was installed with the default port (5590): using System; using System.Collections.Generic; using System.Net.Http; using System.Text; using Newtonsoft.Json; using Newtonsoft.Json.Linq; namespace ExceptionReportingSample { class ModbusField { public string StreamId { get; set; } public int ScanRate { get; set; } } enum Alert { Normal, High, Low } class ExceptionReporting { static HttpClient _client = new HttpClient(); private static List\u003cstring\u003e StreamIds = new List\u003cstring\u003e(new string[] { \"SwitchState1\", \"SwitchState2\", \"SwitchState3\", \"SwitchState4\" }); private const string ValueRangeHigh = \"ValueRangeHigh\"; private const string ValueRangeLow = \"ValueRangeLow\"; private const string ValueRangeOut = \"ValueRangeOut\"; private const string TypeId = \"ValueRange\"; private const string ModbusComponentId = \"Modbus1\"; private const double lowValue = -1.0; private const double highValue = 1.0; private const double normalValue = 0.0; public static Alert _alert = Alert.Normal; static TimeSpan GetPollingIntervalFromModbus(string modbusComponentId, List\u003cstring\u003e StreamIds) { int pollingMilliseconds = 5000; string endpoint = $\"http://localhost:5590/api/v1/configuration/{modbusComponentId}/DataSelection\"; $\"http:  localhost:5590 api v1 configuration {modbusComponentId} DataSelection\"; string modbusConfig = _client.GetStringAsync(endpoint).Result; List\u003cModbusField\u003e values = JsonConvert.DeserializeObject\u003cList\u003cModbusField\u003e\u003e(modbusConfig); foreach (var value in values) { foreach (string StreamId in StreamIds) { if (StreamId == value.StreamId \u0026\u0026 value.ScanRate \u003c pollingMilliseconds) { pollingMilliseconds = value.ScanRate; } } } return TimeSpan.FromMilliseconds(pollingMilliseconds); } static bool GetStreamValue(string StreamId) { bool value = false; string lastValueUri = string.Format(\"http://localhost:5590/api/v1/tenants/default/namespaces/default/streams/{0}/Data/Last\", string.Format(\"http:  localhost:5590 api v1 tenants default namespaces default streams {0} Data Last\", StreamId); string lastValueJson = _client.GetStringAsync(lastValueUri).Result; Dictionary\u003cstring, object\u003e values = JsonConvert.DeserializeObject\u003cDictionary\u003cstring, object\u003e\u003e(lastValueJson); object objValue = values[\"Value\"]; if (objValue is Boolean) value = (bool)objValue; return value; } static bool FindOrCreateType(string typeId) { string typeUri = string.Format(\"http://localhost:5590/api/v1/tenants/default/namespaces/default/types/{0}\", string.Format(\"http:  localhost:5590 api v1 tenants default namespaces default types {0}\", typeId); HttpResponseMessage response = _client.GetAsync(typeUri).Result; if (response.IsSuccessStatusCode) return true; string typeJson = @\"{\"\"Id\"\": \"\"\" + typeId + @\"\"\",\"\"Name\"\": \"\"\" + typeId + @\"\"\",\"\"SdsTypeCode\"\": 1,\"\"Properties\"\": [{\"\"Id\"\": \"\"Time\"\",\"\"Name\"\": \"\"Time\"\",\"\"IsKey\"\": true,\"\"SdsType\"\": {\"\"SdsTypeCode\"\": 16}},{\"\"Id\"\": \"\"Measurement\"\",\"\"Name\"\": \"\"Measurement\"\",\"\"SdsType\"\": {\"\"SdsTypeCode\"\": 14}}]}\"; var content = new StringContent(typeJson, Encoding.UTF8, \"application/json\"); \"application json\"); response = _client.PostAsync(typeUri, content).Result; return response.IsSuccessStatusCode; } static bool FindOrCreateStream(string streamId, string typeId) { string streamUri = string.Format(\"http://localhost:5590/api/v1/tenants/default/namespaces/default/streams/{0}/\", string.Format(\"http:  localhost:5590 api v1 tenants default namespaces default streams {0} \", streamId); HttpResponseMessage response = _client.GetAsync(streamUri).Result; if (response.IsSuccessStatusCode) return true; string streamJson = @\"{\"\"Id\"\": \"\"\" + streamId + @\"\"\",\"\"Name\"\": \"\"\" + streamId + @\"\"\",\"\"TypeId\"\": \"\"\" + typeId + @\"\"\"}\"; var content = new StringContent(streamJson, Encoding.UTF8, \"application/json\"); \"application json\"); response = _client.PostAsync(streamUri, content).Result; return response.IsSuccessStatusCode; } static bool WriteStreamValue(string StreamId, double value, DateTime timestamp) { string dataUri = string.Format(\"http://localhost:5590/api/v1/tenants/default/namespaces/default/streams/{0}/Data\", string.Format(\"http:  localhost:5590 api v1 tenants default namespaces default streams {0} Data\", StreamId); string dataJson = @\"[{\"\"Time\"\": \"\"\" + timestamp.ToString(\"o\") + @\"\"\",\"\"Measurement\"\":\" + value.ToString() + \"}]\"; var content = new StringContent(dataJson, Encoding.UTF8, \"application/json\"); \"application json\"); HttpResponseMessage response = _client.PostAsync(dataUri, content).Result; return response.IsSuccessStatusCode; } static Alert CheckStatus(int numberTrue) { if (numberTrue \u003e 3) return Alert.High; if (numberTrue \u003c 1) return Alert.Low; return Alert.Normal; } static bool ReportChange(Alert oldAlert, Alert newAlert) { bool success = true; DateTime now = DateTime.UtcNow; switch (oldAlert) { case Alert.Normal: if (Alert.Low == newAlert) { return WriteStreamValue(ValueRangeLow, lowValue, now) \u0026\u0026 WriteStreamValue(ValueRangeOut, lowValue, now); } if (Alert.High == newAlert) { return WriteStreamValue(ValueRangeHigh, highValue, now) \u0026\u0026 WriteStreamValue(ValueRangeOut, highValue, now); } break; case Alert.High: if (Alert.Low == newAlert) { return WriteStreamValue(ValueRangeLow, lowValue, now) \u0026\u0026 WriteStreamValue(ValueRangeOut, lowValue, now) \u0026\u0026 WriteStreamValue(ValueRangeHigh, normalValue, now); } if (Alert.Normal == newAlert) { return WriteStreamValue(ValueRangeOut, normalValue, now) \u0026\u0026 WriteStreamValue(ValueRangeHigh, normalValue, now); } break; case Alert.Low: if (Alert.Normal == newAlert) { return WriteStreamValue(ValueRangeOut, normalValue, now) \u0026\u0026 WriteStreamValue(ValueRangeLow, normalValue, now); } if (Alert.High == newAlert) { return WriteStreamValue(ValueRangeLow, normalValue, now) \u0026\u0026 WriteStreamValue(ValueRangeOut, highValue, now) \u0026\u0026 WriteStreamValue(ValueRangeHigh, highValue, now); } break; default: break; } return success; } static void Main(string[] args) { TimeSpan pollingInterval = GetPollingIntervalFromModbus(ModbusComponentId, StreamIds); FindOrCreateType(TypeId); FindOrCreateStream(ValueRangeHigh, TypeId); FindOrCreateStream(ValueRangeLow, TypeId); FindOrCreateStream(ValueRangeOut, TypeId); while (true) { int numberTrue = 0; foreach (string StreamId in StreamIds) { bool value = GetStreamValue(StreamId); if (value) numberTrue++; } Alert currentAlert = CheckStatus(numberTrue); if (currentAlert != _alert) { if (ReportChange(_alert, currentAlert)) { _alert = currentAlert; } } System.Threading.Thread.Sleep(pollingInterval); Console.WriteLine(\"ValueRange should be \" + _alert); } } } }"
                                                    },
    "content/overview/command-line-linux-quick-start.html":  {
                                                                 "href":  "content/overview/command-line-linux-quick-start.html",
                                                                 "title":  "Command line quick start - Linux",
                                                                 "keywords":  "Command line quick start - Linux The EdgeCmd utility is OSIsoft\u0027s proprietary tool for configuring Edge Data Store from a command line. EdgeCmd must be installed on the device with Edge Data Store. For instructions on installing EdgeCmd, see the EdgeCmd utility help . To access EdgeCmd on Linux: Open a command prompt. Enter the following command to start the edgecmd.exe tool from any directory. debian@beaglebone:~$ edgecmd help Type edgecmd help and press Enter. The EdgeCmd utility launches, displaying the following introductory material and a command prompt at the end: ************************************************************************************************************************ Welcome to OSIsoft Edge Data Store configuration utility. Utility version: 1.0.0.148 ************************************************************************************************************************ --------------------------------------------------------------------------------------------------------- Command-line options =\u003e \u0027Configuration\u0027, \u0027Help\u0027 --------------------------------------------------------------------------------------------------------- Please enter ID of a component you would like to configure or to get component specific help output. Example: ./edgecmd . edgecmd Help ComponentId ./edgecmd . edgecmd Configuration ComponentId To get set of components registered to the Edge Data Store please run: ./edgecmd . edgecmd Configuration System Components To configure the system, please use \u0027System\u0027 as the ComponentId. Example of getting System help output: ./edgecmd . edgecmd Help System Example of configuring System Logging level: ./edgecmd . edgecmd Configuration System logging LogLevel=Warning debian@beaglebone:~$"
                                                             },
    "content/overview/command-line-quick-start-windows.html":  {
                                                                   "href":  "content/overview/command-line-quick-start-windows.html",
                                                                   "title":  "Command line quick start - Windows",
                                                                   "keywords":  "Command line quick start - Windows The EdgeCmd utility is OSIsoft\u0027s proprietary tool for configuring Edge Data Store from a command line. EdgeCmd must be installed on the device with Edge Data Store. For instructions on installing EdgeCmd, see the EdgeCmd utility help . To access EdgeCmd on Windows: Open a command prompt. Type the following command and press Enter: edgecmd Help The EdgeCmd utility launches, displaying the introductory material followed by a command prompt: ************************************************************************************************************************ Welcome to OSIsoft Edge Data Store configuration utility. Utility version: 1.0.0.148 ************************************************************************************************************************ --------------------------------------------------------------------------------------------------------- Command-line options =\u003e \u0027Configuration\u0027, \u0027Help\u0027 --------------------------------------------------------------------------------------------------------- Please enter ID of a component you would like to configure or to get component specific help output. Example: .\\edgecmd.exe Help ComponentId .\\edgecmd.exe Configuration ComponentId To get set of components registered to the Edge Data Store please run: .\\edgecmd.exe Configuration System Components To configure the system, please use \u0027System\u0027 as the ComponentId. Example of getting System help output: .\\edgecmd.exe Help System Example of configuring System Logging level: .\\edgecmd.exe Configuration System logging LogLevel=Warning C:\\Users\\John\u003e"
                                                               },
    "content/overview/eds-overview.html":  {
                                               "href":  "content/overview/eds-overview.html",
                                               "title":  "Edge Data Store",
                                               "keywords":  "Edge Data Store Edge Data Store (EDS) is an embedded data server that runs on Linux and Windows. EDS provides a lightweight data collection and storage application designed to enable the capturing of data for historical storage and analysis at the edge of networks. A storage component based on sequential data storage technology is provided. You can configure and administer EDS through REST programming, configuration, administrative interfaces, and the EdgeCmd command line tool. EDS complements existing OSIsoft products, and is designed for small devices. You can install and run it on 64-bit Intel/AMD Intel AMD compatible and 32-bit ARM v7/v8 v7 v8 compatible chips. While not a replacement for a PI System, EDS augments the PI System by providing historical data access in situations where deploying a full PI System is impractical. EDS provides native capability via its egress functionality to send data to a PI System or to AVEVA Data Hub for long term historical storage and analysis. EDS provides the following capabilities: OMF data ingress Edge connectivity through Modbus TCP and OPC UA Configurable data storage Data egress to PI Web API and AVEVA Data Hub REST API to enable custom applications for visualization and analytics on Edge Data Store on either Linux, Windows, or both in a variety of programming languages Edge Data Store architecture The following diagram depicts the relationships of architectural components to one another in the Edge Data Store: Edge Data Store data flow Edge Data Store can egress data to both PI Data Archive and AVEVA Data Hub. For more information, see PI egress quick start and AVEVA Data Hub egress quick start . The following diagram depicts the flow of data in Edge Data Store: Edge Data Store components The following diagram depicts the relationship of key functions to relevant components of the Edge Data Store: Data ingress to Edge Data Store Edge Data Store can ingress data in a number of ways. There are two built-in adapters: EDS Modbus TCP and EDS OPC UA . Data can also be ingressed using OSIsoft Message Format (OMF) and the Sequential Data Store SDS REST APIs. The following diagram depicts an OMF data ingress scenario in the Edge Data Store: During installation of Edge Data Store, you can choose to install either an EDS Modbus TCP adapter or an EDS OPC UA adapter, or both. The EDS Modbus and EDS OPC UA adapters require configuration of data source and data selection before they can collect data in Edge Data Store. You can use OMF data ingress once Edge Data Store is installed, with no further configuration steps. Edge Data Store is composed of components, and is designed to allow additional EDS adapters at a later point. Local data read and write access You can access all data in Edge Data Store by using the Sequential Data Store REST API. You can use this data for local applications that perform analytics or visualization. For more information, see SDS quick start . Example EDS visualization application The following diagram depicts the flow of data from a customer visualization application into Edge Data Store, via either OMF or SDS REST calls: Example EDS analytics application The following diagram depicts the flow of data from a customer analytics application into Edge Data Store, via either OMF or SDS REST calls:"
                                           },
    "content/overview/modbus-quick-start.html":  {
                                                     "href":  "content/overview/modbus-quick-start.html",
                                                     "title":  "Modbus TCP EDS adapter quick start",
                                                     "keywords":  "Modbus TCP EDS adapter quick start The Modbus TCP EDS adapter is a component of Edge Data Store that defines connections to and receives data from Modbus TCP capable devices. The Modbus TCP EDS adapter can connect to multiple devices by defining one instance of the adapter for each device. The EDS installation includes the Modbus TCP EDS adapter and the option to add a single Modbus TCP EDS adapter instance. Add additional instances after installation using the system components configuration. For more information about installation, see Install Edge Data Store . To get started collecting data with an instance of the Modbus TCP EDS adapter, you need to configure the data source, which specifies the device connection, and the data selection, which specifies the data to collect. The following diagram depicts the data flow of a single instance of Modbus TCP EDS adapter: The adapter instance requests data from the Modbus TCP device and then the device sends its data. The adapter sends the collected data to the storage component where it is held until it can be egressed to permanent storage in PI Server or AVEVA Data Hub. The adapter instance can be configured from the device where EDS is installed, and EDS collects health information about the adapter that can be egressed. Configure a Modbus TCP data source To configure a data source to connect a Modbus TCP device to the Modbus TCP EDS adapter instance: Using a text editor, copy the example below to create a file in JSON format to describe the location of the Modbus TCP data source. { \"Devices\": [ { \"Id\": \"Device1\", \"IpAddress\": \"\u003cModbus IP Address\u003e\", \"Port\": \u003cPort - usually 502\u003e, } ], \"ReconnectInterval\": \"00:00:01\", \"RequestTimeout\": \"00:00:10\", \"DelayBetweenRequests\": \"00:00:00.5\", \"MaxResponseDataLength\": 125 } Modify the values in the example to match your environment, including the IP address and port for the Modbus data source. Save the file to the device with EDS installed using a file name based on the adapter instance name. For example, to use the adapter instance created during installation, which is Modbus1 , name the file Modbus1DataSource.json . Run the following command from the same directory where the file is located, updating the file name and destination in the script if needed. curl EdgeCmd curl -d \"@Modbus1Datasource.json\" -H \"Content-Type: application/json\" application json\" -X PUT http://localhost:5590/api/v1/configuration/Modbus1/Datasource http:  localhost:5590 api v1 configuration Modbus1 Datasource edgecmd set dataSource -cid Modbus1 -file Modbus1DataSource.json When the command completes successfully, that is a 204 is returned, the Modbus TCP data source has been created. If a 400 error is returned, check the JSON file for errors. If a 404 or 500 error is returned, check that EDS is running on the device. Configure a Schedule Data selection items will use schedules to perform their scans for data. To configure the schedule file, complete the following steps: Using a text editor, copy the example below to create a file in JSON format to define the schedule. [ { \"Id\": \"Schedule1\", \"Period\": \"00:00:01.500\", \"Offset\": \"00:02:03\" } ] Modify the values in the example to match your environment. Save the file to the device with EDS installed using a file name based on the adapter instance name. For example, to use the adapter instance created during installation, which is Modbus1 , name the file Modbus1Schedules.json . Run the following command from the same directory where the file is located, updating the file name and the endpoint URL in the script if needed. curl EdgeCmd curl -d \"@Modbus1Schedules.json\" -H \"Content-Type: application/json\" application json\" -X PUT http://localhost:5590/api/v1/configuration/Modbus1/Schedules http:  localhost:5590 api v1 configuration Modbus1 Schedules edgecmd set schedules -cid Modbus1 -file Modbus1ConfigureSchedules.json Configure Modbus TCP data selection After you create the data source file and schedule file, select the streams to store in EDS by configuring Modbus data selection. To configure the data selection file, complete the following steps: Using a text editor, copy the example below to create a file in JSON format to define each stream to ingress to EDS. [ { \"DeviceId\" : \"Device1\", \"Selected\": true, \"UnitId\": 1, \"RegisterType\": 3, \"RegisterOffset\": 1, \"DataTypeCode\": 20, \"ConversionFactor\": 2, \"ConversionOffset\": 3.4, \"ScheduleId\": \"Schedule1\" }, { \"DeviceId\" : \"Device1\", \"Selected\": true, \"UnitId\": 1, \"RegisterType\": 3, \"RegisterOffset\": 2, \"DataTypeCode\": 20, \"ConversionFactor\": 2, \"ConversionOffset\": 3.4, \"ScheduleId\": \"Schedule1\" } ] Modify the values in the example to match your environment. Save the file to the device with EDS installed using a file name based on the adapter instance name. For example, to use the adapter instance created during installation, which is Modbus1 , name the file Modbus1DataSelection.json . Run the following curl script from the same directory where the file is located, updating the file name and the endpoint URL in the script if needed. curl EdgeCmd curl -d \"@Modbus1Dataselection.json\" -H \"Content-Type: application/json\" application json\" -X PUT http://localhost:5590/api/v1/configuration/Modbus1/Dataselection http:  localhost:5590 api v1 configuration Modbus1 Dataselection edgecmd set dataSelection -cid Modbus1 -file Modbus1DataSelection.json"
                                                 },
    "content/overview/ocs-egress-quick-start.html":  {
                                                         "href":  "content/overview/ocs-egress-quick-start.html",
                                                         "title":  "AVEVA Data Hub egress quick start",
                                                         "keywords":  "AVEVA Data Hub egress quick start Data egress provides a mechanism to transfer data to AVEVA Data Hub using OMF messages. To get started sending data stored in Edge Data Store to AVEVA Data Hub, create an OMF connection in AVEVA Data Hub and configure an egress endpoint with the connection information for AVEVA Data Hub. Create an OMF connection in AVEVA Data Hub To create an OMF connection in AVEVA Data Hub: In AVEVA Data Hub, create a client. For details, see Clients in the AVEVA Data Hub documentation. The Client Id and Client Secret are used for the corresponding properties in the egress configuration. In AVEVA Data Hub, create an OMF connection. For details, see Configure an OMF connection in the AVEVA Data Hub documentation. The OMF Endpoint URL for the connection is used as the egress configuration EndpointId parameter. Create a periodic egress configuration To configure periodic egress to AVEVA Data Hub, follow the steps in Configure data egress ."
                                                     },
    "content/overview/omf-quick-start.html":  {
                                                  "href":  "content/overview/omf-quick-start.html",
                                                  "title":  "OMF ingress quick start",
                                                  "keywords":  "OMF ingress quick start Create a custom application using OSIsoft Message Format to send data to Edge Data Store from sources that cannot use Modbus or OPC UA protocols. The following diagram depicts the data flow from an OMF data collection application into EDS: The OMF application collects data from a data source and sends it to the EDS endpoint. The EDS endpoint sends the data to the storage component where it is held until it can be egressed to permanent storage in PI Server or AVEVA Data Hub. The OMF application must run on the same device as EDS and no authentication is needed. To get started using OMF messages to ingress data into EDS, create an OMF type and container and then write data events to the container using REST APIs. Use the Sequential Data Store (SDS) REST API to read the data back from EDS. The omfversion header must match the version of the OMF spec used to construct the message. EDS supports versions 1.0, 1.1, and 1.2 of the OMF specification. Create an OMF type The first step in OMF data ingress is to create an OMF type that describes the format of the data to be stored in a container. In this example, the data to be written is a timestamp and a numeric value. To create an OMF type: Create an OMF JSON file that defines the type as follows: [{ \"id\": \"MyCustomType\", \"classification\": \"dynamic\", \"type\": \"object\", \"properties\": { \"Timestamp\": { \"type\": \"string\", \"format\": \"date-time\", \"isindex\": true }, \"Value\": { \"type\": \"number\", \"format\": \"float32\" } } }] The value is indexed by a timestamp, and the numeric value that will be stored is a 32-bit floating point value. To create the OMF type in Edge Storage, store the JSON file with the name OmfCreateType.json on the local device. Run the following curl command: curl -d \"@OmfCreateType.json\" -H \"Content-Type: application/json\" application json\" -H \"producertoken: x \" -H \"omfversion: 1.1\" -H \"action: create\" -H \"messageformat: json\" -H \"messagetype: type\" -X POST http://localhost:5590/api/v1/tenants/default/namespaces/default/omf/ http:  localhost:5590 api v1 tenants default namespaces default omf  When this command completes successfully, an OMF type with the same name is created on the server. Any number of containers can be created from the type, as long as they use a timestamp as an index and have a 32-bit floating point value. The create type message needs to be sent first before container and data messages, but it does not cause an error if the same message is sent at a later time. Create an OMF container The next step in writing OMF data is to create an OMF container. As with an OMF type, the create container message only needs to be sent once before sending data events, but resending the same definition again does not cause an error. To create an OMF container: Create an OMF JSON file that defines the container as follows: [{ \"id\": \"MyCustomContainer\", \"typeid\": \"MyCustomType\" }] This container references the OMF type that was created earlier, and an error will occur if the type does not exist when the container is created. To create the OMF container in Edge Storage, store the JSON file with the name OmfCreateContainer.json on the local device. To create the SDS stream to store data defined by the type, run the following curl command: curl -d \"@OmfCreateContainer.json\" -H \"Content-Type: application/json\" application json\" -H \"producertoken: x \" -H \"omfversion: 1.1\" -H \"action: create\" -H \"messageformat: json\" -H \"messagetype: container\" -X POST http://localhost:5590/api/v1/tenants/default/namespaces/default/omf/ http:  localhost:5590 api v1 tenants default namespaces default omf  Write data events to the OMF container Once a type and container are defined, follow these steps to write data to the container: Create an OMF JSON file to define data events to be stored in the SDS Stream created in the previous steps. For best performance, batch OMF values together, as in the following example: [{ \"containerid\": \"MyCustomContainer\", \"values\": [{ \"Timestamp\": \"2019-07-16T15:18:24.9870136Z\", \"Value\": 12345.6789 }, { \"Timestamp\": \"2019-07-16T15:18:25.9870136Z\", \"Value\": 12346.6789 } ] }] To write the data to EDS, store the JSON file with the name OmfCreateDataEvents.json on the local device. To write data values to the SDS stream, run the following curl command: curl -d \"@OmfCreateDataEvents.json\" -H \"Content-Type: application/json\" application json\" -H \"producertoken: x \" -H \"omfversion: 1.1\" -H \"action: create\" -H \"messageformat: json\" -H \"messagetype: data\" -X POST http://localhost:5590/api/v1/tenants/default/namespaces/default/omf/ http:  localhost:5590 api v1 tenants default namespaces default omf  Use SDS to read last data written To use the SDS REST API to read back the last data event written to the server: Start the curl command line tool. Run the following curl command to return the last value written: curl http://localhost:5590/api/v1/tenants/default/namespaces/default/streams/MyCustomContainer/Data/Last http:  localhost:5590 api v1 tenants default namespaces default streams MyCustomContainer Data Last Sample output: {\"Timestamp\": \"2019-07-16T15:18:25.9870136Z\", \"Value\": 12346.6789} Use SDS to read a range of data events To use the SDS REST API to read back the a range of data written to the server: Start the curl command line tool. Run the following curl command to return up to 100 values after the startIndex specified: curl \"http://localhost:5590/api/v1/tenants/default/namespaces/default/streams/MyCustomContainer/Data?startIndex=2017-07-08T13:00:00Z\u0026count=100\" \"http:  localhost:5590 api v1 tenants default namespaces default streams MyCustomContainer Data?startIndex=2017-07-08T13:00:00Z\u0026count=100\" Sample output: [{\"Timestamp\": \"2019-07-16T15:18:24.9870136Z\",\"Value\": 12345.6789}, {\"Timestamp\": \"2019-07-16T15:18:25.9870136Z\", \"Value\": 12346.6789}] Both values that were entered are returned. This command returns up to 100 values after the specified timestamp."
                                              },
    "content/overview/opc-ua-quick-start.html":  {
                                                     "href":  "content/overview/opc-ua-quick-start.html",
                                                     "title":  "OPC UA EDS adapter quick start",
                                                     "keywords":  "OPC UA EDS adapter quick start The OPC UA EDS adapter is a component of Edge Data Store that defines connections to and receives data from OPC UA capable devices. The OPC UA EDS adapter can connect to multiple devices by defining one instance of the adapter for each device. The EDS installation includes the OPC UA EDS adapter and the option to add a single OPC UA EDS adapter instance. Add additional instances after installation using the system components configuration. For more information about installation, see Install Edge Data Store . To get started collecting data with an instance of the OPC UA EDS adapter, you need to configure the data source, which specifies the device connection, and the data selection, which specifies the data to collect. The following diagram depicts the data flow for a single instance of OPC UA EDS adapter instance: The adapter instance polls the OPC UA device and then collects data from the device. The adapter then sends the data to the storage component where it is held until it can be egressed to permanent storage in PI Server or AVEVA Data Hub. The adapter instance can be configured from the device where EDS is installed, and EDS collects health information about the adapter that can be egressed. Configure an OPC UA data source To configure a data source to connect an OPC UA device to an OPC UA EDS adapter instance: Using a text editor, copy the example below to create a file in JSON format with the location of the OPC UA data source. { \"EndpointUrl\": \"opc.tcp://\u003cip \"opc.tcp:  \u003cip address\u003e:\u003cport - often 62541\u003e/\u003cserver 62541\u003e \u003cserver path\u003e\", \"UseSecureConnection\": false, \"UserName\": null, \"Password\": null, \"IncomingTimestamp\": \"Source\", \"StreamIdPrefix\": \"OpcUa\" } Modify the values in the example to match your environment, including the IP address and port for the OPC UA data source. Save the file to the device with EDS installed using a file name based on the adapter instance name. For example, to use the adapter instance created during installation, which is OpcUa1 , name the file OpcUa1Datasource.json . Run the following command from the directory where the file is located, updating the file name and the destination in the script if needed. curl EdgeCmd curl -d \"@OpcUa1Datasource.json\" -H \"Content-Type: application/json\" application json\" -X PUT http://localhost:5590/api/v1/configuration/OpcUa1/Datasource http:  localhost:5590 api v1 configuration OpcUa1 Datasource edgecmd set dataSource -cid OpcUa1 -file OpcUa1DataSource.json When the command completes successfully (a 204 message is returned by curl), the OPC UA data source has been created. If you receive a 400 error, check the data source JSON file for errors. If you receive a 404 or 500 error, check that Edge Data Store is running on the device. Configure OPC UA data selection When you create the data source file, the OPC UA adapter auto generates the data selection file, which lists all available streams in the designated data source. To configure the data selection file: Save the data selection file to the local device and name it based on the adapter instance name. For example, to use the adapter instance created during installation, which is OpcUa1 , name the file OpcUa1Dataselection.json . Open the file in a text editor. It should look similar to the following example: [{ \"Selected\": false, \"Name\": \"Cold Side Inlet Temperature\", \"NodeId\": \"ns=2;s=Line1.HeatExchanger1001.ColdSideInletTemperature\", \"StreamId\": null }, { \"Selected\": false, \"Name\": \"Hot Side Inlet Temperature\", \"NodeId\": \"ns=2;s=Line1.HeatExchanger1001.HotSideInletTemperature\", \"StreamId\": null }, { \"Selected\": false, \"Name\": \"Hot Side Outlet Temperature\", \"NodeId\": \"ns=2;s=Line1.HeatExchanger1001.HotSideOutletTemperature\", \"StreamId\": null }, { \"Selected\": false, \"Name\": \"Cold Side Inlet Temperature\", \"NodeId\": \"ns=2;s=Line1.HeatExchanger1002.ColdSideInletTemperature\", \"StreamId\": null }, { \"Selected\": false, \"Name\": \"Hot Side Outlet Temperature\", \"NodeId\": \"ns=2;s=Line1.HeatExchanger1002.HotSideOutletTemperature\", \"StreamId\": null } ] To ingress a stream to Edge Data Store, change the value of the Selected key from false to true . All streams in the auto generated data selection file are initially set to false . Save the the file. Run the following command from the directory where the file is located, updating the file name and destination in the script if needed: curl EdgeCmd curl -d \"@OpcUa1Dataselection.json\" -H \"Content-Type: application/json\" application json\" -X PUT http://localhost:5590/api/v1/configuration/OpcUa1/Dataselection http:  localhost:5590 api v1 configuration OpcUa1 Dataselection edgecmd set dataSelection -cid OpcUa1 -file OpcUa1DataSelection.json"
                                                 },
    "content/overview/performance.html":  {
                                              "href":  "content/overview/performance.html",
                                              "title":  "Performance",
                                              "keywords":  "Performance Edge Data Store is designed to run on a variety of low-powered devices and to serve data to custom applications that run on the same platform. To assist in determining the appropriate hardware and software configuration for a specific use, EDS was tested on a variety of different devices???from single board computers to industrial edge gateways, on Linux and Windows operating systems, and with different data stream counts and with the supported ingress methods collecting data at different event rates. Use the following performance testing information to determine the appropriate hardware and software to use with EDS in your scenarios. Edge Data Store Performance Testing Hardware EDS performance test cases were divided into three categories based on commonly available, consumer grade and industrial grade devices. Several devices in each category were tested to validate performance expectations and the results of these tests are summarized below in terms of the maximum supported data stream count and data ingress rate for each category. Small devices (e.g., 1 core ARM CPU, 512 MB RAM, Linux): 30 data streams, 30 events /   sec Medium devices (e.g., 2 core ARM or Intel CPU, 2 GB RAM, Linux or Windows): 300 data streams, 300 events /   sec Large devices (e.g., 4 core ARM or Intel CPU, 4 GB RAM, Linux or Windows): 3,000 data streams, 3,000 events /   sec It is possible that lower performance results may be realized on other devices with similar hardware and software configurations. Other applications running on the same device may also affect actual performance of EDS. The data stream counts and throughput rates shown above are the upper limits of what is supported for EDS for each device category. Ingress Performance EDS can ingress data using the EDS Modbus TCP adapter, the EDS OPC UA adapter, and/or and or a custom OMF application developed by others. Each of these data ingress methods have a different performance profile, so the performance of EDS using these different methods will vary. When selecting the device to host EDS, there are a few general principles that arose from the performance testing: EDS adapters use less CPU than custom OMF applications when using the Edge Data Store OMF endpoint. In general, across all hardware platforms, CPU usage for EDS Modbus TCP and EDS OPC UA adapters is roughly half of that used by custom OMF applications at the same event rates. RAM usage is largely determined by the number of data streams written to. More data streams collected in EDS by any data ingress method require additional RAM (in addition to requiring more storage space). EDS is slightly more efficient running on Windows than Linux on similar devices. Both CPU and RAM usage are slightly lower when EDS is running on Windows 10 than on Linux when comparing them on the AMD64/Intel AMD64 Intel x64 devices, which support both operating systems. SSD storage is recommended for maximum performance and reliability. EDS performance testing was completed on a variety of storage media - SSDs, HDDs, eMMC, and SD cards. EDS performance testing was successful using all storage technologies but for maximum performance and reliability, SSD storage is the best choice. Periodic Egress Performance EDS generates OMF messages when configured to egress data to a PI Server or to AVEVA Data Hub. An important part of periodic egress performance is the amount of network bandwidth available between the device hosting the EDS and the PI Web API OMF endpoint or the AVEVA Data Hub OMF endpoint. The performance numbers presented in this section reflect use of a network with a 1 GB LAN connection and a high-speed connection to the Internet. If EDS is installed on a device in a location with limited network bandwidth, a lower level of egress performance can be expected. Data egress has a much smaller performance impact on EDS than data ingress. Generally speaking, the performance impact of data egress on CPU and RAM usage is only a small percentage of the CPU and RAM usage of data ingress, so data egress configuration is not a major factor in EDS system design. Periodic Egress Performance to PI Web API Performance testing of periodic egress between EDS and the PI Web API OMF endpoint was completed with a 1 GB LAN connection between the EDS test device and the PI Web API. The PI Web API was hosted on server class PC that also included a PI Server. The EDS device was configured for backfill and several million events were sent to the PI Web API. In all cases, EDS egress performance exceeded 10,000 events per second. In addition, extended tests were run over several weeks with an egress rate of 3,000 events per second. Periodic Egress Performance to AVEVA Data Hub Performance testing of periodic egress between EDS and the AVEVA Data Hub OMF endpoint was completed with a high-speed Internet connection between the EDS test device and AVEVA Data Hub running in the Microsoft Azure West US Data Center, approximately 2,500 miles away. The EDS device was configured for backfill and several million events were sent to AVEVA Data Hub. In all cases, EDS egress performance exceeded 10,000 events per second. In addition, extended tests were run over several weeks with a lower egress rate."
                                          },
    "content/overview/pi-egress-quick-start.html":  {
                                                        "href":  "content/overview/pi-egress-quick-start.html",
                                                        "title":  "PI Server egress quick start",
                                                        "keywords":  "PI Server egress quick start Data egress provides a mechanism to transfer data to PI Server using OMF messages through a PI Web API endpoint. To get started sending data stored in Edge Data Store to a PI Server, create a PI Web API OMF endpoint and configure periodic egress to use the PI Web API endpoint. Create a PI Web API OMF endpoint To create a PI Web API OMF endpoint: Install PI Web API and enable the OSIsoft Message Format (OMF) Services feature. During configuration, choose an AF database and PI Data Archive where metadata and data will be stored. The account used in an egress configuration needs permissions to create AF elements, element templates, and PI points. Configure PI Web API to use Basic authentication. For complete steps, as well as best practices and recommendations, see the PI Web API User Guide . Note: The certificate used by PI Web API must be trusted by the device running EDS, otherwise the egress configuration ValidateEndpointCertificate property needs to be set to false , which can be the case with a self-signed certificate, but should only be used for testing purposes. Create a periodic egress configuration To configure periodic egress for the PI Server, follow the steps in Configure data egress ."
                                                    },
    "content/overview/quick-start-guides.html":  {
                                                     "href":  "content/overview/quick-start-guides.html",
                                                     "title":  "Quick start guides",
                                                     "keywords":  "Quick start guides The quick start guides in this section provide basic instructions to get started with Edge Data Store components and utilities. Refer to the respective configuration sections for detailed instructions on each EDS component or utility. The examples in each quick start guide use curl, a commonly available tool on both Windows and Linux. You can use the same operations with any programming language or tool that supports making REST calls or configure EDS components with the EdgeCmd utility. If available on your device, use a browser to complete data retrieval steps using GET commands to validate successful configurations. All quick start guide examples assume EDS has been installed and is accessible through a REST API using the default installed port, which is 5590 ."
                                                 },
    "content/overview/scale-performance.html":  {
                                                    "href":  "content/overview/scale-performance.html",
                                                    "title":  "Design considerations",
                                                    "keywords":  "Design considerations Before installing Edge Data Store, determine your storage and throughput needs and select devices that meet those needs. Edge Storage role The Edge Storage component is integrated with the EDS and does not replace any existing storage technology produced by OSIsoft. The Edge Storage component is a resilient and reliable data store, but is limited in the duration and scope of the data it retains. By default, the storage component processes data in a FIFO (first in first out) method: as new data comes in and the size of streams exceeds the configured limits, older data is purged. You must egress data that you need permanently retained to either PI Data Archive, using the PI Web API OMF endpoint, or to AVEVA Data Hub, using the AVEVA Data Hub OMF ingress endpoint. Edge Storage scale The Edge Storage component provides an appropriate level of storage performance for small devices. For the smallest of these devices, throughput may be limited to tens of events per second. For larger devices with faster processors, memory, and storage, this could increase up to 3,000 events per second. The Edge Storage component is designed for small devices in Edge scenarios. If high throughput or large stream counts are required, AVEVA Data Hub or PI Data Archive are more appropriate choices. Sizing of Edge devices For EDS, there are three supported tiers of performance: Small Devices: 1 Core CPU, 512 MB RAM. 30 events/second, events second, 200 streams total. Medium Devices: 2 Core CPU, 1 GB RAM. 300 events/second, events second, 2000 streams total. Large Devices: 4 Core CPU, 4 GB RAM, SSD storage. 3000 events/second, events second, 3000 streams total. These performance metrics assume solid state storage, which is commonly used in Edge devices."
                                                },
    "content/overview/sds-quick-start.html":  {
                                                  "href":  "content/overview/sds-quick-start.html",
                                                  "title":  "SDS quick start",
                                                  "keywords":  "SDS quick start Create a custom application using Sequential Data Store (SDS) REST API to send data to Edge Data Store from sources that cannot use Modbus or OPC UA protocols. The following diagram depicts the data flow from an SDS custom application into EDS: The SDS application collects data from a data source and sends it to the EDS endpoint. The EDS endpoint sends the data to the storage component where it is held until it can be egressed to permanent storage in PI Server or AVEVA Data Hub. All data from all sources on EDS can be read using the SDS REST APIs on the local device in the default tenant and the default namespace. To get started using the SDS REST API to ingress data into EDS, create an SDS type and stream and then write data events to the SDS stream. Use the Sequential Data Store (SDS) REST API to read the data back from EDS. Create an SDS type To create an SDS type that describes the format of the data to be stored in a container: Create a JSON file using the example below: { \"Id\": \"Simple\", \"Name\": \"Simple\", \"SdsTypeCode\": 1, \"Properties\": [ { \"Id\": \"Time\", \"Name\": \"Time\", \"IsKey\": true, \"SdsType\": { \"SdsTypeCode\": 16 } }, { \"Id\": \"Measurement\", \"Name\": \"Measurement\", \"SdsType\": { \"SdsTypeCode\": 14 } } ] } Note: The data to be written is a timestamp and numeric value. It is indexed by a timestamp, and the numeric value is a 64-bit floating point value. Save the JSON file the name SDSCreateType.json . Run the following curl script from the directory where the file is located: curl -d \"@SDSCreateType.json\" -H \"Content-Type: application/json\" application json\" -X POST http://localhost:5590/api/v1/tenants/default/namespaces/default/types/Simple http:  localhost:5590 api v1 tenants default namespaces default types Simple When this script completes successfully, an SDS type with the same name is created on the server. You can create any number of containers from a single type, as long as they use a timestamp as an index and a 64-bit floating point value. The Type definition needs to be sent first before you send data with a custom application. It does not cause an error to resend the same definition at a later time. Create an SDS stream To create an SDS stream: Create a JSON file using the example below: { \"Id\": \"Simple\", \"Name\": \"Simple\", \"TypeId\": \"Simple\" } Note: This stream references the type you created earlier. An error occurs if the type does not exist when the stream is created. As with an SDS type, create a stream once before sending data events. Resending the same definition repeatedly does not cause an error. Save the JSON file with the name SDSCreateStream.json . Run the following curl script from the directory where the file is located: curl -d \"@SDSCreateStream.json\" -H \"Content-Type: application/json\" application json\" -X POST http://localhost:5590/api/v1/tenants/default/namespaces/default/streams/Simple http:  localhost:5590 api v1 tenants default namespaces default streams Simple When this script completes successfully, an SDS stream is created on the server to store data defined by the specified type. Write data events to the SDS stream After you create a type and container, follow these steps to use SDS to write data to a stream: Create a JSON file using the example below: [{ \"Time\": \"2017-11-23T17:00:00Z\", \"Measurement\": 50.0 }, { \"Time\": \"2017-11-23T18:00:00Z\", \"Measurement\": 60.0 }] Note: This example includes two data events that will be stored in the SDS Stream created in the previous steps. For optimal performance, batch SDS values when writing them. Save the JSON file with the name SDSWriteData.json . Run the following curl script from the directory where the file is located: curl -d \"@SDSWriteData.json\" -H \"Content-Type: application/json\" application json\" -X POST http://localhost:5590/api/v1/tenants/default/namespaces/default/streams/Simple/Data http:  localhost:5590 api v1 tenants default namespaces default streams Simple Data When this script completes successfully, two values are written to the SDS stream. Read last data written using SDS To use the SDS REST API to read back the last data event written to the server: Start the curl command line tool. Run the following curl command to return the last value written: curl http://localhost:5590/api/v1/tenants/default/namespaces/default/streams/MyCustomContainer/Data/Last http:  localhost:5590 api v1 tenants default namespaces default streams MyCustomContainer Data Last Sample output: {\"Timestamp\": \"2019-07-16T15:18:25.9870136Z\", \"Value\": 12346.6789} Read a range of data events written using SDS To use the SDS REST API to read back the a range of data written to the server: Start the curl command line tool. Run the following curl command to return up to 100 values after the startIndex specified: curl \"http://localhost:5590/api/v1/tenants/default/namespaces/default/streams/MyCustomContainer/Data?startIndex=2017-07-08T13:00:00Z\u0026count=100\" \"http:  localhost:5590 api v1 tenants default namespaces default streams MyCustomContainer Data?startIndex=2017-07-08T13:00:00Z\u0026count=100\" Sample output: [{\"Timestamp\": \"2019-07-16T15:18:24.9870136Z\",\"Value\": 12345.6789}, {\"Timestamp\": \"2019-07-16T15:18:25.9870136Z\", \"Value\": 12346.6789}] Both values that were entered are returned. This command returns up to 100 values after the specified timestamp."
                                              },
    "content/overview/visualization-quick-start.html":  {
                                                            "href":  "content/overview/visualization-quick-start.html",
                                                            "title":  "Edge Data Store visualization quick start",
                                                            "keywords":  "Edge Data Store visualization quick start This topic provides a quick start for displaying data from the EDS storage component on the local device where Edge Data Store is installed. The following example should run on any device supported by Edge Data Store, and iterates through all streams in the default namespace, continually displaying the latest values to the screen. using System; using System.Collections.Generic; using System.Net.Http; using Newtonsoft.Json; namespace EdgeDataScroller { class EdgeStream { public string Id { get; set; } } class DataScroller { static HttpClient _client = new HttpClient(); private static Dictionary\u003cstring, Dictionary\u003cstring, object\u003e\u003e GetDataForNamespace(string ns) { Dictionary\u003cstring, Dictionary\u003cstring, object\u003e\u003e outputs = new Dictionary\u003cstring, Dictionary\u003cstring, object\u003e\u003e(); string uri = string.Format(\"http://localhost:5590/api/v1/tenants/default/namespaces/{0}/streams\", string.Format(\"http:  localhost:5590 api v1 tenants default namespaces {0} streams\", ns); string json = _client.GetStringAsync(uri).Result; List\u003cEdgeStream\u003e streams = JsonConvert.DeserializeObject\u003cList\u003cEdgeStream\u003e\u003e(json); foreach (var stream in streams) { string lastValueUri = string.Format(\"http://localhost:5590/api/v1/tenants/default/namespaces/{0}/streams/{1}/Data/Last\", string.Format(\"http:  localhost:5590 api v1 tenants default namespaces {0} streams {1} Data Last\", ns, stream.Id.Trim()); string lastValueJson = _client.GetStringAsync(lastValueUri).Result; Dictionary\u003cstring, object\u003e values = JsonConvert.DeserializeObject\u003cDictionary\u003cstring, object\u003e\u003e(lastValueJson); outputs.Add(stream.Id.Trim(), values); } return outputs; } static void DisplayData(List\u003cstring\u003e namespaces, TimeSpan interval) { Dictionary\u003cstring, Dictionary\u003cstring, Dictionary\u003cstring, object\u003e\u003e\u003e outputs = new Dictionary\u003cstring, Dictionary\u003cstring, Dictionary\u003cstring, object\u003e\u003e\u003e(); Console.WriteLine(\"Data Displayed at \" + DateTime.UtcNow.ToString(\"o\")); foreach (string ns in namespaces) { outputs.Add(ns, GetDataForNamespace(ns)); } foreach (string ns in outputs.Keys) { foreach (string stream in outputs[ns].Keys) { if (null == outputs[ns][stream]) { Console.WriteLine(\"No values for \" + stream); continue; } foreach (string field in outputs[ns][stream].Keys) { object obj = outputs[ns][stream][field]; string value = obj.ToString(); if (obj is DateTime) { value = ((DateTime)obj).ToString(\"o\"); } Console.WriteLine($\"{ns}.{stream}.{field} = {value}\"); } } Console.WriteLine(\"****\"); } Console.WriteLine(string.Empty); System.Threading.Thread.Sleep(interval); } static void Main(string[] args) { List\u003cstring\u003e namespaces = new List\u003cstring\u003e(); namespaces.Add(\"default\"); TimeSpan interval = TimeSpan.FromSeconds(5.0); if (null != args \u0026\u0026 args.Length \u003e 0) { string choice = args[0].Trim().ToLowerInvariant(); if (choice == \"all\") { namespaces.Add(\"diagnostics\"); } if (choice == \"diagnostics\") { namespaces.Clear(); namespaces.Add(\"diagnostics\"); } if (args.Length \u003e 1) { string newInterval = args[1].Trim(); double newValue = -1.0; if (double.TryParse(newInterval, out newValue)) { interval = TimeSpan.FromSeconds(newValue); } } } while (true) DisplayData(namespaces, interval); } } }"
                                                        },
    "content/release-notes/release-notes.html":  {
                                                     "href":  "content/release-notes/release-notes.html",
                                                     "title":  "Edge Data Store 2023 release notes",
                                                     "keywords":  "Edge Data Store 2023 release notes Edge Data Store (EDS) is a lightweight data collection and storage application designed to capture data at the edge of networks for historical storage and analysis. It runs on small, rugged devices or embedded in existing industrial hardware and is designed to be resilient and require minimal installation and administration. Enhancements The following features were added: Added support for OMF 1.2 ingress and egress Added support for the latest OPC UA and Modbus adapters Enhanced SDS performance and stability Enhanced health and diagnostics reporting, including egress IO rate Added support for stream tags and metadata Periodic egress improvements include: Added parameter to specify start time Improved flexibility of egress configuration Added data filtering, consistent with Adapters Added manual egress feature to allow data egress to permanent storage outside of regular periodic schedule Improved compatibility with PI Web API OPC UA component improvements include: Reduced load on OPC UA server during history recovery by caching user access level Enhanced logged messages to include status code in hexadecimal instead of decimal and aliased data types Enhanced edge system configuration secrets management by storing in a centralized location while maintaining backward compatibility Excluded read-only facets from top level configuration in GET requests Increased the payload size to 64 MB Removed logging and eliminated error when the same component is added multiple times Modbus component improvements include: PI Adapter for Modbus now leverages a Schedules configuration consistent with other adapters. Instead of specifying a scanRate for each selection item, you configure a schedule in the schedules config facet, then reference that schedule with the scheduleId property on each selection item. Fixes The following items were resolved: Item Description 298730 Fixed FormatException from parsing UOM CSV using some cultures. 294320 Fixed an issue causing data to stop flowing to a first ADH endpoint after a second endpoint is added. 308001 OPC UA now correctly distinguishes between local and UTC time as inputs to History Recovery only mode. History recovery starttime and endtime supplied in local time format will be treated as a local time by the adapter node instead of the UTC time. 335628 Fixed an issue that prevented built-in adapters from sending string data. 275072 Data collection for the OPC UA server data items will no longer be skipped when the source OPC UA Server has invalid data item attributes like: DataType, Description, BrowseName, DisplayName, UserAccessLevel. 338424 The OpcUa Data Type \u0027UtcTime\u0027 is now supported as a DateTime type. 270854 Fixed an issue that caused data to be collected for selection items with \"selected\": false . Known issues Some EDS egress configuration changes are not respected until EDS is restarted. Upgrade considerations This is a list of known changes in Edge Data Store 2023 that may be relevant to upgrading customers. Downtime during upgrade is expected to be approximately 6 seconds. Built-in Adapter types have been updated since the previous EDS release to either include Data Quality, new OMF 1.2 feature, or to improve their naming. EDS will automatically migrate to these new types using StreamViews. Clients using EDS???s generic OMF endpoint must manually migrate if types are updated. For detail, see the KB article, OPC UA Adapter - Upgrade from v1.1 to v1.2 . Periodic egress configuration has been reorganized in order to support the reuse of common configuration blocks. When EDS is upgraded, it will automatically migrate existing configurations to the new format. However, any subsequent configuration changes made will need to use the updated API and format. See documentation for more information. The ApplyPrefixToStreamId configuration property has been removed. OMF 1.2 is now supported. EDS supports ingress of both OMF 1.1 and 1.2. However, it will only egress OMF 1.2, so its egress endpoints must be capable of receiving OMF 1.2, which requires PI Web API 2021 or later. Stream Tags and Metadata are now supported. OMF Data Source, if provided, will be written to EDS as custom metadata and then egressed properly using the OMF Data Source property. Upgraded to .NET 6. The following deprecated operating systems are no longer supported: Debian 9 and Ubuntu 18.04. Renamed the Edge Data Store Schema directory to Schemas . OPC UA component considerations: The DeviceStatus value NotConfigured is now Not Configured . Moved trusted certificates from \\EdgeDataStore\\{componentId}\\Certificates\\Trusted\\certs to \\EdgeDataStore\\Certificates\\trusted\\certs . The configuration items ApplyPrefixToStreamId and rootNodeIds have been removed from the Data Source configuration. They will be removed automatically. Existing data selection items that did not have a Stream Id specified will be given a default Stream Id taken from the DefaultStreamIdPattern configuration item in the Data Source configuration. Modbus component considerations: The DeviceStatus value NotConfigured is now Not Configured . Data Source Moved IpAddress and Port to new Devices array to support connecting to multiple Modbus devices. Changed intervals and timeouts to Timespan data type instead of Integer . ConnectTimeout has been removed from the data source configuration. ApplyPrefixToStreamId has been removed from the data source configuration. Data Selection ScanRate replaced by ScheduleId . The RegisterType property no longer supports Input32 or Holding32 register types. Default StreamId now includes DeviceId . Security information and guidance OSIsoft is committed to releasing secure products . This section is intended to provide relevant security-related information to guide your installation or upgrade decision. OSIsoft proactively discloses aggregate information about the number and severity of security vulnerabilities addressed in each release. The tables below provide an overview of security issues addressed and their relative severity based on standard scoring . Component Version CVE or Reference CVSS Description zlib 1.2.11 CVE-2018-25032 7.5 The Edge Data Store\u0027s utilization of zlib through .NET 6 does not expose these vulnerabilities. zlib 1.2.11 CVE-2022-37434 9.8 The Edge Data Store\u0027s utilization of zlib through .NET 6 does not expose these vulnerabilities. Distribution kit files EdgeDataStore_1.1.1.46-arm_.deb EdgeDataStore_1.1.1.46-arm64_.deb EdgeDataStore_1.1.1.46-x64_.deb EdgeDataStore_1.1.1.46-x64_.msi"
                                                 },
    "content/sds/compression.html":  {
                                         "href":  "content/sds/compression.html",
                                         "title":  "Compression",
                                         "keywords":  "Compression To more efficiently utilize network bandwidth, the EDS Sequential Data Store supports compression for reading data and writing data through the REST API. Supported compression schemes the EDS Sequential Data Store supports the following compression schemes: gzip deflate Request compression (write data) Specify the compression scheme in the Content-Encoding HTTP header of compressed-content requests. This header provides context to the API to properly decode the request content. Response compression (read data) Request compressed responses from the REST API by specifying one of the supported compression schemes using the Accept-Encoding HTTP header. Compressed responses from the REST API include a Content-Encoding HTTP header indicating the compression scheme used to compress the response content. Note: Specifying a compression scheme with the Accept-Encoding HTTP header does not guarantee a compressed response. Always refer to presence and value of the Content-Encoding HTTP header of the response to properly decode the response content."
                                     },
    "content/sds/indexes.html":  {
                                     "href":  "content/sds/indexes.html",
                                     "title":  "Indexes",
                                     "keywords":  "Indexes Indexes speed up and order the results of searches. A key uniquely identifies a record within a collection of records. Keys are unique within the collection. In SDS, the key of an SdsType is also an index. The key is often referred to as the primary index , while all other indexes are referred to as secondary indexes or secondaries . An SdsType that is used to define an SdsStream must specify a key. When you add data to a stream, every key value must be unique. SDS will not store more than a single event for a given key. An event with a particular key may be deleted or updated, but two events with the same key cannot exist. In .NET, the SdsType properties that define the primary index are identified using an OSIsoft.Sds.SdsMemberAttribute and setting its IsKey field to true . If the key consists of only a single property, you can use the System.ComponentModel.DataAnnotations.KeyAttribute . Property or properties representing the primary index of a type have their SdsTypeProperty.IsKey field set to true . Secondary indexes are defined on SdsStreams and applied to a single property. You can define many secondary indexes and the values do not need to be unique. Supported types for an index The following table shows supported index types. Type SdsTypeCode Boolean 3 Byte 6 Char 4 DateTime 16 DateTimeOffset 20 Decimal 15 Double 14 Guid 19 Int16 7 Int32 9 Int64 11 SByte 5 Single 13 String 18 TimeSpan 21 UInt16 8 UInt32 10 UInt64 12 Compound indexes A single property, such as DateTime , is adequate for defining an index most of the time. For more complex scenarios, SDS allows you to define multiple properties. Indexes defined by multiple properties are known as compound indexes . Only the primary index (or key) supports compound indexes. When you define a compound index within the .NET framework, you should apply the OSIsoft.Sds.SdsMemberAttribute on each property field of the SdsType that is combined to define the index. Set the property IsKey to true and give the Order field a zero-based index value. The Order field defines the precedence of the property when sorting. A property with an order of 0 has highest precedence. When defining compound indexes outside of .NET framework, specify the IsKey and Order fields on the SdsTypeProperty object. You can specify a maximum of three properties to define a compound index. In read and write data operations, specify compound indexes in the URI by ordering each property that composes the index separated by the pipe character, ???|???. To help those using compound indexes, .NET client libraries methods also allow the use of tuples for indexes. Note: Compound indexing only applies to types. In other words, there is no compound indexing for secondary indexes that are on streams. For more information, see Streams . The examples below are for compound indexes on types and not of secondary indexes on streams. REST API example //    Read data located between two compound indexes: GET api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Streams/{streamId}/Data?startIndex={firstIndex|secondIndex|thirdIndex}\u0026endIndex={firstIndex|secondIndex|thirdIndex} api v1 Tenants {tenantId} Namespaces {namespaceId} Streams {streamId} Data?startIndex={firstIndex|secondIndex|thirdIndex}\u0026endIndex={firstIndex|secondIndex|thirdIndex} GET api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Streams/{streamId}/Data?startIndex={firstIndex|secondIndex}\u0026endIndex={firstIndex|secondIndex} api v1 Tenants {tenantId} Namespaces {namespaceId} Streams {streamId} Data?startIndex={firstIndex|secondIndex}\u0026endIndex={firstIndex|secondIndex} //    Delete data with a compound index: DELETE api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Streams/{streamId}/Data?index={firstIndex|secondIndex} api v1 Tenants {tenantId} Namespaces {namespaceId} Streams {streamId} Data?index={firstIndex|secondIndex} DELETE api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Streams/{streamId}/Data?startIndex={firstIndex|secondIndex|thirdIndex}\u0026endIndex={firstIndex|secondIndex|thirdIndex} api v1 Tenants {tenantId} Namespaces {namespaceId} Streams {streamId} Data?startIndex={firstIndex|secondIndex|thirdIndex}\u0026endIndex={firstIndex|secondIndex|thirdIndex} .NET examples //    Read data located between two compound indexes: IEnumerable\u003cDerivedCompoundIndex\u003e compoundValues = await client.GetWindowValuesAsync\u003cDerivedCompoundIndex\u003e(compoundStream.Id, 1/20/2017 1 20 2017 01:00|1/20/2017 01:00|1 20 2017 00:00, 1/20/2017 1 20 2017 02:00|1/20/2017 02:00|1 20 2017 14:00); //    Remove data with a compound index: Task RemoveValueAsync(compoundStream.Id, 1/20/2017 1 20 2017 01:00|1/20/2017 01:00|1 20 2017 00:00);"
                                 },
    "content/sds/indexes/indexes-examples.html":  {
                                                      "href":  "content/sds/indexes/indexes-examples.html",
                                                      "title":  "Index examples",
                                                      "keywords":  "Index examples The following examples show how to build an SdsType representation using the provided sample classes in Python and JavaScript. Sample classes The following sample classes are used in the Example SdsType section below. Python class State(Enum): Ok = 0 Warning = 1 Alarm = 2 class Simple(object): Time = property(getTime, setTime) def getTime(self): return self.__time def setTime(self, time): self.__time = time State = property(getState, setState) def getState(self): return self.__state def setState(self, state): self.__state = state Measurement = property(getValue, setValue) def getValue(self): return self.__measurement def setValue(self, measurement): self.__measurement = measurement JavaScript var State = { Ok: 0, Warning: 1, Alarm: 2 } var Simple = function () { this.Time = null; this.State = null; this.Value = null; } Example SdsType The following code is used to build an SdsType representation of the sample class above: Python # Create the properties # Time is the primary key time = SdsTypeProperty() time.Id = \"Time\" time.Name = \"Time\" time.IsKey = True time.SdsType = SdsType() time.SdsType.Id = \"DateTime\" time.SdsType.Name = \"DateTime\" time.SdsType.SdsTypeCode = SdsTypeCode.DateTime # State is not a pre-defined type. An SdsType must be defined to represent the enum stateTypePropertyOk = SdsTypeProperty() stateTypePropertyOk.Id = \"Ok\" stateTypePropertyOk.Measurement = State.Ok stateTypePropertyWarning = SdsTypeProperty() stateTypePropertyWarning.Id = \"Warning\" stateTypePropertyWarning.Measurement = State.Warning stateTypePropertyAlarm = SdsTypeProperty() stateTypePropertyAlarm.Id = \"Alarm\" stateTypePropertyAlarm.Measurement = State.Alarm stateType = SdsType() stateType.Id = \"State\" stateType.Name = \"State\" stateType.Properties = [ stateTypePropertyOk, stateTypePropertyWarning,\\ stateTypePropertyAlarm ] state = SdsTypeProperty() state.Id = \"State\" state.Name = \"State\" state.SdsType = stateType # Measurement property is a simple non-indexed, pre-defined type measurement = SdsTypeProperty() measurement.Id = \"Measurement\" measurement.Name = \"Measurement\" measurement.SdsType = SdsType() measurement.SdsType.Id = \"Double\" measurement.SdsType.Name = \"Double\" # Create the Simple SdsType simple = SdsType() simple.Id = str(uuid.uuid4()) simple.Name = \"Simple\" simple.Description = \"Basic sample type\" simple.SdsTypeCode = SdsTypeCode.Object simple.Properties = [ time, state, measurement ] JavaScript //    Time is the primary key var timeProperty = new SdsObjects.SdsTypeProperty({ \"Id\": \"Time\", \"IsKey\": true, \"SdsType\": new SdsObjects.SdsType({ \"Id\": \"dateType\", \"SdsTypeCode\": SdsObjects.SdsTypeCodeMap.DateTime }) }); //    State is not a pre-defined type. A SdsType must be defined to represent the enum var stateTypePropertyOk = new SdsObjects.SdsTypeProperty({ \"Id\": \"Ok\", \"Value\": State.Ok }); var stateTypePropertyWarning = new SdsObjects.SdsTypeProperty({ \"Id\": \"Warning\", \"Value\": State.Warning }); var stateTypePropertyAlarm = new SdsObjects.SdsTypeProperty({ \"Id\": \"Alarm\", \"Value\": State.Alarm }); var stateType = new SdsObjects.SdsType({ \"Id\": \"State\", \"Name\": \"State\", \"SdsTypeCode\": SdsObjects.SdsTypeCodeMap.Int32Enum, \"Properties\": [stateTypePropertyOk, stateTypePropertyWarning, stateTypePropertyAlarm, stateTypePropertyRed] }); //    Value property is a simple non-indexed, pre-defined type var valueProperty = new SdsObjects.SdsTypeProperty({ \"Id\": \"Value\", \"SdsType\": new SdsObjects.SdsType({ \"Id\": \"doubleType\", \"SdsTypeCode\": SdsObjects.SdsTypeCodeMap.Double }) }); //    Create the Simple SdsType var simpleType = new SdsObjects.SdsType({ \"Id\": \"Simple\", \"Name\": \"Simple\", \"Description\": \"This is a simple Sds type\", \"SdsTypeCode\": SdsObjects.SdsTypeCodeMap.Object, \"Properties\": [timeProperty, stateProperty, valueProperty] }); The Time property is identified as the Key by defining its SdsTypeProperty as follows: Python # Time is the primary key time = SdsTypeProperty() time.Id = \"Time\" time.Name = \"Time\" time.IsKey = True time.SdsType = SdsType() time.SdsType.Id = \"DateTime\" time.SdsType.Name = \"DateTime\" time.SdsType.SdsTypeCode = SdsTypeCode.DateTime JavaScript //    Time is the primary key var timeProperty = new SdsObjects.SdsTypeProperty({ \"Id\": \"Time\", \"IsKey\": true, \"SdsType\": new SdsObjects.SdsType({ \"Id\": \"dateType\", \"SdsTypeCode\": SdsObjects.SdsTypeCodeMap.DateTime }) }); Note: The time.IsKey field is set to true. To read data using the key, define a start index and an end index. For DateTime, use ISO 8601 representation of dates and times. To query for a window of values between January 1, 2022 and February 1, 2022, define indexes as 2022-01-01T08:00:00.000Z and 2022-02-01T08:00:00.000Z , respectively. For additional information, see Reading data ."
                                                  },
    "content/sds/indexes/indexes-in-dotnet.html":  {
                                                       "href":  "content/sds/indexes/indexes-in-dotnet.html",
                                                       "title":  "Indexes in .NET framework",
                                                       "keywords":  "Indexes in .NET framework The following examples are in csharp, but you can apply the concepts such as simple, compound, and secondary indexes to any language. For more information on indexes in JavaScript and Python, see Indexes outside of .NET framework . Simple indexes When working in .NET, use the SdsTypeBuilder together with either the OSIsoft.Sds.SdsMemberAttribute (preferred) or the System.ComponentModel.DataAnnotations.KeyAttribute to identify the property that defines the simple index. Using the SdsTypeBuilder eliminates potential errors that might occur when working with SdsTypes manually. public enum State { Ok, Warning, Alarm } public class Simple { [SdsMember(IsKey = true, Order = 0) ] public DateTime Time { get; set; } public State State { get; set; } public Double Measurement { get; set; } } SdsType simpleType = SdsTypeBuilder.CreateSdsType\u003cSimple\u003e(); To read data that is located between two indexes, define both a start index and an end index. For DateTime , use the ISO 8601 representation of dates and times. For example, to query for a window of simple values between January 1, 2010 and February 1, 2010, you can define indexes and query as follows: IEnumerable\u003cSimple\u003e values = await client.GetWindowValuesAsync\u003cSimple\u003e(simpleStream.Id, \"2010-01-01T08:00:00.000Z\",\"2010-02-01T08:00:00.000Z\"); For more information about querying data, see Read data . Secondary indexes Secondary indexes are defined at the stream level. To add indexes to a stream, add them to the stream Indexes field. For example, to add a second index on Measurement , use the following code: SdsStreamIndex measurementIndex = new SdsStreamIndex() { SdsTypePropertyId = simpleType.Properties.First(p =\u003e p.Id.Equals(\"Measurement\")).Id }; SdsStream secondary = new SdsStream() { Id = \"Simple with Secondary\", TypeId = simpleType.Id, Indexes = new List\u003cSdsStreamIndex\u003e() { measurementIndex } }; secondary = await config.GetOrCreateStreamAsync(secondary); To read data indexed by a secondary index, use a filtered GET method( IEnumerable\u003cSimple\u003e orderedBySecondary = await client.GetFilteredValuesAsync\u003cSimple\u003e(secondary.Id, \"Measurement gt 0 and Measurement lt 6\"); ). Use indexes to order data. On a stream level, you can set the property to be the secondary index. To improve performance when working with a large set of data: Ensure that the property is a secondary index. Use logical operators for filtering. await client.UpdateValuesAsync\u003cSimple\u003e(secondary.Id, new List\u003cSimple\u003e() { new Simple() { Time = time, State = State.Ok, Measurement = 5 }, new Simple() { Time = time + TimeSpan.FromSeconds(1), State = State.Ok, Measurement = 4 }, new Simple() { Time = time + TimeSpan.FromSeconds(2), State = State.Ok, Measurement = 3 }, new Simple() { Time = time + TimeSpan.FromSeconds(3), State = State.Ok, Measurement = 2 }, new Simple() { Time = time + TimeSpan.FromSeconds(4), State = State.Ok, Measurement = 1 }, }); IEnumerable\u003cSimple\u003e orderedByKey = await client.GetWindowValuesAsync\u003cSimple\u003e(secondary.Id, time.ToString(\"o\"), time.AddSeconds(4).ToString(\"o\")); foreach (Simple value in orderedByKey) Console.WriteLine(\"{0}: {1}\", value.Time, value.Measurement); Console.WriteLine(); IEnumerable\u003cSimple\u003e orderedBySecondary = await client.GetFilteredValuesAsync\u003cSimple\u003e(secondary.Id, \"Measurement gt 0 and Measurement lt 6\"); foreach (Simple value in orderedBySecondary) Console.WriteLine(\"{0}: {1}\", value.Time, value.Measurement); Console.WriteLine(); //    Output: //    1/20/2017 1 20 2017 12:00:00 AM: 5 //    1/20/2017 1 20 2017 12:00:01 AM: 4 //    1/20/2017 1 20 2017 12:00:02 AM: 3 //    1/20/2017 1 20 2017 12:00:03 AM: 2 //    1/20/2017 1 20 2017 12:00:04 AM: 1 //    //    1/20/2017 1 20 2017 12:00:04 PM: 1 //    1/20/2017 1 20 2017 12:00:03 PM: 2 //    1/20/2017 1 20 2017 12:00:02 PM: 3 //    1/20/2017 1 20 2017 12:00:01 PM: 4 //    1/20/2017 1 20 2017 12:00:00 PM: 5 Compound indexes Compound indexes are defined using the SdsMemberAttribute as follows: public class Simple { [SdsMember(IsKey = true, Order = 0)] public DateTime Time { get; set; } public State State { get; set; } public Double Measurement { get; set; } } public class DerivedCompoundIndex : Simple { [SdsMember(IsKey = true, Order = 1)] public DateTime Recorded { get; set; } } Events of type DerivedCompoundIndex are sorted first by the Time parameter and then by the Recorded parameter. A collection of times is sorted as follows: Time Recorded Measurement 01:00 00:00 0 01:00 01:00 2 01:00 14:00 5 02:00 00:00 1 02:00 01:00 3 02:00 02:00 4 02:00 14:00 6 If the Order parameter was reversed, with Recorded set to 0 and Time set to 1, the results are sorted as follows: Time Recorded Measurement 01:00 00:00 0 02:00 00:00 1 01:00 01:00 2 02:00 01:00 3 02:00 02:00 4 01:00 14:00 5 02:00 14:00 6 //    estimates at 1/20/2017 1 20 2017 00:00 await client.UpdateValuesAsync(compoundStream.Id, new List\u003cDerivedCompoundIndex\u003e() { new DerivedCompoundIndex() { Time = DateTime.Parse(\"1/20/2017 DateTime.Parse(\"1 20 2017 01:00\"), Recorded = DateTime.Parse(\"1/20/2017 DateTime.Parse(\"1 20 2017 00:00\"), State = State.Ok, Measurement = 0 }, new DerivedCompoundIndex() { Time = DateTime.Parse(\"1/20/2017 DateTime.Parse(\"1 20 2017 02:00\"), Recorded = DateTime.Parse(\"1/20/2017 DateTime.Parse(\"1 20 2017 00:00\"), State = State.Ok, Measurement = 1 }, }); //    measure and estimates at 1/20/2017 1 20 2017 01:00 await client.UpdateValuesAsync(compoundStream.Id, new List\u003cDerivedCompoundIndex\u003e() { new DerivedCompoundIndex() { Time = DateTime.Parse(\"1/20/2017 DateTime.Parse(\"1 20 2017 01:00\"), Recorded = DateTime.Parse(\"1/20/2017 DateTime.Parse(\"1 20 2017 01:00\"), State = State.Ok, Measurement = 2 }, new DerivedCompoundIndex() { Time = DateTime.Parse(\"1/20/2017 DateTime.Parse(\"1 20 2017 02:00\"), Recorded = DateTime.Parse(\"1/20/2017 DateTime.Parse(\"1 20 2017 01:00\"), State = State.Ok, Measurement = 3 }, }); //    measure at 1/20/2017 1 20 2017 02:00 await client.UpdateValuesAsync(compoundStream.Id, new List\u003cDerivedCompoundIndex\u003e() { new DerivedCompoundIndex() { Time = DateTime.Parse(\"1/20/2017 DateTime.Parse(\"1 20 2017 02:00\"), Recorded = DateTime.Parse(\"1/20/2017 DateTime.Parse(\"1 20 2017 02:00\"), State = State.Ok, Measurement = 4 }, }); //    adjust earlier values at 1/20/2017 1 20 2017 14:00 await client.UpdateValuesAsync(compoundStream.Id, new List\u003cDerivedCompoundIndex\u003e() { new DerivedCompoundIndex() { Time = DateTime.Parse(\"1/20/2017 DateTime.Parse(\"1 20 2017 01:00\"), Recorded = DateTime.Parse(\"1/20/2017 DateTime.Parse(\"1 20 2017 14:00\"), State = State.Ok, Measurement = 5 }, new DerivedCompoundIndex() { Time = DateTime.Parse(\"1/20/2017 DateTime.Parse(\"1 20 2017 02:00\"), Recorded = DateTime.Parse(\"1/20/2017 DateTime.Parse(\"1 20 2017 14:00\"), State = State.Ok, Measurement = 6 }, }); var from = new Tuple\u003cDateTime, DateTime\u003e(DateTime.Parse(\"1/20/2017 DateTime\u003e(DateTime.Parse(\"1 20 2017 01:00\"), DateTime.Parse(\"1/20/2017 DateTime.Parse(\"1 20 2017 00:00\")); var to = new Tuple\u003cDateTime, DateTime\u003e(DateTime.Parse(\"1/20/2017 DateTime\u003e(DateTime.Parse(\"1 20 2017 02:00\"), DateTime.Parse(\"1/20/2017 DateTime.Parse(\"1 20 2017 14:00\")); var compoundValues = await client.GetWindowValuesAsync\u003cDerivedCompoundIndex, DateTime, DateTime\u003e(compoundStream.Id, from, to); foreach (DerivedCompoundIndex value in compoundValues) Console.WriteLine(\"{0}:{1} {2}\", value.Time, value.Recorded, value.Measurement); //    Output: //    1/20/2017 1 20 2017 1:00:00 AM:1/20/2017 AM:1 20 2017 12:00:00 AM 0 //    1/20/2017 1 20 2017 1:00:00 AM:1/20/2017 AM:1 20 2017 1:00:00 AM 2 //    1/20/2017 1 20 2017 1:00:00 AM:1/20/2017 AM:1 20 2017 2:00:00 PM 5 //    1/20/2017 1 20 2017 2:00:00 AM:1/20/2017 AM:1 20 2017 12:00:00 AM 1 //    1/20/2017 1 20 2017 2:00:00 AM:1/20/2017 AM:1 20 2017 1:00:00 AM 3 //    1/20/2017 1 20 2017 2:00:00 AM:1/20/2017 AM:1 20 2017 2:00:00 AM 4 //    1/20/2017 1 20 2017 2:00:00 AM:1/20/2017 AM:1 20 2017 2:00:00 PM 6 Note: The GetWindowValuesAsync() call specifies an expected return type and the index types as generic parameters."
                                                   },
    "content/sds/indexes/indexes-outside-dotnet.html":  {
                                                            "href":  "content/sds/indexes/indexes-outside-dotnet.html",
                                                            "title":  "Indexes outside of .NET framework",
                                                            "keywords":  "Indexes outside of .NET framework The following examples are in Python and JavaScript. Simple indexes When the .NET SdsTypeBuilder is unavailable, you must build indexes manually. The following examples show SdsTypes defined in Python and Java Script . For samples in other languages, go to AVEVA Data Hub code samples in GitHub . To build an SdsType representation of sample classes in Python and Java Script, see Sample below: Python JavaScript class State(Enum): Ok = 0 Warning = 1 Alarm = 2 class Simple(object): Time = property(getTime, setTime) def getTime(self): return self.__time def setTime(self, time): self.__time = time State = property(getState, setState) def getState(self): return self.__state def setState(self, state): self.__state = state Measurement = property(getValue, setValue) def getValue(self): return self.__measurement def setValue(self, measurement): self.__measurement = measurement var State = { Ok: 0, Warning: 1, Alarm: 2 } var Simple = function () { this.Time = null; this.State = null; this.Value = null; } Sample The following code is used to build an SdsType representation of the sample class above. Python JavaScript # Create the properties # Time is the primary index time = SdsTypeProperty() time.Id = \"Time\" time.Name = \"Time\" time.IsKey = True time.SdsType = SdsType() time.SdsType.Id = \"DateTime\" time.SdsType.Name = \"DateTime\" time.SdsType.SdsTypeCode = SdsTypeCode.DateTime # State is not a pre-defined type. An SdsType must be defined to represent the enum stateTypePropertyOk = SdsTypeProperty() stateTypePropertyOk.Id = \"Ok\" stateTypePropertyOk.Measurement = State.Ok stateTypePropertyWarning = SdsTypeProperty() stateTypePropertyWarning.Id = \"Warning\" stateTypePropertyWarning.Measurement = State.Warning stateTypePropertyAlarm = SdsTypeProperty() stateTypePropertyAlarm.Id = \"Alarm\" stateTypePropertyAlarm.Measurement = State.Alarm stateType = SdsType() stateType.Id = \"State\" stateType.Name = \"State\" stateType.Properties = [ stateTypePropertyOk, stateTypePropertyWarning,\\ stateTypePropertyAlarm ] state = SdsTypeProperty() state.Id = \"State\" state.Name = \"State\" state.SdsType = stateType # Measurement property is a simple non-indexed, pre-defined type measurement = SdsTypeProperty() measurement.Id = \"Measurement\" measurement.Name = \"Measurement\" measurement.SdsType = SdsType() measurement.SdsType.Id = \"Double\" measurement.SdsType.Name = \"Double\" # Create the Simple SdsType simple = SdsType() simple.Id = str(uuid.uuid4()) simple.Name = \"Simple\" simple.Description = \"Basic sample type\" simple.SdsTypeCode = SdsTypeCode.Object simple.Properties = [ time, state, measurement ] //    Time is the primary key var timeProperty = new SdsObjects.SdsTypeProperty({ \"Id\": \"Time\", \"IsKey\": true, \"SdsType\": new SdsObjects.SdsType({ \"Id\": \"dateType\", \"SdsTypeCode\": SdsObjects.SdsTypeCodeMap.DateTime }) }); //    State is not a pre-defined type. SdsType must be defined to represent the enum var stateTypePropertyOk = new SdsObjects.SdsTypeProperty({ \"Id\": \"Ok\", \"Value\": State.Ok }); var stateTypePropertyWarning = new SdsObjects.SdsTypeProperty({ \"Id\": \"Warning\", \"Value\": State.Warning }); var stateTypePropertyAlarm = new SdsObjects.SdsTypeProperty({ \"Id\": \"Alarm\", \"Value\": State.Alarm }); var stateType = new SdsObjects.SdsType({ \"Id\": \"State\", \"Name\": \"State\", \"SdsTypeCode\": SdsObjects.SdsTypeCodeMap.Int32Enum, \"Properties\": [stateTypePropertyOk, stateTypePropertyWarning, stateTypePropertyAlarm, stateTypePropertyRed] }); //    Value property is a simple non-indexed, pre-defined type var valueProperty = new SdsObjects.SdsTypeProperty({ \"Id\": \"Value\", \"SdsType\": new SdsObjects.SdsType({ \"Id\": \"doubleType\", \"SdsTypeCode\": SdsObjects.SdsTypeCodeMap.Double }) }); //    Create the Simple SdsType var simpleType = new SdsObjects.SdsType({ \"Id\": \"Simple\", \"Name\": \"Simple\", \"Description\": \"This is a simple Sds type\", \"SdsTypeCode\": SdsObjects.SdsTypeCodeMap.Object, \"Properties\": [timeProperty, stateProperty, valueProperty] }); The Time property is identified as the primary index by defining its SdsTypeProperty as follows: Python JavaScript # Time is the primary index time = SdsTypeProperty() time.Id = \"Time\" time.Name = \"Time\" time.IsKey = True time.SdsType = SdsType() time.SdsType.Id = \"DateTime\" time.SdsType.Name = \"DateTime\" time.SdsType.SdsTypeCode = SdsTypeCode.DateTime //    Time is the primary index var timeProperty = new SdsObjects.SdsTypeProperty({ \"Id\": \"Time\", \"IsKey\": true, \"SdsType\": new SdsObjects.SdsType({ \"Id\": \"dateType\", \"SdsTypeCode\": SdsObjects.SdsTypeCodeMap.DateTime }) }); Note that the time.IsKey field is set to true. To read data using the index, you define a start index and an end index. For DateTime , use the ISO 8601 representation of dates and times. To query for a window of values between January 1, 2010 and February 1, 2010, define indexes as ???2010-01-01T08:00:00.000Z??? and ???2010-02-01T08:00:00.000Z???, respectively. For additional information, see Read data . Secondary indexes Secondary indexes are defined at the stream level. To create a stream using the Simple class and adding a secondary index on the Measurement , use the previously defined type. Then create SdsStreamIndex specifying the Measurement property and define a stream identifying the Measurement as the secondary index as shown in the following example. Python JavaScript # Create the properties measurementIndex = SdsStreamIndex() measurementIndex.SdsTypePropertyId = measurement.Id stream = SdsStream() stream.Id = str(uuid.uuid4()) stream.Name = \"SimpleWithSecond\" stream.Description = \"Simple with secondary index\" stream.TypeId = simple.Id stream.Indexes = [ measurementIndex ] var measurementIndex = new SdsObjects.SdsStreamIndex({ \"SdsTypePropertyId\": valueProperty.Id }); var stream = new SdsObjects.SdsStream({ \"Id\": \"SimpleWithSecond\", \"Name\": \"SimpleWithSecond\", \"Description\": \"Simple with secondary index\", \"TypeId\": simpleTypeId, \"Indexes\": [ measurementIndex ] }); Compound indexes Consider the following Python and JavaScript types. Python JavaScript class Simple(object): # First-order index property Time = property(getTime, setTime) def getTime(self): return self.__time def setTime(self, time): self.__time = time State = property(getState, setState) def getState(self): return self.__state def setState(self, state): self.__state = state Measurement = property(getValue, setValue) def getValue(self): return self.__measurement def setValue(self, measurement): self.__measurement = measurement class DerivedCompoundIndex(Simple): # Second-order index property @property def Recorded(self): return self.__recorded @Recorded.setter def Recorded(self, recorded): self.__recorded = recorded var Simple = function () { this.Time = null; this.State = null; this.Value = null; } var DerivedCompoundIndex = function() { Simple.call(this); this.Recorded = null; } To turn the simple type shown in the example into a type supporting the DerivedCompoundIndex type with a compound index based on the Simple.Time and DerivedCompoundIndex.Recorded , extend the type as shown in the following example. Python JavaScript # We set the order for this property. The order of the first property defaulted to 0 recorded = SdsTypeProperty() recorded.Id = \"Recorded\" recorded.Name = \"Recorded\" recorded.IsKey = True recorded.Order = 1 recorded.SdsType = SdsType() recorded.SdsType.Id = \"DateTime\" recorded.SdsType.Name = \"DateTime\" recorded.SdsType.SdsTypeCode = SdsTypeCode.DateTime # Create the Derived SdsType derived = SdsType() derived.Id = str(uuid.uuid4()) derived.Name = \"Compound\" derived.Description = \"Derived compound index sample type\" derived.BaseType = simple derived.SdsTypeCode = SdsTypeCode.Object derived.Properties = [ recorded ] //    We set the order for this property. The order of the first property defaulted to 0 var recordedProperty = new SdsObjects.SdsTypeProperty({ \"Id\": \"Recorded\", \"Name\": \"Recorded\", \"IsKey\": true, \"Order\": 1, \"SdsType\": new SdsObjects.SdsType({ \"Id\": \"DateTime\", \"Name\": \"DateTime\", \"SdsTypeCode\": SdsObjects.SdsTypeCodeMap.DateTime }) }); //    Create the Derived SdsType var derivedType = new SdsObjects.SdsTyp({ \"Id\": \"Compound\", \"Name\": \"Compound\", \"Description\": \"Derived compound index sample type\", \"BaseType\": simpleType, \"SdsTypeCode\": SdsObjects.SdsTypeCodeMap.Object, \"Properties\": [recordedProperty] }); Data in the stream will be ordered as shown in the following table. Time Recorded Measurement 01:00 00:00 0 01:00 01:00 2 01:00 14:00 5 02:00 00:00 1 02:00 01:00 3 02:00 02:00 4 02:00 14:00 6 If the Order parameters were reversed with Recorded set to 0 and Time set to 1 , the results would be sorted as shown in the following table. Time Recorded Measurement 01:00 00:00 0 02:00 00:00 1 01:00 01:00 2 02:00 01:00 3 02:00 02:00 4 01:00 14:00 5 02:00 14:00 6"
                                                        },
    "content/sds/read-data.html":  {
                                       "href":  "content/sds/read-data.html",
                                       "title":  "Read data",
                                       "keywords":  "Read data The .NET and REST APIs provide programmatic access to read and write data. This section identifies and describes the APIs used to read SdsStreams data. Results are influenced by SdsTypes , SdsStreamViews , filter expressions , and table format . If you are working in a .NET environment, convenient SDS Client Libraries are available. The ISdsDataService interface, which is accessed using the SdsService.GetDataService() helper, defines the functions that are available. Reading data from streams While SDS is a robust data storage, it performs best if you follow certain guidelines: Maximum limit for events in read data calls Increase the Request-Timeout in the header Enable compression Use available read data APIs Maximum limit for events in read data calls Read data API is limited to retrieve less than 250,000 events per request. An error message is returned when the maximum limit is reached. This maximum limit applies to List Values , List Summaries , List Sampled Values . 400 bad request error { \"Error\": \"The request is not valid.\", \"Reason\": \"Exceeded the maximum return count of 250000 events.\" \"Resolution\": \"Reduce query size and resubmit the request.\" } Increase the Request-Timeout in the header Increase the Request-Timeout in the header to 5 minutes for large range calls that are requesting 250,000 events in a read call. The gateway will send 408 - Operation timed out error if the request needs more than 30 seconds. The range of values that are held in memory can be large and be anywhere between 1 GB and 2 GB, so the system needs enough time to read and return the data. If multiple calls return 408 - Operation timed out error even after increasing the timeout limit to 5 minutes, do one of the following: Reduce the range in the request calls of this type Retry with an exponential back-off policy Compression Include Accept-Encoding: gzip, deflate in the HTTP header. This enables compression. For more information, see Compression . Use available read data APIs Depending on the scenario, there are different read data APIs available. They return an overview of the values instead of reading all values at once. These APIs provide a good high-level view of the values without displaying them all at the same time: List Values with filters Get Summaries Get Sampled Values Single stream reads The following methods for reading a single value are available: Get First Value returns the first value in the stream. Get Last Value returns the last value in the stream. Find Distinct Value returns a value based on a starting index and search criteria. In addition, the following methods support reading multiple values: List Values retrieves a collection of stored values based on the request parameters. List Interpolated Values retrieves a collection of stored or calculated values based on the request parameters. Get Summaries retrieves a collection of evenly spaced summary intervals based on a count and specified start and end indexes. Get Sampled Values retrieves a collection of sampled data based on the request parameters. Bulk reads SDS supports reading from multiple streams in one request. The following method for reading data from multiple streams is available: Join Values retrieves a collection of events across multiple streams and joins the results based on the request parameters. SdsBoundaryType The SdsBoundaryType enum defines how data on the boundary of queries is handled???around the start index for range value queries and around the start and end index for window values. The following are valid values for SdsBoundaryType : Boundary Enumeration value Operation Exact 0 Results include the event at the specified index boundary if a stored event exists at that index. Inside 1 Results include only events within the index boundaries Outside 2 Results include up to one event that falls immediately outside of the specified index boundary. ExactOrCalculated 3 Results include the event at the specified index boundary. If no stored event exists at that index, one is calculated based on the index type and interpolation and extrapolation settings. SdsSearchMode The SdsSearchMode enum defines search behavior when seeking a stored event near a specified index. The following table shows valid values for SdsSearchMode . Mode Enumeration value Operation Exact 0 If a stored event exists at the specified index, that event is returned. Otherwise, no event is returned. ExactOrNext 1 If a stored event exists at the specified index, that event is returned. Otherwise, the next event in the stream is returned. Next 2 Returns the stored event after the specified index. ExactOrPrevious 3 If a stored event exists at the specified index, that event is returned. Otherwise, the previous event in the stream is returned. Previous 4 Returns the stored event before the specified index."
                                   },
    "content/sds/read-data/data-transformations.html":  {
                                                            "href":  "content/sds/read-data/data-transformations.html",
                                                            "title":  "Data transformations",
                                                            "keywords":  "Data transformations SDS supports the following data transformations on read requests: Reading with SdsStreamViews: Changing the shape of the returned data Unit of Measure Conversions: Converting the unit of measure of the data Data transformations are supported for all single stream reads, but transformations have specific endpoints. The following are the base URIs for the transformation endpoints: api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data/Transform/First api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data Transform First api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data/Transform/Last api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data Transform Last api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data/Transform api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data Transform api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data/Transform/Interpolated api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data Transform Interpolated api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data/Transform/Summaries api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data Transform Summaries api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data/Transform/Sampled api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data Transform Sampled Reading with SdsStreamViews When you transform data with an SdsStreamView, the data read is converted to the target type specified in the SdsStreamView. For details on working with stream views, see Stream Views . All stream view transformations are GET HTTP requests. Specify the stream view by appending the stream view identifier to requests to the transformation endpoint. For example, the following request would return the first event in the stream as the target type in the stream view specified by the streamViewId : GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data/Transform/First?streamViewId={streamViewId} api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data Transform First?streamViewId={streamViewId} All single stream data reads support stream view transformations. When you request data with an SdsStreamView, the read characteristics defined by the target type of the SdsStreamView determine what is returned. The read characteristics are discussed in the code samples. Unit of measure conversions SDS supports assigning units of measure (UOM) to stream data. For more information, see Units of measure . If stream data has UOM information associated, SDS supports reading data with unit conversions applied. On each read data request, unit conversions are specified by a user defined collection of SdsStreamPropertyOverride objects in read requests. The properties of the SdsStreamPropertyOverride object are described in the following table. Property Type Optionality Description SdsTypePropertyId String Required Identifier for an SdsTypeProperty with a UOM assigned. Uom String Required Target unit of measure. InterpolationMode SdsInterpolationMode N/A N A Currently not supported in context of data reads. This is supported in the REST API through HTTP POST calls with a request body containing a collection of SdsStreamPropertyOverride objects. All unit conversions are POST HTTP requests. The unit conversion transformation URI is as follows: POST api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data/Transform api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data Transform Request body The Request Body contains a collection of SdsStreamPropertyOverride objects. The example request body below requests SDS to convert the Measurement property of the returned data from meter to centimeter. [ { \"SdsTypePropertyId\" : \"Measurement\", \"Uom\" : \"centimeter\" } ] All single stream data reads with streams that have specified UOMs support UOM conversions."
                                                        },
    "content/sds/read-data/enums.html":  {
                                             "href":  "content/sds/read-data/enums.html",
                                             "title":  "Enums",
                                             "keywords":  "Enums Enums, or enumerated types, are variable types that have a limited set of possible values. The SDS read APIs support the following enums: SdsBoundaryType SdsSearchMode SdsBoundaryType The SdsBoundaryType enum defines how data on the boundary of queries is handled: around the start index for range value queries, and around the start and end index for window values. The following table describes valid SdsBoundaryType values. Boundary Enumeration value Operation Exact 0 Results include the event at the specified index boundary if a stored event exists at that index. Inside 1 Results include only events within the index boundaries. Outside 2 Results include up to one event that falls immediately outside of the specified index boundary. ExactOrCalculated 3 Results include the event at the specified index boundary. If no stored event exists at that index, one is calculated based on the index type and interpolation and extrapolation settings. SdsSearchMode The SdsSearchMode enum defines search behavior when seeking a stored event near a specified index. The following table describes valid values for SdsSearchMode . Mode Enumeration value Operation Exact 0 If a stored event exists at the specified index, that event is returned. Otherwise, no event is returned. ExactOrNext 1 If a stored event exists at the specified index, that event is returned. Otherwise, the next event in the stream is returned. Next 2 Returns the stored event after the specified index ExactOrPrevious 3 If a stored event exists at the specified index, that event is returned. Otherwise, the previous event in the stream is returned. Previous 4 Returns the stored event before the specified index"
                                         },
    "content/sds/read-data/filter-expressions.html":  {
                                                          "href":  "content/sds/read-data/filter-expressions.html",
                                                          "title":  "Filter expressions",
                                                          "keywords":  "Filter expressions Apply filter expressions to any read that returns multiple values, including Get Values , Get Range Values , Get Window Values , and Get Intervals to further control the results of the read. The filter expression is applied to the collection events conditionally filtering events that do not meet the filter conditions. SdsTypeCodes The following tables lists supported and unsupported types within a filter expression. Supports Does not support Boolean Array Byte IEnumerable Char IDictionary DateTime IList DateTimeOffset Decimal Double Nullable Types Guid Int16 Int32 Int64 Sbyte String Timespan UInt16 UInt32 UInt64 Logical operators The following table shows the supported logical operators for use within a filter expression. Operator Description eq Equal to ne Not equal ge Greater than or equal to le Less than or equal to lt Less than gt Greater than ( ) Parenthesis can be used to affect the order of the operation or Or logical operator and And logical operator not Not logical operator - Negation Logical operator examples For the following examples, assume that the SDS Type event includes a field named Value of type double : Value eq 1.0 Value ne 15.6 Value ge 5.0 Value le 8.0 Value gt 5.0 Value lt 4.0 Value gt 2.0 and Value lt 9.0 Value gt 6.0 or Value lt 2.0 not (Value eq 1.0) Math functions The following tables shows supported math functions within a filter expression. Function Description add Addition sub Subtraction mul Multiplication div Division mod Modulo round Rounds to the nearest numeric component without a decimal, with the midpoint rounded away from 0. For example, 0.5 rounds to 1; -0.5 rounds to -1). floor Rounds down to the nearest numeric component without a decimal ceiling Rounds up to the nearest numeric component without a decimal Math function examples For the following examples, assume that the SdsType event includes a field named Value of type double : Value eq (6.0 add 3.0) Value eq (6.0 sub 3.0) Value eq (6.0 mul 3.0) Value eq (6.0 div 3.0) Value eq (7.0 mod 3.0) round(Value) eq 16 floor(Value) eq 15 ceiling(Value) eq 16 String functions String operations are case sensitive. The character index in a string is zero-based. The following table shows supported string functions within a filter expression. Function Description endswith Compare the character at the end of the input string startswith Compare the character at the start of the input string length Examines the string length indexof Examines the character starting at a given index substring Examine characters within another string at a specific location contains Search for characters anywhere in another string tolower Convert characters to lowercase toupper Convert characters to uppercase trim Remove whitespace from front and end of a string concat Concatenate strings together replace Replace one set of characters with another String function examples For the following examples, assume that the SDS Type event includes a field named sValue of type string : Example Result endswith(sValue, \u0027XYZ\u0027) True if sValue ends with the characters ???XYZ??? startswith(sValue, \u0027Val\u0027 True if sValue starts with the characters ???Val??? length(sValue) eq 11 True if sValue is 11 characters indexof(sValue, \u0027ab\u0027) eq 4 True if the 5th and 6th characters are ???ab??? contains(sValue, \u0027ab\u0027) True if characters ???ab??? are found anywhere in sValue substring(sValue, 10) eq \u0027a b\u0027 True if ???a b??? is found in sValue at index 10 tolower(sValue) eq \u0027val5\u0027 Change sValue to lowercase and compare to ???val5??? toupper(sValue) eq \u0027ABC\u0027 Change sValue to uppercase and compare to ???ABC??? trim(sValue) eq \u0027vall22\u0027 Trim whitespace from front and end of sValue and compare to ???val22??? concat(sValue,\u0027xyz\u0027) eq \u0027dataValue_7xyz\u0027 Add characters to sValue and compare to ???dataValue_7xyz??? replace(sValue,\u0027L\u0027,\u0027D\u0027) eq \u0027Dog1\u0027 Replace any ???L??? in sValue with ???D??? and compare to ???Dog1??? DateTime functions The following table shows supported DateTime functions within a filter expression. Function Description year Get year value from DateTime month Get month value from DateTime day Get day value from DateTime hour Get hour value from DateTime minute Get minute value from DateTime second Get second value from DateTime DateTime function examples For the following examples, assume that the SDS Type event includes a field named TimeId of type DateTime : year(TimeId) eq 2015 month(TimeId) eq 11 day(TimeId) eq 3 hour(TimeId) eq 1 minute(TimeId) eq 5 second(TimeId) eq 3 TimeSpan functions The following table shows supported TimeSpan functions for use within a filter expression. Function Description years Get year value from TimeSpan days Get day value from TimeSpan hours Get hour value from TimeSpan minutes Get minute value from TimeSpan seconds Get second value from TimeSpan TimeSpan function examples For the following examples, assume that the SDS Type event includes a field named TimeSpanValue of type TimeSpan : years(TimeSpanValue) eq 1 days(TimeSpanValue) eq 22 hours(TimeSpanValue) eq 1 minutes(TimeSpanValue) eq 1 seconds(TimeSpanValue) eq 2"
                                                      },
    "content/sds/read-data/read-characteristics.html":  {
                                                            "href":  "content/sds/read-data/read-characteristics.html",
                                                            "title":  "Read characteristics",
                                                            "keywords":  "Read characteristics When data is requested at an index for which no stored event exists, the read characteristics determine whether the result is an error, no event, interpolated event, or extrapolated event. The combination of the type of the index and the interpolation and extrapolation modes of the SdsType and the SdsStream determine the read characteristics. Interpolation Interpolation determines how a stream behaves when asked to return an event at an index between two existing events. InterpolationMode determines how the returned event is constructed. The following table lists interpolation modes. Interpolation Mode Enumeration value Operation Default 0 The default InterpolationMode is Continuous. Continuous 0 Interpolates the data using previous and next index values. StepwiseContinuousLeading 1 Returns the data from the previous index. StepwiseContinuousTrailing 2 Returns the data from the next index. Discrete 3 Returns null . ContinuousNullableLeading 4 Returns interpolated data or data from the previous index if either of the surrounding indexes has a null value ContinuousNullableTrailing 5 Returns interpolated data or data from the trailing index if either of the surrounding indexes has a null value Note: Continuous mode cannot return events for values that cannot be interpolated, such as when the type is not numeric. The following table describes how the Continuous interpolation mode affects indexes that occur between data in a stream. InterpolationMode = Continuous or Default Type Result for an index between data in a stream Comment Numeric Types Interpolated 1 Rounding is done as needed for integer types. Time related Types Interpolated DateTime, DateTimeOffset, TimeSpan Nullable Types Interpolated 2 Limited support for nullable numeric types. Array and List Types No event is returned String Type No event is returned Boolean Type Returns value of nearest index Enumeration Types Returns Enum value at 0 This may have a value for the enumeration. GUID No event is returned Version No event is returned IDictionary or IEnumerable No event is returned Dictionary, Array, List, and so on. Empty Type Not supported Object Type Not supported 1 When extreme values are involved in an interpolation (for example, Decimal.MaxValue ) the call might result in a BadRequest exception. 2 For the Continuous interpolation mode, nullable types are interpolated in the same manner as their non-nullable equivalents as long as the values surrounding the requested interpolation index are non-null. If either of the values are null, the interpolated value will be null. If the InterpolationMode is not assigned, the events are interpolated in the default manner, unless the interpolation mode is overridden in the SdsTypeProperty or the SdsStream . For more information on overriding the interpolation mode on a specific type property, see SdsTypeProperty . For more information on overriding the interpolation mode for a specific stream, see Streams . Extrapolation Extrapolation defines how a stream responds to requests with indexes that precede or follow all data in the steam. ExtrapolationMode acts as a master switch to determine whether extrapolation occurs and at which end of the data. ExtrapolationMode works with the InterpolationMode to determine how a stream responds. The following tables show how ExtrapolationMode affects returned values for each InterpolationMode value. If the ExtrapolationMode is not assigned, the events are extrapolated in the default manner, unless the extrapolation mode is overridden on the SdsStream . For more information on overriding the extrapolation mode on a specific stream, see Streams . ExtrapolationMode with InterpolationMode = Default (or Continuous), StepwiseContinuousLeading and StepwiseContinuousTrailing ExtrapolationMode Enumeration value Index before data Index after data All 0 Returns first data value Returns last data value. None 1 No event is returned No event is returned. Forward 2 No event is returned Returns last data value. Backward 3 Returns first data value No event is returned. ExtrapolationMode with InterpolationMode = Discrete ExtrapolationMode Enumeration value Index before data Index after data All 0 No event is returned. No event is returned. None 1 No event is returned. No event is returned. Forward 2 No event is returned. No event is returned. Backward 3 No event is returned. No event is returned. For additional information about the effect of read characteristics for the available read methods, see Read data API ."
                                                        },
    "content/sds/read-data/reading-data-api.html":  {
                                                        "href":  "content/sds/read-data/reading-data-api.html",
                                                        "title":  "Read data API",
                                                        "keywords":  "Read data API The following example API calls show different methods for reading data. Example type, stream, and data Many of the API methods described below contain example requests and responses in JSON to highlight usage and specific behaviors. The following type, stream, and data are used in the examples: Example type SimpleType is an SdsType with a single index. This type is defined in Python and Javascript: .NET Python JavaScript public enum State { Ok, Warning, Alarm } public class SimpleType { [SdsMember(IsKey = true, Order = 0) ] public DateTime Time { get; set; } public State State { get; set; } [SdsMember(Uom = \"meter\")] public Double Measurement { get; set; } } class State(Enum): Ok = 0 Warning = 1 Alarm = 2 class SimpleType(object): Time = property(getTime, setTime) def getTime(self): return self.__time def setTime(self, time): self.__time = time State = property(getState, setState) def getState(self): return self.__state def setState(self, state): self.__state = state Measurement = property(getValue, setValue) def getValue(self): return self.__measurement def setValue(self, measurement): self.__measurement = measurement var State = { Ok: 0, Warning: 1, Alarm: 2, } var SimpleType = function () { this.Time = null; this.State = null; this.Value = null; } Example Stream Simple is an SdsStream of type SimpleType . Example Data Simple has stored values as follows: 11/23/2017 11 23 2017 12:00:00 PM: Ok 0 11/23/2017 11 23 2017 1:00:00 PM: Ok 10 11/23/2017 11 23 2017 2:00:00 PM: Ok 20 11/23/2017 11 23 2017 3:00:00 PM: Ok 30 11/23/2017 11 23 2017 4:00:00 PM: Ok 40 All times are represented at offset 0, GMT. Get First Value Returns the first value in the stream. If no values exist in the stream, null is returned Request GET /api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Streams/{streamId}/Data/First  api v1 Tenants {tenantId} Namespaces {namespaceId} Streams {streamId} Data First Parameters string tenantId Tenant identifier. string namespaceId Namespace identifier. string streamId Stream identifier. Response Status Code Body Type Description 200 Inline Returns a serialized event 400 ErrorResponseBody Missing or invalid inputs 401 ErrorResponseBody Unauthorized 403 ErrorResponseBody Forbidden 404 ErrorResponseBody One of the resources specified was not found. 500 ErrorResponseBody An error occurred while processing the request. 503 ErrorResponseBody Service Unavailable Example response body 400 Response ( ErrorResponseBody ) { \"OperationId\": \"string\", \"Error\": \"string\", \"Reason\": \"string\", \"Resolution\": \"string\", \"Parameters\": { \"property1\": \"string\", \"property2\": \"string\" } } Get Last Value Returns the last value in the stream. If no values exist in the stream, null is returned. Request GET /api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Streams/{streamId}/Data/Last  api v1 Tenants {tenantId} Namespaces {namespaceId} Streams {streamId} Data Last Parameters string tenantId Tenant identifier. string namespaceId Namespace identifier. string streamId Stream identifier. Response Status Code Body Type Description 200 Inline Returns a serialized event 400 ErrorResponseBody Missing or invalid inputs 401 ErrorResponseBody Unauthorized 403 ErrorResponseBody Forbidden 404 ErrorResponseBody One of the resources specified was not found. 500 ErrorResponseBody An error occurred while processing the request. 503 ErrorResponseBody Service Unavailable Example response body 400 Response ( ErrorResponseBody ) { \"OperationId\": \"string\", \"Error\": \"string\", \"Reason\": \"string\", \"Resolution\": \"string\", \"Parameters\": { \"property1\": \"string\", \"property2\": \"string\" } } Find Distinct Value Returns a stored event based on the specified index and searchMode Request GET api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Streams/{streamId}/Data api v1 Tenants {tenantId} Namespaces {namespaceId} Streams {streamId} Data ?index={index}\u0026searchMode={searchMode} Parameters string tenantId Tenant identifier string namespaceId Namespace identifier string streamId Stream identifier string index The index string searchMode The SdsSearchMode , the default is exact Response The response includes a status code and a response body containing a serialized collection with one event. Depending on the request index and searchMode , it is possible to have an empty collection returned. Example request GET api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Streams/Simple/Data api v1 Tenants {tenantId} Namespaces {namespaceId} Streams Simple Data ?index=2017-11-23T13:00:00Z\u0026searchMode=Next The request has an index that matches the index of an existing event, but since a SdsSearchMode of next was specified, the response contains the next event in the stream after the specified index: Example response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Time\": \"2017-11-23T14:00:00Z\", \"State\": 0, \"Measurement\": 20 } ] Example request GET api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Streams/Simple/Data api v1 Tenants {tenantId} Namespaces {namespaceId} Streams Simple Data ?index=2017-11-23T13:30:00Z\u0026searchMode=Next The request specifies an index that does not match an index of an existing event. The next event in the stream is retrieved. Example response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Time\": \"2017-11-23T14:00:00Z\", \"State\": 0, \"Measurement\": 20 } ] List Values Returns a collection of stored values at indexes based on request parameters SDS supports three ways of specifying which stored events to return: Filtered : A filtered request accepts a filter expression . Range : A range request accepts a start index and a count. Window : A window request accepts a start index and end index. This request has an optional continuation token for large collections of events. Filtered Returns a collection of stored values as determined by a filter . The filter limits results by applying an expression against event fields. Filter expressions are explained in detail in the Filter expressions section. Request GET api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Streams/{streamId}/Data api v1 Tenants {tenantId} Namespaces {namespaceId} Streams {streamId} Data ?filter={filter} Parameters string tenantId Tenant identifier string namespaceId Namespace identifier string streamId Stream identifier string filter Filter expression (see Filter expressions ) Response Status Code Body Type Description 200 Inline Returns a serialized collection of events 400 ErrorResponseBody Missing or invalid inputs 401 ErrorResponseBody Unauthorized 403 ErrorResponseBody Forbidden 404 ErrorResponseBody One of the resources specified was not found 500 ErrorResponseBody An error occurred while processing the request 503 ErrorResponseBody Service Unavailable Example request GET api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Streams/Simple/Data api v1 Tenants {tenantId} Namespaces {namespaceId} Streams Simple Data ?filter=Measurement gt 10 The events in the stream with Measurement greater than 10 are returned. Example response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Time\": \"2017-11-23T14:00:00Z\", \"Measurement\": 20 }, { \"Time\": \"2017-11-23T15:00:00Z\", \"Measurement\": 30 }, { \"Time\": \"2017-11-23T16:00:00Z\", \"Measurement\": 40 } ] Note that State is not included in the JSON as its value is the default value. Range Returns a collection of stored values as determined by a startIndex and count . Additional optional parameters specify the direction of the range, how to handle events near or at the start index, whether to skip a certain number of events at the start of the range, and how to filter the data. Request GET api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Streams/{streamId}/Data api v1 Tenants {tenantId} Namespaces {namespaceId} Streams {streamId} Data ?startIndex={startIndex}\u0026count={count}[\u0026skip={skip}\u0026reversed={reversed} \u0026boundaryType={boundaryType}\u0026filter={filter}] Parameters string tenantId Tenant identifier string namespaceId Namespace identifier string streamId Stream identifier string startIndex Index identifying the beginning of the series of events to return int count The number of events to return int skip Optional value specifying the number of events to skip at the beginning of the result bool reversed Optional specification of the direction of the request. By default, range requests move forward from startIndex, collecting events after startIndex from the stream. A reversed request collects events before startIndex from the stream. SdsBoundaryType boundaryType Optional SdsBoundaryType specifies the handling of events at or near startIndex string filter Optional filter expression Response The response includes a status code and a response body containing a serialized collection of events. Example request GET api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Streams/Simple/Data api v1 Tenants {tenantId} Namespaces {namespaceId} Streams Simple Data ?startIndex=2017-11-23T13:00:00Z\u0026count=100 This request returns a response with up to 100 events starting at 13:00 and extending forward toward the end of the stream: Example response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Time\": \"2017-11-23T13:00:00Z\", \"Measurement\": 10 }, { \"Time\": \"2017-11-23T14:00:00Z\", \"Measurement\": 20 }, { \"Time\": \"2017-11-23T15:00:00Z\", \"Measurement\": 30 }, { \"Time\": \"2017-11-23T16:00:00Z\", \"Measurement\": 40 } ] Note that State is not included in the JSON as its value is the default value. Example request To reverse the direction of the request, set reversed to true . The following request returns up to 100 events starting at 13:00 and extending back toward the start of the stream: GET api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Streams/Simple/Data api v1 Tenants {tenantId} Namespaces {namespaceId} Streams Simple Data ?startIndex=2017-11-23T13:00:00Z\u0026count=100\u0026reversed=true Example response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Time\": \"2017-11-23T13:00:00Z\", \"Measurement\": 10 }, { \"Time\": \"2017-11-23T12:00:00Z\" } ] Note that State is not included in the JSON as its value is the default value. Further, Measurement is not included in the second, 12:00:00, event as zero is the default value for numbers. The following request specifies a boundary type of Outside for a reversed-direction range request. The response will contain up to 100 events. The boundary type Outside indicates that up to one event outside the boundary will be included in the response. For a reverse direction range request, this means one event forward of the specified start index. In a default direction range request, it would mean one event before the specified start index. Example request GET api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Streams/Simple/Data api v1 Tenants {tenantId} Namespaces {namespaceId} Streams Simple Data ?startIndex=2017-11-23T13:00:00Z\u0026count=100\u0026reversed=true \u0026boundaryType=2 Example response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Time\": \"2017-11-23T14:00:00Z\", \"State\": 0, \"Measurement\": 20 }, { \"Time\": \"2017-11-23T13:00:00Z\", \"State\": 0, \"Measurement\": 10 }, { \"Time\": \"2017-11-23T12:00:00Z\", \"State\": 0, \"Measurement\": 0 } ] The event outside of the index is the next event or the event at 14:00 because the request operates in reverse. Example request Adding a filter to the request means only events that meet the filter criteria are returned: GET api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Streams/Simple/Data api v1 Tenants {tenantId} Namespaces {namespaceId} Streams Simple Data ?startIndex=2017-11-23T13:00:00Z\u0026count=100\u0026reversed=true \u0026boundaryType=2\u0026filter=Measurement gt 10 Example response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Time\": \"2017-11-23T14:00:00Z\", \"State\": 0, \"Measurement\": 20 } ] Window Returns a collection of stored events based on the specified startIndex and endIndex For handling events at and near the boundaries of the window, a single SdsBoundaryType that applies to both the start and end indexes can be passed with the request, or separate boundary types may be passed for the start and end individually. Paging is supported for window requests with a large number of events. To retrieve the next page of values, include the continuationToken from the results of the previous request. For the first request, specify a null or empty string for the continuationToken . Requests GET api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Streams/{streamId}/Data api v1 Tenants {tenantId} Namespaces {namespaceId} Streams {streamId} Data ?startIndex={startIndex}\u0026endIndex={endIndex} GET api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Streams/{streamId}/Data api v1 Tenants {tenantId} Namespaces {namespaceId} Streams {streamId} Data ?startIndex={startIndex}\u0026endIndex={endIndex}\u0026boundaryType={boundaryType} GET api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Streams/{streamId}/Data api v1 Tenants {tenantId} Namespaces {namespaceId} Streams {streamId} Data ?startIndex={startIndex}\u0026startBoundaryType={startBoundaryType} \u0026endIndex={endIndex}\u0026endBoundaryType={endBoundaryType} GET api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Streams/{streamId}/Data api v1 Tenants {tenantId} Namespaces {namespaceId} Streams {streamId} Data ?startIndex={startIndex}\u0026endIndex={endIndex} \u0026count={count}\u0026continuationToken={continuationToken} GET api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Streams/{streamId}/Data api v1 Tenants {tenantId} Namespaces {namespaceId} Streams {streamId} Data ?startIndex={startIndex}\u0026startBoundaryType={startBoundaryType} \u0026endIndex={endIndex}\u0026endBoundaryType={endBoundaryType}\u0026filter={filter}\u0026count={count} \u0026continuationToken={continuationToken} Parameters string tenantId Tenant identifier string namespaceId Namespace identifier string streamId Stream identifier string startIndex Index bounding the beginning of the series of events to return string endIndex Index bounding the end of the series of events to return int count Optional maximum number of events to return. If count is specified, a continuationToken must also be specified. SdsBoundaryType boundaryType Optional SdsBoundaryType specifies handling of events at or near the start and end indexes SdsBoundaryType startBoundaryType Optional SdsBoundaryType specifies the first value in the result in relation to the start index. If startBoundaryType is specified, endBoundaryType must be specified. SdsBoundaryType endBoundaryType Optional SdsBoundaryType specifies the last value in the result in relation to the end index. If startBoundaryType is specified, endBoundaryType must be specified. string filter Optional filter expression string continuationToken Optional token used to retrieve the next page of data. If count is specified, a continuationToken must also be specified. Response The response includes a status code and a response body containing a serialized collection of events. A continuation token can be returned if specified in the request. Example request The following requests all stored events between 12:30 and 15:30: GET api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Streams/Simple/Data api v1 Tenants {tenantId} Namespaces {namespaceId} Streams Simple Data ?startIndex=2017-11-23T12:30:00Z\u0026endIndex=2017-11-23T15:30:00Z The response will contain the event stored at the specified index: Example response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Time\": \"2017-11-23T13:00:00Z\", \"Measurement\": 10 }, { \"Time\": \"2017-11-23T14:00:00Z\", \"Measurement\": 20 }, { \"Time\": \"2017-11-23T15:00:00Z\", \"Measurement\": 30 } ] Note that State is not included in the JSON as its value is the default value. Example request When the request is modified to specify a boundary type of Outside, the value before 13:30 and the value after 15:30 are included: GET api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Streams/Simple/Data api v1 Tenants {tenantId} Namespaces {namespaceId} Streams Simple Data ?startIndex=2017-11-23T12:30:00Z\u0026endIndex=2017-11-23T15:30:00Z \u0026boundaryType=2 Example response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Time\": \"2017-11-23T12:00:00Z\" }, { \"Time\": \"2017-11-23T13:00:00Z\", \"Measurement\": 10 }, { \"Time\": \"2017-11-23T14:00:00Z\", \"Measurement\": 20 }, { \"Time\": \"2017-11-23T15:00:00Z\", \"Measurement\": 30 }, { \"Time\": \"2017-11-23T16:00:00Z\", \"Measurement\": 40 } ] Note that State is not included in the JSON as its value is the default value. Further, Measurement is not included in the second event (12:00:00) as zero is the default value for numbers. Example request With a start boundary of Inside, only values inside the start boundary (after 13:30) are included in the result. With an end boundary of Outside, one value outside the end index (after 15:30) is included: GET api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Streams/Simple/Data api v1 Tenants {tenantId} Namespaces {namespaceId} Streams Simple Data ?startIndex=2017-11-23T12:30:00Z\u0026\u0026startBoundaryType=1 \u0026endIndex=2017-11-23T15:30:00Z\u0026endBoundaryType=2 Example response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Time\": \"2017-11-23T13:00:00Z\", \"State\": 0, \"Measurement\": 10 }, { \"Time\": \"2017-11-23T14:00:00Z\", \"State\": 0, \"Measurement\": 20 }, { \"Time\": \"2017-11-23T15:00:00Z\", \"State\": 0, \"Measurement\": 30 }, { \"Time\": \"2017-11-23T16:00:00Z\", \"State\": 0, \"Measurement\": 40 } ] Example request In order to page the results of the request, a continuation token may be specified. This requests the first page of the first two stored events between start index and end index by indicating count is 2 and continuationToken is an empty string: GET api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Streams/Simple/Data api v1 Tenants {tenantId} Namespaces {namespaceId} Streams Simple Data ?startIndex=2017-11-23T12:30:00Z\u0026endIndex=2017-11-23T15:30:00Z \u0026count=2\u0026continuationToken= Example response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json { \"Results\": [ { \"Time\": \"2017-11-23T13:00:00Z\", \"State\": 0, \"Measurement\": 10 }, { \"Time\": \"2017-11-23T14:00:00Z\", \"State\": 0, \"Measurement\": 20 } ], \"ContinuationToken\": \"2017-11-23T14:00:00.0000000Z\" } Example request This request uses the continuation token from the previous page to request the next page of stored events: GET api/v1/Tenants/{tenantId}}/Namespaces/{namespaceId}/Streams/Simple/Data api v1 Tenants {tenantId}} Namespaces {namespaceId} Streams Simple Data ?startIndex=2017-11-23T12:30:00Z\u0026endIndex=2017-11-23T15:30:00Z \u0026count=2\u0026continuationToken=2017-11-23T14:00:00Z Example response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json { \"Results\": [ { \"Time\": \"2017-11-23T15:00:00Z\", \"State\": 0, \"Measurement\": 30 } ], \"ContinuationToken\": null } In this case, the results contain the final event. The returned continuation token is null . List Interpolated Values Returns a collection of values based on request parameters. The stream\u0027s read characteristics determine how events are calculated for indexes at which no stored event exists. For more information, see Interpolation and Extrapolation . Interpolation is not supported for streams with compound indexes. SDS supports two ways of specifying which interpolated events to return: Index Collection : One or more indexes can be passed to the request in order to retrieve events at specific indexes. Interval : An interval can be specified with a start index, end index, and count. This will return the specified count of events evenly spaced from start index to end index. Index collection Returns events at the specified indexes. If no stored event exists at a specified index, the stream\u0027s read characteristics determine how the returned event is calculated. For more information, see Interpolation and Extrapolation . Request GET api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Streams/{streamId}/Data/ api v1 Tenants {tenantId} Namespaces {namespaceId} Streams {streamId} Data  Interpolated?index={index}[\u0026index={index}...] Parameters string tenantId Tenant identifier string namespaceId Namespace identifier string streamId Stream identifier. string index One or more indexes Response Status Code Body Type Description 200 Inline Returns a serialized collection of events. Depending on the specified indexes and read characteristics of the stream, it is possible to have less events returned than specified indexes. An empty collection can also be returned. 400 ErrorResponseBody Missing or invalid inputs 401 ErrorResponseBody Unauthorized 403 ErrorResponseBody Forbidden 404 ErrorResponseBody One of the resources specified was not found 500 ErrorResponseBody An error occurred while processing the request 503 ErrorResponseBody Service Unavailable Example request Consider a stream of type Simple with the default InterpolationMode of Continuous and ExtrapolationMode of All . In the following request, the specified index matches an existing stored event: GET api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Streams/Simple/Data/ api v1 Tenants {tenantId} Namespaces {namespaceId} Streams Simple Data  Interpolated?index=2017-11-23T13:00:00Z The response will contain the event stored at the specified index. Example response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Time\": \"2017-11-23T13:00:00Z\", \"State\": 0, \"Measurement\": 10 } ] Example request The following request specifies an index for which no stored event exists: GET api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Streams/Simple/Data/ api v1 Tenants {tenantId} Namespaces {namespaceId} Streams Simple Data  Interpolated?index=2017-11-23T13:30:00Z Because the index is a valid type for interpolation and the stream has a InterpolationMode of Continuous , this request receives a response with an event interpolated at the specified index: Example response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Time\": \"2017-11-23T13:30:00Z\", \"State\": 0, \"Measurement\": 15 } ] Example request Consider a stream of type Simple with an InterpolationMode of Discrete and ExtrapolationMode of All . In the following request, the specified indexes only match two existing stored events: GET api/v1/Tenants/{tenantId}}/Namespaces/{namespaceId}/Streams/Simple/Data api v1 Tenants {tenantId}} Namespaces {namespaceId} Streams Simple Data Interpolated?index=2017-11-23T12:30:00Z\u0026index=2017-11-23T13:00:00Z\u0026index=2017-11-23T14:00:00Z For this request, the response contains events for two of the three specified indexes. Example response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Time\": \"2017-11-23T13:00:00Z\", \"State\": 0, \"Measurement\": 10 }, { \"Time\": \"2017-11-23T14:00:00Z\", \"State\": 0, \"Measurement\": 20 } ] Interval Returns events at evenly spaced intervals based on the specified start index, end index, and count. If no stored event exists at an index interval, the stream\u0027s read characteristics determine how the returned event is calculated. For more information, see Interpolation and Extrapolation . Request GET api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Streams/{streamId}/Data/ api v1 Tenants {tenantId} Namespaces {namespaceId} Streams {streamId} Data  Interpolated?startIndex={startIndex}\u0026endIndex={endIndex}\u0026count={count} Parameters string tenantId Tenant identifier string namespaceId Namespace identifier string streamId Stream identifier string startIndex The index defining the beginning of the window string endIndex The index defining the end of the window int count The number of events to return. Read characteristics of the stream determine how the events are constructed. Response Status Code Body Type Description 200 Inline Returns a serialized collection of events. Depending on the specified indexes and read characteristics of the stream, it is possible to have less events returned than specified indexes. An empty collection can also be returned. 400 ErrorResponseBody Missing or invalid inputs 401 ErrorResponseBody Unauthorized 403 ErrorResponseBody Forbidden 404 ErrorResponseBody One of the resources specified was not found 500 ErrorResponseBody An error occurred while processing the request 503 ErrorResponseBody Service Unavailable Example request For a stream, named Simple, of type Simple for the following request: GET api/v1/Tenants/{tenantId}}/Namespaces/{namespaceId}/Streams/Simple/Data/ api v1 Tenants {tenantId}} Namespaces {namespaceId} Streams Simple Data  Interpolated?startIndex=2017-11-23T13:00:00Z\u0026endIndex=2017-11-23T15:00:00Z\u0026count=3 the start and end fall exactly on event indexes, and the number of events from start to end match the count of three (3). Example response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Time\": \"2017-11-23T13:00:00Z\", \"State\": 0, \"Measurement\": 10 }, { \"Time\": \"2017-11-23T14:00:00Z\", \"State\": 0, \"Measurement\": 20 }, { \"Time\": \"2017-11-23T15:00:00Z\", \"State\": 0, \"Measurement\": 30 } ] Get Summaries Returns summary intervals between a specified start and end index. Index types that cannot be interpolated do not support summary requests. Strings are an example of indexes that cannot be interpolated. Summaries are not supported for streams with compound indexes. Interpolating between two indexes that consist of multiple properties is not defined and results in non-determinant behavior. Summary values supported by SdsSummaryType enum: Summary Enumeration value Count 1 Minimum 2 Maximum 4 Range 8 Mean 16 StandardDeviation 64 Total 128 Skewness 256 Kurtosis 512 WeightedMean 1024 WeightedStandardDeviation 2048 WeightedPopulationStandardDeviation 4096 Count, Minimum, Maximum, and Range are based only on stored events and do not include interpolated values. Mean, Standard Deviation, Skewness, and Kurtosis are event-weighted calculations. Total, Weighted Mean, WeightedStandardDeviation, and WeightedPopulationStandardDeviation are index-weighted calculations. Currently, these values can only be calculated for properties of the following types: Type SdsTypeCode Boolean 3 Byte 6 Char 4 Decimal 15 Int16 7 Int32 9 Int64 11 SByte 5 Single 13 UInt16 8 UInt32 10 UInt64 12 DateTime 16 Double 14 DateTimeOffset 20 TimeSpan 21 NullableBoolean 103 NullableByte 106 NullableChar 104 NullableDecimal 115 NullableInt16 107 NullableInt32 109 NullableInt64 111 NullableSByte 105 NullableSingle 113 NullableUInt16 108 NullableUInt32 110 NullableUInt64 112 NullableDateTime 116 NullableDouble 114 NullableDateTimeOffset 120 NullableTimeSpan 121 NOTE : Properties marked with an InterpolationMode of Discrete do not support summaries. Unsupported properties will be excluded from the summaries returned. Request GET api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Streams/{streamId}/Data/ api v1 Tenants {tenantId} Namespaces {namespaceId} Streams {streamId} Data  Summaries?startIndex={startIndex}\u0026endIndex={endIndex}\u0026count={count}[\u0026filter={filter}] Parameters string tenantId Tenant identifier string namespaceId Namespace identifier string streamId Stream identifier string startIndex The start index for the intervals string endIndex The end index for the intervals int count The number of intervals requested string filter Optional filter expression Response The response includes a status code and a response body containing a serialized collection of SdsIntervals. Each SdsInterval has a start, end, and collection of summary values. Property Details Start The start of the interval End The end of the interval Summaries The summary values for the interval, keyed by summary type. The nested dictionary contains property name keys and summary calculation result values. Example request The following request calculates two summary intervals between the startIndex and endIndex : GET api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Streams/Simple/Data/ api v1 Tenants {tenantId} Namespaces {namespaceId} Streams Simple Data  Summaries?startIndex=2017-11-23T12:00:00Z\u0026endIndex=2017-11-23T16:00:00Z\u0026count=2 Example response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Start\": { \"Time\": \"2017-11-23T12:00:00Z\", \"State\": 0, \"Measurement\": 0 }, \"End\": { \"Time\": \"2017-11-23T14:00:00Z\", \"State\": 0, \"Measurement\": 20 }, \"Summaries\": { \"Count\": { \"Time\": 3, \"Measurement\": 3 }, \"Minimum\": { \"Measurement\": 0 }, \"Maximum\": { \"Measurement\": 20 }, \"Range\": { \"Measurement\": 20 }, \"Total\": { \"Measurement\": 20 }, \"Mean\": { \"Measurement\": 10 }, \"StandardDeviation\": { \"Measurement\": 7.0710678118654755 }, \"PopulationStandardDeviation\": { \"Measurement\": 5 }, \"WeightedMean\": { \"Measurement\": 10 }, \"WeightedStandardDeviation\": { \"Measurement\": 7.0710678118654755 }, \"WeightedPopulationStandardDeviation\": { \"Measurement\": 5 }, \"Skewness\": { \"Measurement\": 0 }, \"Kurtosis\": { \"Measurement\": -2 } } }, { \"Start\": { \"Time\": \"2017-11-23T14:00:00Z\", \"State\": 0, \"Measurement\": 20 }, \"End\": { \"Time\": \"2017-11-23T16:00:00Z\", \"State\": 0, \"Measurement\": 40 }, \"Summaries\": { \"Count\": { \"Time\": 3, \"Measurement\": 3 }, \"Minimum\": { \"Measurement\": 30 }, \"Maximum\": { \"Measurement\": 40 }, \"Range\": { \"Measurement\": 10 }, \"Total\": { \"Measurement\": 60 }, \"Mean\": { \"Measurement\": 30 }, \"StandardDeviation\": { \"Measurement\": 7.0710678118654755 }, \"PopulationStandardDeviation\": { \"Measurement\": 5 }, \"WeightedMean\": { \"Measurement\": 30 }, \"WeightedStandardDeviation\": { \"Measurement\": 7.0710678118654755 }, \"WeightedPopulationStandardDeviation\": { \"Measurement\": 5 }, \"Skewness\": { \"Measurement\": 0 }, \"Kurtosis\": { \"Measurement\": -2 } } } ] SDS also supports summary requests for nullable SdsType s. It means an SdsType has at least a nullable SdsTypeProperty . Example The following example contains a nullable double property with interpolation mode set to continuous: .NET public class SimpleType { [SdsMember(IsKey = true, Order = 0) ] public DateTime Time { get; set; } [SdsMember(Uom = \"meter\")] public double? Measurement { get; set; } } Measurement has stored values as follows: 11/23/2017 11 23 2017 12:00:01 PM: Measurement 2 11/23/2017 11 23 2017 12:00:02 PM: Measurement 2 11/23/2017 11 23 2017 12:00:03 PM: Measurement null 11/23/2017 11 23 2017 12:00:04 PM: Measurement 1 11/23/2017 11 23 2017 12:00:05 PM: Measurement 2 11/23/2017 11 23 2017 12:00:06 PM: Measurement null 11/23/2017 11 23 2017 12:00:07 PM: Measurement null 11/23/2017 11 23 2017 12:00:08 PM: Measurement 3 While calculating weighted summaries, if we encounter a null value at a given index then we would consider interpolation mode of property to find the interpolated value of the given interval. See the table below for [12:00:02 PM, 12:00:03 PM] interval. The values are 2 and null at 12:00:02 and 12:00:03 PM respectively. Interpolation Mode Weight in seconds Value in meter Continuous 0 0 ContinuousNullableLeading StepwiseContinuousLeading 1 2 ContinuousNullableTrailing StepwiseContinuousTrailing 0 0 Similarly, for intervals [12:00:03 PM, 12:00:04 PM] and [12:00:04 PM, 12:00:05 PM] respectively, the table would look like below: Interpolation Mode Weight in seconds Value in meter Continuous 0 0 ContinuousNullableLeading StepwiseContinuousLeading 0 0 ContinuousNullableTrailing StepwiseContinuousTrailing 1 1 Interpolation Mode Weight in seconds Value in meter Continuous 1 1.5 ContinuousNullableLeading StepwiseContinuousLeading 1 1 ContinuousNullableTrailing StepwiseContinuousTrailing 1 2 Note: Non-weighted summaries disregard null values and treat them as non-existent. In the example above, non-weighted summaries for Measurement would be calculated based on (2,2,1,2,3) whereas weighted summaries for Measurement consider null values for its calculation. For more information see Interpolation . Example request The following request calculates one summary interval between the startIndex and endIndex : GET api/v1-preview/Tenants/{tenantId}/Namespaces/{namespaceId}/Streams/Simple/Data/ api v1-preview Tenants {tenantId} Namespaces {namespaceId} Streams Simple Data  Summaries?startIndex=2017-11-23T12:00:01Z\u0026endIndex=2017-11-23T12:00:08Z\u0026count=1 Example response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Start\": { \"Time\": \"2017-11-23T12:00:01Z\", \"Measurement\": 2 }, \"End\": { \"Time\": \"2017-11-23T12:00:08Z\", \"Measurement\": 3 }, \"Summaries\": { \"Count\": { \"Time\": 8, \"Measurement\": 5 }, \"Minimum\": { \"Measurement\": 1 }, \"Maximum\": { \"Measurement\": 3 }, \"Range\": { \"Measurement\": 2 }, \"Total\": { \"Measurement\": 7.5 }, \"Mean\": { \"Measurement\": 1.875 }, \"StandardDeviation\": { \"Measurement\": 0.478713553878169 }, \"PopulationStandardDeviation\": { \"Measurement\": 0.41457809879442492 }, \"WeightedMean\": { \"Measurement\": 1.75 }, \"WeightedStandardDeviation\": { \"Measurement\": 0.35355339059327379 }, \"WeightedPopulationStandardDeviation\": { \"Measurement\": 0.25 }, \"Skewness\": { \"Measurement\": 0.49338220021815865 }, \"Kurtosis\": { \"Measurement\": -1.3719008264462809 } } } ] Get Sampled Values Returns representative data sampled by intervals between a specified start and end index. Sampling is driven by a specified property or properties of the stream\u0027s Sds Type. Property types that cannot be interpolated do not support sampling requests. Strings are an example of a property that cannot be interpolated. For more information see Interpolation . Request GET api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Streams/{streamId}/Data/ api v1 Tenants {tenantId} Namespaces {namespaceId} Streams {streamId} Data  Sampled?startIndex={startIndex}\u0026endIndex={endIndex}\u0026intervals={intervals}\u0026sampleBy={sampleBy} [\u0026sampleBy={sampleBy}\u0026...\u0026boundaryType={boundaryType}\u0026startBoundaryType={startBoundaryType} \u0026endBoundaryType={endBoundaryType}\u0026filter={filter}] Parameters string tenantId Tenant identifier string namespaceId Namespace identifier string streamId Stream identifier string startIndex The start index for the intervals string endIndex The end index for the intervals int intervals The number of intervals requested string sampleBy Property or properties to use when sampling SdsBoundaryType boundaryType Optional SdsBoundaryType specifies the handling of events at or near the startIndex and endIndex SdsBoundaryType startBoundaryType Optional SdsBoundaryType specifies the handling of events at or near the startIndex SdsBoundaryType endBoundaryType Optional SdsBoundaryType specifies the handling of events at or near the endIndex string filter Optional filter expression Response Status Code Body Type Description 200 Inline Returns a serialized collection of events 400 ErrorResponseBody Missing or invalid inputs 401 ErrorResponseBody Unauthorized 403 ErrorResponseBody Forbidden 404 ErrorResponseBody One of the resources specified was not found 500 ErrorResponseBody An error occurred while processing the request 503 ErrorResponseBody Service Unavailable Example request The following request returns two sample intervals between the startIndex and endIndex : GET api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Streams/Simple/Data/ api v1 Tenants {tenantId} Namespaces {namespaceId} Streams Simple Data  Sampled?startIndex=2019-01-01T00:00:00Z\u0026endIndex=2019-01-02T00:00:00Z\u0026intervals=2\u0026sampleBy=Measurement Example response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Time\": \"2019-01-01T00:00:01Z\", \"State\": 1, \"Measurement\": 1 }, { \"Time\": \"2019-01-01T00:11:50Z\", \"State\": 2, \"Measurement\": 0.00006028870675578446 }, { \"Time\": \"2019-01-01T11:55:33Z\", \"Measurement\": 6.277981349066863 }, { \"Time\": \"2019-01-01T12:00:00Z\", \"Measurement\": 3.101013140344655 }, { \"Time\": \"2019-01-01T12:00:01Z\", \"State\": 1, \"Measurement\": 4.101013140344655 }, { \"Time\": \"2019-01-01T12:01:50Z\", \"State\": 2, \"Measurement\": 0.0036776111121028521 }, { \"Time\": \"2019-01-01T23:57:23Z\", \"State\": 2, \"Measurement\": 6.2816589601789659 }, { \"Time\": \"2019-01-02T00:00:00Z\", \"Measurement\": 6.20202628068931 } ] Note that State is not included in the JSON when its value is the default value. 400 Response ( ErrorResponseBody ) { \"OperationId\": \"string\", \"Error\": \"string\", \"Reason\": \"string\", \"Resolution\": \"string\", \"Parameters\": { \"property1\": \"string\", \"property2\": \"string\" } } Join Values Returns data from multiple streams, which are joined based on the request specifications. The streams must be of the same type. SDS supports the following types of joins: SdsJoinMode Enumeration value Operation Inner 0 Results include the stored events with common indexes across specified streams. Outer 1 Results include the stored events for all indexes across all streams. Interpolated 2 Results include events for each index across all streams for the request index boundaries. Some events may be interpolated. MergeLeft 3 Results include one event for each index across all streams selecting events at the indexes based on left to right order of the streams. MergeRight 4 Results include one event for each index across all streams selecting events at the indexes based on right to left order of the streams. SDS supports GET and POST join requests: GET : The stream, joinMode, start index, and end index are specified in the request URI path. POST : Only the joinMode is specified in the URI. The streams and read specification for each stream are specified in the body of the request. GET request GET api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Bulk/Streams/Data/Joins api v1 Tenants {tenantId} Namespaces {namespaceId} Bulk Streams Data Joins ?streams={streams}\u0026joinMode={joinMode}\u0026startIndex={startIndex}\u0026endIndex={endIndex} [\u0026boundaryType={boundaryType}\u0026startBoundaryType={startBoundaryType} \u0026endBoundaryType={endBoundaryType}\u0026filter={filter}\u0026count={count}] Parameters string tenantId Tenant identifier. string namespaceId Namespace identifier string streams Commas separated list of stream identifiers SdsJoinMode joinMode Type of join: inner, outer, interpolated, merge left or merge right string startIndex Index identifying the beginning of the series of events to return string endIndex Index identifying the end of the series of events to return [Optional] int count Maximum number of events to return [Optional] SdsBoundaryType boundaryType SdsBoundaryType specifies the handling of events at or near the startIndex and endIndex [Optional] SdsBoundaryType startBoundaryType SdsBoundaryType specifies the handling of events at or near the startIndex [Optional] SdsBoundaryType endBoundaryType SdsBoundaryType specifies the handling of events at or near the endIndex [Optional] string filter Filter expression Response Status Code Body Type Description 200 Inline Returns a serialized collection of events 400 ErrorResponseBody Missing or invalid inputs 401 ErrorResponseBody Unauthorized 403 ErrorResponseBody Forbidden 404 ErrorResponseBody One of the resources specified was not found 500 ErrorResponseBody An error occurred while processing the request 503 ErrorResponseBody Service Unavailable Examples Data from streams Simple1 and Simple2 will be used to illustrate how each join operation works. Stream data Simple1 HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Time\": \"2017-11-23T11:00:00Z\", \"Measurement\": 10 }, { \"Time\": \"2017-11-23T13:00:00Z\", \"Measurement\": 20 }, { \"Time\": \"2017-11-23T14:00:00Z\", \"Measurement\": 30 }, { \"Time\": \"2017-11-23T16:00:00Z\", \"Measurement\": 40 } ] Time Measurement 2017-11-23T11:00:00Z 10 2017-11-23T13:00:00Z 20 2017-11-23T14:00:00Z 30 2017-11-23T16:00:00Z 40 Stream data Simple2 HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Time\": \"2017-11-23T12:00:00Z\", \"Measurement\": 50 }, { \"Time\": \"2017-11-23T14:00:00Z\", \"Measurement\": 60 }, { \"Time\": \"2017-11-23T15:00:00Z\", \"Measurement\": 70 }, { \"Time\": \"2017-11-23T17:00:00Z\", \"Measurement\": 80 } ] Time Measurement 2017-11-23T12:00:00Z 50 2017-11-23T14:00:00Z 60 2017-11-23T15:00:00Z 70 2017-11-23T17:00:00Z 80 Inner Join example request GET api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Bulk/Streams/Data/Joins api v1 Tenants {tenantId} Namespaces {namespaceId} Bulk Streams Data Joins ?streams=Simple1,Simple2\u0026joinMode=inner \u0026startIndex=0001-01-01T00:00:00.0000000\u0026endIndex=9999-12-31T23:59:59.9999999 Response Measurements from both streams with common indexes. Example response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ [ { \"Time\": \"2017-11-23T14:00:00Z\", \"Measurement\": 30 }, { \"Time\": \"2017-11-23T14:00:00Z\", \"Measurement\": 60 } ] ] Time Simple 1 Measurement Simple 2 Measurement 2017-11-23T14:00:00Z 30 60 Outer Join example request GET api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Bulk/Streams/Data/Joins api v1 Tenants {tenantId} Namespaces {namespaceId} Bulk Streams Data Joins ?streams=Simple1,Simple2\u0026joinMode=outer \u0026startIndex=0001-01-01T00:00:00.0000000\u0026endIndex=9999-12-31T23:59:59.9999999 Response All Measurements from both streams, with default values at indexes where a stream does not have a value. Example response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ [ { \"Time\": \"2017-11-23T11:00:00Z\", \"Measurement\": 10 }, null ], [ null, { \"Time\": \"2017-11-23T12:00:00Z\", \"Measurement\": 50 } ], [ { \"Time\": \"2017-11-23T13:00:00Z\", \"Measurement\": 20 }, null ], [ { \"Time\": \"2017-11-23T14:00:00Z\", \"Measurement\": 30 }, { \"Time\": \"2017-11-23T14:00:00Z\", \"Measurement\": 60 } ], [ null, { \"Time\": \"2017-11-23T15:00:00Z\", \"Measurement\": 70 } ], [ { \"Time\": \"2017-11-23T16:00:00Z\", \"Measurement\": 40 }, null ], [ null, { \"Time\": \"2017-11-23T17:00:00Z\", \"Measurement\": 80 } ] ] Index Simple 1 Measurement Simple 2 Measurement 2017-11-23T11:00:00Z 10 null 2017-11-23T12:00:00Z null 50 2017-11-23T13:00:00Z 20 null 2017-11-23T14:00:00Z 30 60 2017-11-23T15:00:00Z null 70 2017-11-23T16:00:00Z 40 null 2017-11-23T17:00:00Z null 80 Default value is null for SdsTypes. Interpolated Join example request GET api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Bulk/Streams/Data/Joins api v1 Tenants {tenantId} Namespaces {namespaceId} Bulk Streams Data Joins ?streams=Simple1,Simple2\u0026joinMode=interpolated \u0026startIndex=0001-01-01T00:00:00.0000000\u0026endIndex=9999-12-31T23:59:59.9999999 Response All Measurements from both streams with missing values interpolated. If the missing values are between valid measurements within a stream, they are interpolated. For more information, see Interpolation . If the missing values are outside of the boundary values, they are extrapolated. For more information, see Extrapolation . Note: The Interpolated SdsJoinMode currently does not support SdsInterpolationModes of the streams. All join requests with interpolations will honor the interpolation mode of the stream type or type property. Example response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ [ { \"Time\": \"2017-11-23T11:00:00Z\", \"Measurement\": 10 }, { \"Time\": \"2017-11-23T11:00:00Z\", \"Measurement\": 50 } ], [ { \"Time\": \"2017-11-23T12:00:00Z\", \"Measurement\": 15 }, { \"Time\": \"2017-11-23T12:00:00Z\", \"Measurement\": 50 } ], [ { \"Time\": \"2017-11-23T13:00:00Z\", \"Measurement\": 20 }, { \"Time\": \"2017-11-23T13:00:00Z\", \"Measurement\": 55 } ], [ { \"Time\": \"2017-11-23T14:00:00Z\", \"Measurement\": 30 }, { \"Time\": \"2017-11-23T14:00:00Z\", \"Measurement\": 60 } ], [ { \"Time\": \"2017-11-23T15:00:00Z\", \"Measurement\": 35 }, { \"Time\": \"2017-11-23T15:00:00Z\", \"Measurement\": 70 } ], [ { \"Time\": \"2017-11-23T16:00:00Z\", \"Measurement\": 40 }, { \"Time\": \"2017-11-23T16:00:00Z\", \"Measurement\": 75 } ], [ { \"Time\": \"2017-11-23T17:00:00Z\", \"Measurement\": 40 }, { \"Time\": \"2017-11-23T17:00:00Z\", \"Measurement\": 80 } ] ] Index Simple 1 Measurement Simple 2 Measurement 2017-11-23T11:00:00Z 10 50 2017-11-23T12:00:00Z 15 50 2017-11-23T13:00:00Z 20 55 2017-11-23T14:00:00Z 30 60 2017-11-23T15:00:00Z 35 70 2017-11-23T16:00:00Z 40 75 2017-11-23T17:00:00Z 40 80 Interpolated values are in bold . Extrapolated values are in italics . MergeLeft Join example request GET api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Bulk/Streams/Data/Joins api v1 Tenants {tenantId} Namespaces {namespaceId} Bulk Streams Data Joins ?streams=Simple1,Simple2\u0026joinMode=mergeleft \u0026startIndex=0001-01-01T00:00:00.0000000\u0026endIndex=9999-12-31T23:59:59.9999999 Response Similar to OuterJoin , but value at each index is the first available value at that index when iterating the given list of streams from left to right. Example response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Time\": \"2017-11-23T11:00:00Z\", \"Measurement\": 10 }, { \"Time\": \"2017-11-23T12:00:00Z\", \"Measurement\": 50 }, { \"Time\": \"2017-11-23T13:00:00Z\", \"Measurement\": 20 }, { \"Time\": \"2017-11-23T14:00:00Z\", \"Measurement\": 30 }, { \"Time\": \"2017-11-23T15:00:00Z\", \"Measurement\": 70 }, { \"Time\": \"2017-11-23T16:00:00Z\", \"Measurement\": 40 }, { \"Time\": \"2017-11-23T17:00:00Z\", \"Measurement\": 80 } ] Index Simple1 Simple2 Returned MergeLeft Join Values 2017-11-23T11:00:00Z 10 10 2017-11-23T12:00:00Z 50 50 2017-11-23T13:00:00Z 20 20 2017-11-23T14:00:00Z 30 60 30 2017-11-23T15:00:00Z 70 70 2017-11-23T16:00:00Z 40 40 2017-11-23T17:00:00Z 80 80 Takes the value from the stream on the left ( Simple1 ) at \"2017-11-23T14:00:00Z\". MergeRight Join example request GET api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Bulk/Streams/Data/Joins api v1 Tenants {tenantId} Namespaces {namespaceId} Bulk Streams Data Joins ?streams=Simple1,Simple2\u0026joinMode=mergeright \u0026startIndex=0001-01-01T00:00:00.0000000\u0026endIndex=9999-12-31T23:59:59.9999999 Response Similar to OuterJoin , but value at each index is the first available value at that index when iterating the given list of streams from right to left. Example response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Time\": \"2017-11-23T11:00:00Z\", \"Measurement\": 10 }, { \"Time\": \"2017-11-23T12:00:00Z\", \"Measurement\": 50 }, { \"Time\": \"2017-11-23T13:00:00Z\", \"Measurement\": 20 }, { \"Time\": \"2017-11-23T14:00:00Z\", \"Measurement\": 60 }, { \"Time\": \"2017-11-23T15:00:00Z\", \"Measurement\": 70 }, { \"Time\": \"2017-11-23T16:00:00Z\", \"Measurement\": 40 }, { \"Time\": \"2017-11-23T17:00:00Z\", \"Measurement\": 80 } ] Index Simple1 Simple2 Returned MergeRight Join Values 2017-11-23T11:00:00Z 10 10 2017-11-23T12:00:00Z 50 50 2017-11-23T13:00:00Z 20 20 2017-11-23T14:00:00Z 30 60 60 2017-11-23T15:00:00Z 70 70 2017-11-23T16:00:00Z 40 40 2017-11-23T17:00:00Z 80 80 Takes the value from the stream on the right ( Simple2 ) at \"2017-11-23T14:00:00Z\". POST request POST api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Bulk/Streams/Data/Joins? api v1 Tenants {tenantId} Namespaces {namespaceId} Bulk Streams Data Joins? joinMode={joinMode} Parameters string tenantId Tenant identifier string namespaceId Namespace identifier SdsJoinMode joinMode Type of join: inner, outer, interpolated, merge left or merge right Request body Read option specific to each stream Response The response includes a status code and a response body containing multiple serialized events. Outer Join example request POST api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Bulk/Streams/Data/Joins api v1 Tenants {tenantId} Namespaces {namespaceId} Bulk Streams Data Joins ?joinMode=outer Outer Join example request body Different start indexes and end indexes are specified per stream. [ { \"StreamId\": \"Simple1\", \"Options\": { \"StartIndex\": \"2017-11-23T11:00:00Z\", \"EndIndex\": \"2017-11-23T14:00:00Z\", \"StartBoundaryType\": \"Exact\", \"EndBoundaryType\": \"Exact\", \"Count\": 100, \"Filter\": \"\" } }, { \"StreamId\": \"Simple2\", \"Options\": { \"StartIndex\": \"2017-11-23T15:00:00Z\", \"EndIndex\": \"2017-11-23T17:00:00Z\", \"StartBoundaryType\": \"Exact\", \"EndBoundaryType\": \"Exact\", \"Count\": 100, \"Filter\": \"\" } } ] Outer Join example response body Only events within the two streams\u0027 specified index boundaries are considered for the outer join operation. HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ [ { \"Time\": \"2017-11-23T11:00:00Z\", \"Measurement\": 10 }, null ], [ { \"Time\": \"2017-11-23T13:00:00Z\", \"Measurement\": 20 }, null ], [ { \"Time\": \"2017-11-23T14:00:00Z\", \"Measurement\": 30 }, null ], [ null, { \"Time\": \"2017-11-23T15:00:00Z\", \"Measurement\": 70 } ], [ null, { \"Time\": \"2017-11-23T17:00:00Z\", \"Measurement\": 80 } ] ] Index Simple 1 Measurement Simple 2 Measurement 2017-11-23T11:00:00Z 10 null 2017-11-23T13:00:00Z 20 null 2017-11-23T14:00:00Z 30 null 2017-11-23T15:00:00Z null 70 2017-11-23T17:00:00Z null 80 Not all values from both streams are included because the query restricts each stream. See Outer Join GET request above to compare. Definitions ErrorResponseBody Contains the error message format that follows the AVEVA Data Hub error standards Properties Property Name Data Type Required Nullable Description OperationId string false true Operation unique identifier of action that caused the error Error string false true Error description Reason string false true Reason for the error Resolution string false true Resolution to resolve the error Parameters object false true IDs or values that are creating or are affected by the error { \"OperationId\": \"string\", \"Error\": \"string\", \"Reason\": \"string\", \"Resolution\": \"string\", \"Parameters\": { \"property1\": \"string\", \"property2\": \"string\" } }"
                                                    },
    "content/sds/read-data/response-format.html":  {
                                                       "href":  "content/sds/read-data/response-format.html",
                                                       "title":  "Response format",
                                                       "keywords":  "Response format The format of the response is specified in the API call. For read APIs, the supported response formats are: JSON - the default response format for SDS, which is used in all examples in this documentation. Default JSON responses do not include any values that are equal to the default value for their type. Verbose JSON - responses include all values, including defaults, in the returned JSON payload. To specify verbose JSON return, add the header Accept-Verbosity with a value of verbose to the request. SDS - specified by setting the Accept header in the request to application/sds application sds ."
                                                   },
    "content/sds/read-data/table-format.html":  {
                                                    "href":  "content/sds/read-data/table-format.html",
                                                    "title":  "Table format",
                                                    "keywords":  "Table format You can organize results of a query into tables by directing the form parameter to return a table. Two table forms are available: table and header table. Apply the table format to any read that returns multiple values and summaries by setting the form variable to specify a table or a table with headers. When the form parameter is specified as table , ?form=table , events are returned in row column form. Results include a collection named Columns that lists column name and type and a collection named Rows containing a collection of rows matching the order of the columns. Specifying a form of type table-headers , ?form=tableh , results in a collection where the Rows collection contains a column header list. Table format can be applied to any read that returns multiple values and summaries. Csharp Python JavaScript public enum State { Ok, Warning, Alarm } public class Simple { [SdsMember(IsKey = true, Order = 0) ] public DateTime Time { get; set; } public State State { get; set; } public Double Measurement { get; set; } } class State(Enum): Ok = 0 Warning = 1 Alarm = 2 class Simple(object): Time = property(getTime, setTime) def getTime(self): return self.__time def setTime(self, time): self.__time = time State = property(getState, setState) def getState(self): return self.__state def setState(self, state): self.__state = state Measurement = property(getValue, setValue) def getValue(self): return self.__measurement def setValue(self, measurement): self.__measurement = measurement var State = { Ok: 0, Warning: 1, Alarm: 2, } var Simple = function () { this.Time = null; this.State = null; this.Value = null; } With values as follows: 4/1/2017 4 1 2017 7:00:00 AM : Warning 0 4/1/2017 4 1 2017 7:01:00 AM : Warning 1 4/1/2017 4 1 2017 7:02:00 AM : Warning 2 4/1/2017 4 1 2017 7:03:00 AM : Warning 3 4/1/2017 4 1 2017 7:04:00 AM : Warning 4 4/1/2017 4 1 2017 7:05:00 AM : Warning 5 4/1/2017 4 1 2017 7:06:00 AM : Warning 6 4/1/2017 4 1 2017 7:07:00 AM : Warning 7 4/1/2017 4 1 2017 7:08:00 AM : Warning 8 4/1/2017 4 1 2017 7:09:00 AM : Warning 9 The following is a request to retrieve values using the window parameters: GET api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Streams/Simple/Data api v1 Tenants {tenantId} Namespaces {namespaceId} Streams Simple Data ?startIndex=2017-04-01T07:00:00Z\u0026endIndex=2017-04-01T07:10:00Z The following response is returned by the code above: Content-Type: application/json application json [ { \"Time\":\"2017-04-01T07:00:00Z\", \"State\":1 }, { \"Time\":\"2017-04-01T07:01:00Z\", \"State\":1, \"Measurement\":1.0 }, { \"Time\":\"2017-04-01T07:02:00Z\", \"State\":1, \"Measurement\":2.0 }, { \"Time\":\"2017-04-01T07:03:00Z\", \"State\":1, \"Measurement\":3.0 }, { \"Time\":\"2017-04-01T07:04:00Z\", \"State\":1, \"Measurement\":4.0 }, { \"Time\":\"2017-04-01T07:05:00Z\", \"State\":1, \"Measurement\":5.0 }, { \"Time\":\"2017-04-01T07:06:00Z\", \"State\":1, \"Measurement\":6.0 }, { \"Time\":\"2017-04-01T07:07:00Z\", \"State\":1, \"Measurement\":7.0 }, { \"Time\":\"2017-04-01T07:08:00Z\", \"State\":1, \"Measurement\":8.0 }, { \"Time\":\"2017-04-01T07:09:00Z\", \"State\":1, \"Measurement\":9.0 } ] To retrieve the results in table format, add the form variable and specify table. GET api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Streams/Simple/Data api v1 Tenants {tenantId} Namespaces {namespaceId} Streams Simple Data ?startIndex=2017-04-01T07:00:00Z\u0026endIndex=2017-04-01T07:10:00Z \u0026form=table Response Content-Type: application/json application json { \"Name\":\"Simple\", \"Columns\":[ { \"Name\":\"Time\", \"Type\":\"DateTime\" }, { \"Name\":\"State\", \"Type\":\"Int32Enum\" }, { \"Name\":\"Measurement\", \"Type\":\"Double\" } ], \"Rows\":[ [ \"2017-04-01T07:00:00Z\", 1, 0.0 ], [ \"2017-04-01T07:01:00Z\", 1, 1.0 ], [ \"2017-04-01T07:02:00Z\", 1, 2.0 ], [ \"2017-04-01T07:03:00Z\", 1, 3.0 ], [ \"2017-04-01T07:04:00Z\", 1, 4.0 ], [ \"2017-04-01T07:05:00Z\", 1, 5.0 ], [ \"2017-04-01T07:06:00Z\", 1, 6.0 ], [ \"2017-04-01T07:07:00Z\", 1, 7.0 ], [ \"2017-04-01T07:08:00Z\", 1, 8.0 ], [ \"2017-04-01T07:09:00Z\", 1, 9.0 ] ] } To retrieve the results in table format with column headers, add the form variable and specify tableh . GET api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Streams/Simple/Data api v1 Tenants {tenantId} Namespaces {namespaceId} Streams Simple Data ?startIndex=2017-04-01T07:00:00Z\u0026endIndex=2017-04-01T07:10:00Z \u0026form=tableh Response Content-Type: application/json application json { \"Name\":\"Simple\", \"Columns\":[ { \"Name\":\"Time\", \"Type\":\"DateTime\" }, { \"Name\":\"State\", \"Type\":\"Int32Enum\" }, { \"Name\":\"Measurement\", \"Type\":\"Double\" } ], \"Rows\":[ [ \"Time\", \"State\", \"Measurement\" ], [ \"2017-04-01T07:00:00Z\", 1, 0.0 ], [ \"2017-04-01T07:01:00Z\", 1, 1.0 ], [ \"2017-04-01T07:02:00Z\", 1, 2.0 ], [ \"2017-04-01T07:03:00Z\", 1, 3.0 ], [ \"2017-04-01T07:04:00Z\", 1, 4.0 ], [ \"2017-04-01T07:05:00Z\", 1, 5.0 ], [ \"2017-04-01T07:06:00Z\", 1, 6.0 ], [ \"2017-04-01T07:07:00Z\", 1, 7.0 ], [ \"2017-04-01T07:08:00Z\", 1, 8.0 ], [ \"2017-04-01T07:09:00Z\", 1, 9.0 ] ] }"
                                                },
    "content/sds/sds-streams.html":  {
                                         "href":  "content/sds/sds-streams.html",
                                         "title":  "Streams",
                                         "keywords":  "Streams SdsStreams are collections of sequentially occurring values indexed by a single property, typically time series data. You define SdsStreams to organize incoming data from another system into the AVEVA Data Hub. To define an SdsStream, you must first define an SdsType, which defines the structure of the data you want to stream into a selected namespace. SDS stores collections of events and provides convenient ways to find and associate events. Events of consistent structure are stored in streams. Streams are referenced by their identifier or Id field. Stream identifiers must be unique within a namespace. A stream must include a TypeId that references the identifier of an existing type. Stream management using the .NET SDS client libraries is performed through ISdsMetadataService . Create the ISdsMetadataService using one of the SdsService.GetMetadataService() factory methods. The following table shows the required and optional stream fields. Fields not listed are reserved for internal SDS use. Property Type Optionality Searchable Details Id String Required Yes An identifier for referencing the stream TypeId String Required Yes The SdsType identifier of the type to be used for this stream Name String Optional Yes Friendly name Description String Optional Yes Description text Indexes IList\u003cSdsStreamIndex\u003e Optional No Used to define secondary indexes for stream InterpolationMode SdsInterpolationMode Optional No Interpolation setting of the stream. Default is null. ExtrapolationMode SdsExtrapolationMode Optional No Extrapolation setting of the stream. Default is null. PropertyOverrides IList\u003cSdsStreamPropertyOverride\u003e Optional No Used to define unit of measure and interpolation mode overrides for a stream. Tags * IList\u003cString\u003e Optional Yes A list of tags denoting special attributes or categories. Metadata * IDictionary\u003cString, String\u003e Optional Yes A dictionary of string keys and associated string values. Note: You can access stream metadata and tags through Metadata API and Tags API respectively. Because they are associated with SdsStream objects, you can use them as search criteria. Rules for the stream identifier (SdsStream.Id) The stream identifier, SdsStream.Id , has the following requirements: Is not case sensitive. Cannot just be whitespace. Cannot contain leading or trailing whitespace. Cannot contain forward slash (\"/\"). (\" \"). Contains a maximum of 100 characters. Indexes While you define the primary index on the type, the stream is where you define secondary indexes. If the primary index defined on the type is a compound index, secondary indexes on the stream are allowed as long as that compound index does not have more than two properties. For more information on compound indexes, see Indexes . Interpolation and extrapolation The InterpolationMode , ExtrapolationMode , and PropertyOverrides can be used to determine how a specific stream reads data. These read characteristics are inherited from the type if they are not defined at the stream level. For more information about type read characteristics and how these characteristics dictate how events are read, see Types . SdsStreamPropertyOverride The SdsStreamPropertyOverride object provides a way to override interpolation behavior and unit of measure for individual SdsType Properties for a specific SdsStream. The SdsStreamPropertyOverride object has the following structure. Property Type Optionality Details SdsTypePropertyId String Required SdsTypeProperty identifier InterpolationMode SdsInterpolationMode Optional Interpolation setting. Default is null Uom String Optional Unit of measure The unit of measure can be overridden for any type property defined by the stream type, including primary keys and secondary indexes. For more information about type property units of measure, see Types . Read characteristics of the stream are determined by the type and the PropertyOverrides of the stream. The interpolation mode for non-index properties can be defined and overridden at the stream level. For more information about type read characteristics, see Types . If the SdsType InterpolationMode is set to Discrete , it cannot be overridden at any level. When InterpolationMode is set to Discrete and an event is not defined for that index, a null value is returned for the entire event."
                                     },
    "content/sds/sds-types.html":  {
                                       "href":  "content/sds/sds-types.html",
                                       "title":  "Types",
                                       "keywords":  "Types An SdsType, also called a type, defines the shape of a single measured event or object. A type gives structure to the data. For example, if a device measures three things, such as longitude, latitude, and speed, at the same time, then the SdsType should include those three properties. An SdsType defines the structure of an event stored in an SdsStream and how to associate events within the SdsStream. An event is a single unit whose properties have values that relate to the index; that is, each property of an SdsType event is related to the event???s index. Each event is a single unit. SdsTypes can define simple atomic types, such as integers, floats, strings, arrays, and dictionaries, or complex types with nested SdsTypes using the Properties collection of an SdsType. An SdsType used to define an SdsStream must have a key, which is a property, or a combination of properties that constitute an ordered, unique identity. Because the key is ordered, it functions as an index. It is known as the primary index. While a timestamp (DateTime) is a very common key, any ordered data can be used. Other indexes (secondary indexes), are defined in the SdsStream. For more details on indexes, see Indexes . An SdsType is referenced by its identifier or Id field. SdsType identifiers must be unique within a Namespace. An SdsType can also refer other SdsTypes by using their identifiers. This enables type reusability. Nested types and base types are automatically created as separate types. For further information, see SdsType reusability . Once an SdsType is created, it is immutable and its definition cannot be changed. If the SdsType definition is incorrect, you must delete and recreate it, and it can only be deleted if no streams, stream views, or other types reference it. Only the SdsTypes used to define SdsStreams or SdsStreamViews are required to be added to the Sequential Data Store. SdsTypes that define properties or base types are contained within the parent SdsType do not need to be added to the Data Store separately. SdsTypes define how events are associated and read within an SdsStream. When attempting to read non-existent indexes, indexes that fall between, before, or after existing indexes, the results are determined by the interpolation and extrapolation settings of the SdsType. For more information about interpolation and extrapolation, see Read characteristics . SdsType fields and properties The following table shows the SdsType fields. Fields that are not included are reserved for internal SDS use. Property Type Optionality Searchable Details ID String Required Yes Identifier for referencing the type Name String Optional Yes Friendly name Description String Optional Yes Description text SdsTypeCode SdsTypeCode Required No Numeric code identifying the base SdsType InterpolationMode SdsInterpolationMode Optional No Interpolation setting of the type. Default is Continuous. ExtrapolationMode SdsExtrapolationMode Optional No Extrapolation setting of the type. Default is All. Properties IList\u003cSdsTypeProperty\u003e Required Yes, with limitations List of SdsTypeProperty items. See SdsTypeProperty below. For search limitations, see Search in SDS . Rules for the type identifier (SdsType.ID) The type identifier, SdsType.ID , has the following requirements: Is not case sensitive Can contain spaces Cannot contain forward slash (\"/\") (\" \") Contains a maximum of 100 characters Type management using the .NET SDS client libraries methods is performed through ISdsMetadataService . You can create ISdsMetadataService using one of the SdsService.GetMetadataService() factory methods. .NET client libraries provide SdsTypeBuilder to help build SdsTypes from .NET types. SdsTypeProperty The Properties collection defines each field in an SdsType. Type properties will appear in every stream that is created from a given type. The following table shows the required and optional SdsTypeProperty fields. Fields that are not included are reserved for internal SDS use. Property Type Optionality Details Id String Required Identifier for referencing the type Name String Optional Friendly name Description String Optional Description text SdsType SdsType Required Field defining the property\u0027s Type IsKey Boolean Required Identifies the property as the Key (Primary Index) Value Object Optional Value of the property Order Int Optional Order of comparison within a compound index InterpolationMode SdsInterpolationMode Optional Interpolation setting of the property. Default is null. Uom String Optional Unit of Measure of the property. For a list of units of measures that are supported for an SdsTypeProperty, see Units of measure . The SdsTypeProperty identifier has the same requirements as the SdsType identifier, which are: Is not case sensitive Can contain spaces Cannot contain forward slash (\"/\") (\" \") Contains a maximum of 100 characters IsKey IsKey is a Boolean value that identifies the SdsType Key. Each SdsType needs a Key to function as the primary index. A key defined by more than one property is called a compound key. A compound key can be defined by a maximum of three properties. In a compound key, each property that is included in the key is specified as IsKey . The Order field then defines how the keys are combined in the compound key. Value The Value field is used for properties that represent a value. For example, the named constant for an enum is a property with a value. When representing an enum in a SdsType, the SdsType properties collection defines the constant list for the enum. The SdsTypeProperty Identifier represents the name of the constant and the SdsTypeProperty value represents the value of the constant (see the enum State definitions below). InterpolationMode InterpolationMode is assigned when the property of the event should be interpolated in a specific way that differs from the interpolation mode of the SdsType. InterpolationMode is only applied to a property that is not part of the index. If the InterpolationMode is not set, the property inherits the InterpolationMode of the SdsType. An SdsType with the InterpolationMode set to Discrete cannot have a property with an InterpolationMode . For more information on interpolation of events, see Interpolation . UOM Uom is the unit of measure for the property. The Uom of a property may be specified by the name or the abbreviation. The names and abbreviations of Uoms are case sensitive. The InterpolationMode and Uom of a property can be overridden on the stream. For more information, see SdsStreamPropertyOverride in Streams . Supported units of measure For a list of units of measures that are supported for an SdsTypeProperty, see Units of measure ."
                                   },
    "content/sds/sds-views.html":  {
                                       "href":  "content/sds/sds-views.html",
                                       "title":  "Stream views",
                                       "keywords":  "Stream views Stream views provide flexibility in the use of SdsTypes. While you cannot actually change the properties of SdsTypes themselves, use the stream views feature to create a view of a selected SdsStream that appears as if you had changed the SdsType on which it is based. Create a stream view by choosing a source and target type. Then, create a set of mappings between properties of those two types. Using a stream view to leverage existing SdsType properties is preferable to creating a new SdsType because the SdsStream that is based on the SdsType continues to function with its previously archived stream data intact. See the impact of the stream view on a stream either in an ad hoc manner through a GET method or by assigning the stream view to a stream with a PUT method. An SdsStreamView provides a way to map stream data requests from one data type to another. You can apply a stream view to any read or GET operation. SdsStreamView is used to specify the mapping between source and target types. Stream view fields and properties The following table shows the SdsStreamView fields. Fields that are not included are reserved for internal SDS use. For more information on search limitations, see Search in SDS . Property Type Optionality Searchable Details Id String Required Yes Identifier for referencing the stream view Name String Optional Yes Friendly name Description String Optional Yes Description text SourceTypeId String Required Yes Identifier of the SdsType of the SdsStream TargetTypeId String Required Yes Identifier of the SdsType to convert events to Properties IList\u003cSdsStreamViewProperty\u003e Optional Yes, with limitations Property level mapping Note: SdsStreamViewProperty objects are not searchable. Only the SdsStreamViewProperty\u0027s SdsStreamView is searchable by its Id , SourceTypeId , and TargetTypeId . These are used to return the top level SdsStreamView object when searching. The same is true for nested SdsStreamViewProperties. Rules for the stream view identifier (SdsStreamView.Id) The type identifier, SdsStreamView.ID , has the following requirements: Is not case sensitive Can contain spaces Cannot contain forward slash (\"/\") (\" \") Contains a maximum of 100 characters SdsStreamView mapping SDS attempts to map properties from the source to the destination. When the mapping is straightforward, SDS automatically maps properties from the source to the target type. For example: The properties are in the same position The properties are of the same data type The properties have the same name When SDS is unable to determine how to map a source property, the property is removed. If SDS encounters a target property that it cannot map to, the property is added and configured with a default value. To map a property that SDS cannot map automatically, define an SdsStreamViewProperty and add it to the SdsStreamView???s properties. The following table show mapping compatibilities. SDS largely supports mapping within the same data type. Source type\\ Target type Numeric types Nullable numeric types Enumeration types Nullable enumeration types Object types Numeric types Yes Yes No No No Nullable numeric types Yes Yes No No No Enumeration types No No Yes Yes No Nullable enumeration types No No Yes Yes No Object types No No No No Yes* * Mappable if typeId matches between the source and the target type SdsStreamViewProperty The SdsStreamView properties collection provides detailed instructions for mapping of event properties. Each SdsStreamViewProperty in the properties collection defines the mapping of an event???s property. SdsStreamView properties are required only when property mapping is not straightforward. Additionally, if you do not want a type property mapped, it is not necessary to create an SdsStreamView property for it. The following table shows the required and optional SdsStreamViewProperty fields. Property Type Optionality Details SourceId String Required Identifier of the SdsTypeProperty from the source SdsType Properties list TargetId String Required Identifier of the SdsTypeProperty from the target SdsType Properties list SdsStreamView SdsStreamView Optional Additional mapping instructions for derived types. Supports nested properties SdsStreamViewMap When an SdsStreamView is added, SDS defines a plan mapping and stores it as an SdsStreamViewMap . The SdsStreamViewMap provides a detailed property-by-property definition of the mapping. The following table shows the SdsStreamViewMap fields. The SdsStreamViewMap cannot be written to SDS, it can only be retrieved from SDS, so required and optional have no meaning. Property Type Optionality Details SourceTypeId String Required Identifier of the SdsType of the SdsStream TargetTypeId String Required Identifier of the SdsType to convert events to Properties IList\u003cSdsStreamViewMapProperty\u003e Optional Property level mapping Properties /   SdsStreamViewMapProperty The SdsStreamViewMapProperty is similar an SdsStreamViewProperty but adds a mode detailing one or more actions taken on the property. The following table shows the SdsStreamViewMapProperty fields. The SdsStreamViewMap cannot be written to SDS; it can only be retrieved from SDS, so required and optional have no meaning. Property Type Details SourceTypeId String Identifier of the SdsType of the SdsStream TargetTypeId String Identifier of the SdsType to convert events to Mode SdsStreamViewMode Aggregate of actions applied to the properties. SdsStreamViewModes are combined via binary arithmetic. SdsStreamViewMap SdsStreamViewMap Mapping for derived types The available SdsStreamViewModes are shown in the following table. Name Value Description None 0x0000 No action FieldAdd 0x0001 Add a property matching the specified SdsTypeProperty FieldRemove 0x0002 Remove the property matching the specified SdsTypeProperty FieldRename 0x0004 Rename the property matching the source SdsTypeProperty to the target SdsTypeProperty FieldMove 0x0008 Move the property from the location in the source to the location in the target FieldConversion 0x0016 Converts the source property to the target type InvalidFieldConversion 0x0032 Cannot perform the specified mapping Changing stream type Stream views can be used to change the type defining a stream. You cannot modify the SdsType; types are immutable. But you can map a stream from its current type to a new type. To update a stream\u0027s type, define an SdsStreamView and PUT the stream view to the following: PUT api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Type?streamViewId={streamViewId} api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Type?streamViewId={streamViewId} For details, see Update Stream Type ."
                                   },
    "content/sds/search.html":  {
                                    "href":  "content/sds/search.html",
                                    "title":  "Search in SDS",
                                    "keywords":  "Search in SDS You can search for objects using texts, phrases and fields in Sequential Data Store (SDS). The REST APIs or .NET client libraries methods GetStreamsAsync , GetTypesAsync , and GetStreamViewsAsync return items that match the search criteria within a given namespace. By default, the query parameter applies to all searchable object fields. For example, a namespace contains the following streams: streamId Name Description stream1 tempA Temperature from DeviceA stream2 pressureA Pressure from DeviceA stream3 calcA Calculation from DeviceA values A GetStreamsAsync call with different queries will return below: Query string Returns temperature stream1 calc* stream3 DeviceA* stream1, stream2, stream3 humidity* nothing Requests GET api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Streams?query=name:pump api v1 Tenants {tenantId} Namespaces {namespaceId} Streams?query=name:pump name:pressure\u0026orderby=name GET api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Streams?query=name:pump api v1 Tenants {tenantId} Namespaces {namespaceId} Streams?query=name:pump name:pressure\u0026orderby=id asc GET api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Streams?query=name:pump api v1 Tenants {tenantId} Namespaces {namespaceId} Streams?query=name:pump name:pressure\u0026orderby=name desc GET api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Streams?query=name:pump api v1 Tenants {tenantId} Namespaces {namespaceId} Streams?query=name:pump name:pressure\u0026orderby=name desc\u0026skip=10\u0026count=20 Parameters string query [Optional] Parameter representing the search criteria. If unspecified, returns all values. Can be used with skip , count and orderby . int skip [Optional] Parameter representing the zero-based offset of the first SdsStream to retrieve. The number of matched items to skip over before returning. If unspecified, a default value of 0 is used. Use when more items match the search criteria than can be returned in a single call. int count [Optional] Parameter representing the maximum number of streams to retrieve. If unspecified, a default value of 100 is used. string orderby [Optional] Parameter representing the sorted order in which streams are returned. Requires a field name ( orderby=name , for example). Default order is ascending ( asc ). Add desc for descending order ( orderby=name desc , for example). If unspecified, there is no sorting of results. .NET client libraries methods If there are 175 streams that match the search criteria \"temperature\" in a single call for example, the following call will return the first 100 matches: _metadataService.GetStreamsAsync(query:\"temperature\", skip:0, count:100) If skip is set to 100 , the following call will return the remaining 75 matches while skipping over the first 100: _metadataService.GetStreamsAsync(query:\"temperature\", skip:100, count:100) Search for streams Streams search is exposed through the REST API and the client libraries method GetStreamsAsync . For more information on stream properties, see Streams . Searchable Properties Property Searchable Id Yes TypeId Yes Name Yes Description Yes Indexes No InterpolationMode No ExtrapolationMode No PropertyOverrides No Searchable Child Resources Property Searchable Metadata * Yes Tags * Yes ACL No Owner No Note: You can access stream metadata and tags through Metadata API and Tags API respectively. Metadata and tags are associated with streams and can be used as search criteria. See How search works with stream metadata for more information. Request Search for streams using the REST API and specifying the optional query parameter: GET api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Streams?query={query}\u0026skip={skip}\u0026count={count} api v1 Tenants {tenantId} Namespaces {namespaceId} Streams?query={query}\u0026skip={skip}\u0026count={count} Parameters string query [Optional] Parameter representing the search criteria. If unspecified, returns all values. Can be used with skip , count and orderby . int skip [Optional] Parameter representing the zero-based offset of the first SdsStream to retrieve. If unspecified, a default value of 0 is used. Use when more items match the search criteria than can be returned in a single call. int count [Optional] Parameter representing the maximum number of streams to retrieve. If unspecified, a default value of 100 is used. .NET client libraries method GetStreamsAsync is used to search for and return streams. _metadataService.GetStreamsAsync(query:\"QueryString\", skip:0, count:100); The stream fields valid for search are identified in the fields table located on the [Streams](xref:eam metadata has unique syntax rules. For details, see How search works with stream metadata . Search for types Type search is exposed through the REST API and the client libraries method GetTypesAsync . For more information on type properties, see Types . Searchable Properties Property Searchable Id Yes Name Yes Description Yes SdsTypeCode No InterpolationMode No ExtrapolationMode No Properties Yes, with limitations* Note: The Name and Id of an SdsType are included in its Properties field. Similarly, Name and Id of a nested type are included in its Properties . If there are two types with the same Properties , Name , or Id , the search will return both types in the result. Request Search for types using the REST API and specifying the optional query parameter: GET api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Types?query={query}\u0026skip={skip}\u0026count={count} api v1 Tenants {tenantId} Namespaces {namespaceId} Types?query={query}\u0026skip={skip}\u0026count={count} Parameters string query [Optional] Parameter representing the search criteria. If unspecified, returns all values. Can be used with skip , count and orderby . int skip [Optional] Parameter representing the zero-based offset of the first type to retrieve. If unspecified, a default value of 0 is used. Use when more items match the search criteria than can be returned in a single call. int count [Optional] Parameter representing the maximum number of types to retrieve. If unspecified, a default value of 100 is used. .NET client libraries method GetTypesAsync is used to search for and return types. _metadataService.GetTypesAsync(query:\"QueryString\", skip:0, count:100); Search for stream views Stream view search is exposed through the REST API and the client libraries method GetStreamViewsAsync . For more information on stream view properties, see Stream Views . Searchable Properties Property Searchable Id Yes Name Yes Description Yes SourceTypeId Yes TargetTypeId Yes Properties Yes, with limitations* Note: The Properties collection contains a list of SdsStreamViewProperty objects. The query attempts to find a match on the SdsStreamViewProperty \u0027s Id , SourceTypeId , and TargetTypeId fields. The Properties collection of nested views are also searched. See the following example. Example You can search for ComplexView using the Id (\"NestedView\"), SourceTypeId , and TargetTypeId of NestedView but not its Description (\"An example of a nested view\"). { \"Id\":\"ComplexView\", \"Name\":\"ComplexView\", \"SourceTypeId\":\"ComplexSourceType\", \"TargetTypeId\":\"ComplexTargetType\", \"Description\":null, \"Properties\":[ { \"SourceId\":\"Value\", \"TargetId\":\"Value\", \"SdsStreamView\":{ \"Id\":\"NestedView\", \"SourceTypeId\":\"NestedType\", \"TargetTypeId\":\"NestedType\", \"Description\":\"An example of a nested view\", \"Properties\":[ { \"SourceId\":\"Value\", \"TargetId\":\"Value\" } ] } } ] } Request Search for stream views using the REST API and specifying the optional query parameter: GET api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/StreamViews?query={query}\u0026skip={skip}\u0026count={count} api v1 Tenants {tenantId} Namespaces {namespaceId} StreamViews?query={query}\u0026skip={skip}\u0026count={count} Parameters string query [Optional] Parameter representing the search criteria. If unspecified, returns all values. Can be used with skip , count and orderby . int skip [Optional] Parameter representing the zero-based offset of the first stream view to retrieve.If unspecified, a default value of 0 is used. Use when more items match the search criteria than can be returned in a single call. int count [Optional] Parameter representing the maximum number of stream views to retrieve. If unspecified, a default value of 100 is used. .NET client libraries method GetStreamViewsAsync is used to search for and return stream views. _metadataService.GetStreamViewsAsync(query:\"QueryString\", skip:0, count:100); Tokenization Tokenization is the process of breaking a string sequence into pieces called tokens using specific characters to delimit tokens. User-specified queries are tokenized into search terms. How the query string is tokenized can affect search results. Delimit the terms with either a space or one or more punctuation characters, such as * , ! , ? , . , and a space. A query string followed without space by other punctuation characters does not trigger tokenization and is treated as part of the term. If your query has a wildcard ( * ) operator after a punctuation character, neither the punctuation nor the wildcard operator is tokenized. To specifically search for a term that has trailing punctuation, enclose the query in quotation marks to ensure that the punctuation is part of the query. See examples below: Term Tokenized Term Description Device.1 Device.1 The token includes .1 because there is no space between it and Device . Device!!1 Device!!1 The token includes !!1 because there is no space between it and Device . Device. Device . and the following space demarcates Device as the token term. Device!! Device !! and the following space demarcates Device as the token term. Device!* Device The token does not include !* because neither is tokenized if a wildcard operator follows a punctuation character. \"Device!\"* Device! Device! is the token because the string is enclosed in double quotes. Search operators You can use search operators in the query string to get more refined search results. Use operators AND , OR , and NOT in all caps. Operator Description AND AND operator. cat AND dog searches for both \"cat\" and \"dog\". OR OR operator. cat OR dog searches for either \"cat\" or \"dog\", or both. NOT NOT operator. cat NOT dog searches for \"cat\" or those without \"dog\". * Wildcard operator. Matches 0 or more characters. log* searches for those starting with \"log\" (\"log\", \"logs\" or \"logger\" for example.); ignores case. : Field-scoped query. Specifies a field to search. id:stream* searches for streams whose id field starts with \"stream\", but will not search other fields like name or description . See Field-scoping operator below. \" \" Quote operator. Scopes the search to an exact sequence of characters. While dog food (without quotes) searches for instances with \"dog\" or \"food\" anywhere in any order, \"dog food\" (with quotes) will only match instances that contain the whole string together and in that order. ( ) Precedence operator. motel AND (wifi OR luxury) searches for either \"wifi\" or \"luxury\", or \"wifi\" and \"luxury\" at the intersection of \"motel\". Examples Query string Matches field value Does not match field value mud AND log log mud mud log mud log mud OR log log mud mud log mutt look mud AND (NOT log) mud mud log mud AND (log OR pump*) mud log mud pumps mud bath name:stream* AND (description:pressure OR description:pump) The name starts with \"stream\" and the description has either \"pressure\" or \"pump\", or both. string Field-scoping ( : ) operator You can qualify the search to a specific field using the : operator. fieldname:fieldvalue Request GET api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Streams?query=name:pump api v1 Tenants {tenantId} Namespaces {namespaceId} Streams?query=name:pump name:pressure .NET client libraries method GetStreamsAsync(query:\"name:pump name:pressure\"); Wildcard ( * ) operator You can use the wildcard operator ( * ) to complement an incomplete string. It can only be used once per token, unless they are at the beginning and end, for example *Tank* , but not *Ta*nk , Ta*nk* , or *Ta*nk* . Query string Matches field value Does not match field value log* log logger analog *log analog alog logg *log* analog alogger lop l*g log logg lake swimming ( * does not span across two tokens) Supported Not Supported * *log l*g log* *log* \"my log\"* *l*g* *l*g l*g* \"my\"*\"log\" Request GET api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Streams?query=log* api v1 Tenants {tenantId} Namespaces {namespaceId} Streams?query=log* .NET client libraries method GetStreamsAsync(query:\"log*\"); Double quotes ( \"\" ) operator Tokenized search terms are delimited by whitespace and punctuation. To include these delimiters in a search, enclose them in double quotes. When using double quotes, the matching string must include the whole value of the field on the object being searched. Partial strings will not be matched unless wildcards are used. For example, if you are searching for a stream with description Pump three on unit five , a query \"unit five\" will not match the description, but *\"unit five\" will. Note that while wildcard ( * ) can be used either in or outside of quotes, it is treated as a string literal inside quotes. For example, you can search for \"dog food\"* to find a string that starts with \"dog food\", but if you search for \"dog food*\" , it will only match the value of \"dog food*\". Query string Matches field value Does not match field value \"pump pressure\" pump pressure pressure pressure pump pump pressure gauge \"pump pressure\"* pump pressure pump pressure gauge pressure pressure pump the pump pressure gauge *\"pump pressure\" pump pressure the pump pressure pressure pressure pump the pump pressure gauge *\"pump pressure\"* pump pressure pump pressure gauge the pump pressure gauge pressure pressure pump \"pump*pressure\" pump*pressure pump pressure the pump pressure gauge Request GET api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Streams?query=\"pump api v1 Tenants {tenantId} Namespaces {namespaceId} Streams?query=\"pump pressure\" .NET client libraries method GetStreamsAsync(query:\"\\\\\"pump pressure\\\\\"\"); How search works with stream metadata Stream metadata behaves differently with search syntax rules described in the previous sections. A namespace with streams with respective metadata key-value pairs streamId Metadata stream1 { manufacturer, company } { serial, abc } stream2 { serial, a1 } stream3 { status, active } { second key, second value } Field-scoping ( : ) Operator Stream metadata key is only searchable in association with its value. This pairing is defined using the field-scoping ( : ) operator. myStreamMetadataKey:myStreamMetadataValue Metadata key is not searched if the operator ( : ) is missing in the query string: the search is then limited to metadata values along with other searchable fields in the stream. Query string Returns Description manufacturer:company stream1 Searches and returns stream1 company stream1 Searches only the metadata values due to lack of : operator and returns stream1 a* stream1, stream2, stream3 Searches the metadata values and returns all three streams Request GET api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Streams?query=manufacturer:company api v1 Tenants {tenantId} Namespaces {namespaceId} Streams?query=manufacturer:company .NET client libraries method GetStreamsAsync(query:\"manufacturer:company\"); Wildcard ( * ) Operator Wildcard ( * ) character can be used both in metadata keys and values with one caveat: wildcard ( * ) used in the field (left of field-scoped operator ( : )) will only search within SdsStream metadata. Query string Returns Description manufa*turer:compan* stream1 Searches and returns stream1 ser*al:a* stream1, stream2 Searches and returns stream1 and stream2 s*:a* stream1, stream2, stream3 Searches and returns all three streams Id:stream* stream1, stream2, stream3 Searches all fields and returns three streams. Id*:stream* nothing Wildcard in the field limits the search to metadata. Returns nothing because there is no metadata by that name that meets the criteria. Request GET api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Streams?query=manufa*turer:compan* api v1 Tenants {tenantId} Namespaces {namespaceId} Streams?query=manufa*turer:compan* .NET client libraries method GetStreamsAsync(query:\"manufa*turer:compan*\"); Special characters in search queries \u003c!--- This section is an exact replication of the same titled section in asset-search-api.md --\u003e Add the backslash escape character ( \\ ) before any special characters in search queries. The following special characters require an escape character: \" | /   * \\ ( ) : The following are examples of using the escape character in query strings. Example Field Value Query String Austin\\Dallas\\Fort Worth Austin\\\\Dallas\\\\Fort Worth 1:100 1\\:100"
                                },
    "content/sds/search/search-operators.html":  {
                                                     "href":  "content/sds/search/search-operators.html",
                                                     "title":  "Search operators",
                                                     "keywords":  "Search operators Specify search operators in the query string to return more specific search results. Operators Description AND AND operator. For example, cat AND dog searches for streams containing both \"cat\" and \"dog\". AND must be in all caps. OR OR operator. For example, cat OR dog searches for streams containing either \"cat\" or \"dog\" or both. OR must be in all caps. NOT NOT operator. For example, cat NOT dog searches for streams that have the \"cat\" term or do not have \"dog\". NOT must be in all caps. * Wildcard operator. For example, cat* searches for streams that have a term that starts with \"cat\", ignoring case. : Field-scoped query. For example, id:stream* will search for streams where the id field starts with \"stream\", but will not search on other fields like name or description . Note: Field names are camel case and are case sensitive. ( ) Precedence operator. For example, motel AND (wifi OR luxury) searches for streams containing motel and either wifi or luxury (or both). Note: You can use the wildcard * only once for each search term, except for the case of a Contains type query clause. In that case, two wildcards are allowed: one as prefix and one as suffix; for example, *Tank* is valid, but *Ta*nk , Ta*nk* , and *Ta*nk* are not supported. The wildcard * only works when specifying a single search term. For example, you can search for Tank* , *Tank , Ta*nk but not Tank Meter* . : Operator Set the fields to search using the following syntax: fieldname:fieldvalue Request The following example shows the \u0027:\u0027 operator. GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams?query=name:pump api v1 Tenants default Namespaces {namespaceId} Streams?query=name:pump name:pressure * Operator Use the \u0027*\u0027 character as a wildcard to specify an incomplete string. Query string Matches field value Does not match field value log* log logger analog *log analog alog logg *log* analog alogger lop l*g log logg lop Supported Not Supported * *log l*g log* *log* *l*g* *l*g l*g* Request The following example shows the \u0027*\u0027 operator. GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams?query=log* api v1 Tenants default Namespaces {namespaceId} Streams?query=log* Other operator examples Query string Matches field value Does not match field value mud AND log log mud mud log mud log mud OR log log mud mud log mud AND (NOT log) mud mud log mud AND (log OR pump*) mud log mud pumps mud bath name:stream* AND (description:pressure OR description:pump) The name starts with \"stream\" and the description includes either term \"pressure\" or term \"pump\""
                                                 },
    "content/sds/sequential-data-store.html":  {
                                                   "href":  "content/sds/sequential-data-store.html",
                                                   "title":  "SDS reference",
                                                   "keywords":  "SDS reference The Sequential Data Store (SDS) is a cloud-based streaming data storage that is optimized for storing sequential data, typically time-series. It can store any data that is indexed by an ordered sequence. Use SDS to store, retrieve, and analyze data. An SdsType, also called a type, defines the shape of a single measured event or object. A type gives structure to the data. For example, if you are measuring three things, such as longitude, latitude, and speed, from a device at the same time, then you need to include those three properties in the type. An SdsStream, also called a stream, is a collection of ordered events, where each event is an instance of the type. You create and write data to streams using a simple REST (REpresentational State Transfer) API (Application Programming Interface). You can use the streams to store simple or complex data types to suit your application needs. You can define simple or complex indexes to arrange and relate your data. An assortment of methods with customizable behaviors are available to read data and easily obtain needed information. Edge Data Store includes the Sequential Data Store (SDS) REST APIs to read and write data stored locally on the Edge Data Store device. SDS is the same technology used in AVEVA Data Hub for storing data, so the usage of the REST APIs is very similar to AVEVA Data Hub for reading and writing data. All data from all sources on the Edge Data Store can be read using the SDS REST APIs on the local device, in the default tenant and the default namespace. In addition, the default tenant has a diagnostics namespace where diagnostic data are written by the Edge Data Store and installed components that can be read to monitor the health of a running system using the SDS REST APIs. SDS is an advanced storage engine that is also used in AVEVA Data Hub. While it works very well for storing OMF compatible data in EDS, you can also use it for advanced scenarios where data stored in SDS cannot be converted to OMF. To convert streams in EDS for egress using OMF, streams in EDS must have only a single time-based index."
                                               },
    "content/sds/streams/sds-streams-api.html":  {
                                                     "href":  "content/sds/streams/sds-streams-api.html",
                                                     "title":  "SdsStream API",
                                                     "keywords":  "SdsStream API The REST APIs provide programmatic access to read and write SDS data. The APIs in this section interact with SdsStreams. For general SdsStream information, see Streams . List Streams Returns a list of streams Request GET /api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Streams?query={query}\u0026skip={skip}\u0026count={count}\u0026orderby={orderby}  api v1 Tenants {tenantId} Namespaces {namespaceId} Streams?query={query}\u0026skip={skip}\u0026count={count}\u0026orderby={orderby} Parameters string tenantId Tenant identifier string namespaceId Namespace identifier [Optional] string query Query identifier. See Search in SDS for information about specifying the search parameter. [Optional] int skip Parameter representing the zero-based offset of the first object to retrieve. If unspecified, a default value of 0 is used. [Optional] int count Parameter representing the maximum number of objects to retrieve. If unspecified, a default value of 100 is used. [Optional] string orderby Parameter representing sorted order. A field name is required. The sorting is based on the stored values for a given field (of type string ). For example, orderby=name would sort the returned results by the name values (ascending by default). Additionally, a value can be provided along with the field name to identify whether to sort ascending or descending, by using values asc or desc , respectively. For example, orderby=name desc would sort the returned results by the name values, descending. If no value is specified, there is no sorting of results. Response Status Code Body Type Description 200 SdsStream [] Returns a list of SdsStream objects 400 ErrorResponseBody Missing or invalid inputs 401 ErrorResponseBody Unauthorized 403 ErrorResponseBody Forbidden 404 ErrorResponseBody One of the resources specified was not found. 500 ErrorResponseBody An error occurred while processing the request. 503 ErrorResponseBody Service Unavailable Example response body 200 Response ( SdsStream []) [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"TypeId\": \"string\", \"Indexes\": [ { \"SdsTypePropertyId\": \"string\" } ], \"InterpolationMode\": 0, \"ExtrapolationMode\": 0, \"PropertyOverrides\": [ { \"SdsTypePropertyId\": \"string\", \"Uom\": \"string\", \"InterpolationMode\": 0 } ] } ] Get Stream Returns the specified stream Request GET /api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Streams/{streamId}  api v1 Tenants {tenantId} Namespaces {namespaceId} Streams {streamId} Parameters string tenantId Tenant identifier string namespaceId Namespace identifier string streamId Stream identifier Response Status Code Body Type Description 200 SdsStream Returns the SdsStream 400 ErrorResponseBody Missing or invalid inputs 401 ErrorResponseBody Unauthorized 403 ErrorResponseBody Forbidden 404 ErrorResponseBody One of the resources specified was not found. 500 ErrorResponseBody An error occurred while processing the request. 503 ErrorResponseBody Service Unavailable Example response body 200 Response ( SdsStream ) { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"TypeId\": \"string\", \"Indexes\": [ { \"SdsTypePropertyId\": \"string\" } ], \"InterpolationMode\": 0, \"ExtrapolationMode\": 0, \"PropertyOverrides\": [ { \"SdsTypePropertyId\": \"string\", \"Uom\": \"string\", \"InterpolationMode\": 0 } ] } Get Or Create Stream Creates the specified stream. If a stream with a matching identifier already exists, SDS compares the existing stream with the stream that was sent. Request POST /api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Streams/{streamId}  api v1 Tenants {tenantId} Namespaces {namespaceId} Streams {streamId} Parameters string tenantId Tenant identifier string namespaceId Namespace identifier string streamId Stream identifier Response Status Code Body Type Description 200 SdsStream Returns the SdsStream 201 SdsStream Returns the SdsStream 400 ErrorResponseBody Missing or invalid inputs 401 ErrorResponseBody Unauthorized 403 ErrorResponseBody Forbidden 404 ErrorResponseBody One of the resources specified was not found. 409 ErrorResponseBody Conflict 500 ErrorResponseBody An error occurred while processing the request. 503 ErrorResponseBody Service Unavailable Example response body 200 Response ( SdsStream ) { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"TypeId\": \"string\", \"Indexes\": [ { \"SdsTypePropertyId\": \"string\" } ], \"InterpolationMode\": 0, \"ExtrapolationMode\": 0, \"PropertyOverrides\": [ { \"SdsTypePropertyId\": \"string\", \"Uom\": \"string\", \"InterpolationMode\": 0 } ] } Create Or Update Stream Creates the specified stream. If a stream with the same Id already exists, the definition of the stream is updated. Request PUT /api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Streams/{streamId}  api v1 Tenants {tenantId} Namespaces {namespaceId} Streams {streamId} Parameters string tenantId Tenant identifier string namespaceId Namespace identifier string streamId Stream identifier Response Status Code Body Type Description 201 SdsStream Returns the SdsStream 204 None Returns the SdsStream 400 ErrorResponseBody Missing or invalid inputs 401 ErrorResponseBody Unauthorized 403 ErrorResponseBody Forbidden 404 ErrorResponseBody One of the resources specified was not found. 409 ErrorResponseBody Conflict 500 ErrorResponseBody An error occurred while processing the request. 503 ErrorResponseBody Service Unavailable Example response body 201 Response ( SdsStream ) { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"TypeId\": \"string\", \"Indexes\": [ { \"SdsTypePropertyId\": \"string\" } ], \"InterpolationMode\": 0, \"ExtrapolationMode\": 0, \"PropertyOverrides\": [ { \"SdsTypePropertyId\": \"string\", \"Uom\": \"string\", \"InterpolationMode\": 0 } ] } Delete Stream Deletes a stream from the specified tenant and namespace Request DELETE /api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Streams/{streamId}  api v1 Tenants {tenantId} Namespaces {namespaceId} Streams {streamId} Parameters string tenantId Tenant identifier string namespaceId Namespace identifier string streamId Stream identifier Response Status Code Body Type Description 204 None SdsStream was successfully deleted 400 ErrorResponseBody Missing or invalid inputs 401 ErrorResponseBody Unauthorized 403 ErrorResponseBody Forbidden 404 ErrorResponseBody One of the resources specified was not found. 500 ErrorResponseBody An error occurred while processing the request. 503 ErrorResponseBody Service Unavailable Example response body 400 Response ( ErrorResponseBody ) { \"OperationId\": \"string\", \"Error\": \"string\", \"Reason\": \"string\", \"Resolution\": \"string\", \"Parameters\": { \"property1\": \"string\", \"property2\": \"string\" } } Get Stream Type Returns the type definition that is associated with a given stream Request GET /api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Streams/{streamId}/Type  api v1 Tenants {tenantId} Namespaces {namespaceId} Streams {streamId} Type Parameters string tenantId Tenant identifier string namespaceId Namespace identifier string streamId Stream identifier Response Status Code Body Type Description 200 SdsType Returns the SdsType 400 ErrorResponseBody Missing or invalid inputs 401 ErrorResponseBody Unauthorized 403 ErrorResponseBody Forbidden 404 ErrorResponseBody One of the resources specified was not found. 500 ErrorResponseBody An error occurred while processing the request. 503 ErrorResponseBody Service Unavailable Example response body 200 Response ( SdsType ) { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": 0, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": 0, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": null, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": [ {} ], \"Properties\": [ {} ], \"BaseType\": null, \"DerivedTypes\": [ {} ], \"InterpolationMode\": null, \"ExtrapolationMode\": null } ], \"Properties\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"Order\": 0, \"IsKey\": true, \"FixedSize\": 0, \"SdsType\": null, \"Value\": null, \"Uom\": \"string\", \"InterpolationMode\": null, \"IsQuality\": true } ], \"BaseType\": { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": null, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": [ null ], \"Properties\": [ null ], \"BaseType\": null, \"DerivedTypes\": [ null ], \"InterpolationMode\": null, \"ExtrapolationMode\": null }, \"DerivedTypes\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": null, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": [ {} ], \"Properties\": [ {} ], \"BaseType\": null, \"DerivedTypes\": [ {} ], \"InterpolationMode\": null, \"ExtrapolationMode\": null } ], \"InterpolationMode\": 0, \"ExtrapolationMode\": 0 } ], \"Properties\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"Order\": 0, \"IsKey\": true, \"FixedSize\": 0, \"SdsType\": { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": null, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": [ null ], \"Properties\": [ null ], \"BaseType\": null, \"DerivedTypes\": [ null ], \"InterpolationMode\": null, \"ExtrapolationMode\": null }, \"Value\": null, \"Uom\": \"string\", \"InterpolationMode\": 0, \"IsQuality\": true } ], \"BaseType\": { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": 0, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": null, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": [ null ], \"Properties\": [ null ], \"BaseType\": null, \"DerivedTypes\": [ null ], \"InterpolationMode\": null, \"ExtrapolationMode\": null } ], \"Properties\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"Order\": 0, \"IsKey\": true, \"FixedSize\": 0, \"SdsType\": null, \"Value\": null, \"Uom\": \"string\", \"InterpolationMode\": null, \"IsQuality\": true } ], \"BaseType\": { \"Id\": null, \"Name\": null, \"Description\": null, \"SdsTypeCode\": null, \"IsGenericType\": null, \"IsReferenceType\": null, \"GenericArguments\": null, \"Properties\": null, \"BaseType\": null, \"DerivedTypes\": null, \"InterpolationMode\": null, \"ExtrapolationMode\": null }, \"DerivedTypes\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": null, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": [ null ], \"Properties\": [ null ], \"BaseType\": null, \"DerivedTypes\": [ null ], \"InterpolationMode\": null, \"ExtrapolationMode\": null } ], \"InterpolationMode\": 0, \"ExtrapolationMode\": 0 }, \"DerivedTypes\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": 0, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": null, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": [ {} ], \"Properties\": [ {} ], \"BaseType\": null, \"DerivedTypes\": [ {} ], \"InterpolationMode\": null, \"ExtrapolationMode\": null } ], \"Properties\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"Order\": 0, \"IsKey\": true, \"FixedSize\": 0, \"SdsType\": null, \"Value\": null, \"Uom\": \"string\", \"InterpolationMode\": null, \"IsQuality\": true } ], \"BaseType\": { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": null, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": [ null ], \"Properties\": [ null ], \"BaseType\": null, \"DerivedTypes\": [ null ], \"InterpolationMode\": null, \"ExtrapolationMode\": null }, \"DerivedTypes\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": null, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": [ {} ], \"Properties\": [ {} ], \"BaseType\": null, \"DerivedTypes\": [ {} ], \"InterpolationMode\": null, \"ExtrapolationMode\": null } ], \"InterpolationMode\": 0, \"ExtrapolationMode\": 0 } ], \"InterpolationMode\": 0, \"ExtrapolationMode\": 0 } Update Stream Type Returns the type definition that is associated with a given stream Request PUT /api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Streams/{streamId}/Type  api v1 Tenants {tenantId} Namespaces {namespaceId} Streams {streamId} Type Parameters string tenantId Tenant identifier string namespaceId Namespace identifier string streamId Stream identifier Response Status Code Body Type Description 204 None SdsType was successfully updated 400 ErrorResponseBody Missing or invalid inputs 401 ErrorResponseBody Unauthorized 403 ErrorResponseBody Forbidden 404 ErrorResponseBody One of the resources specified was not found. 409 ErrorResponseBody Conflict 500 ErrorResponseBody An error occurred while processing the request. 503 ErrorResponseBody Service Unavailable Example response body 400 Response ( ErrorResponseBody ) { \"OperationId\": \"string\", \"Error\": \"string\", \"Reason\": \"string\", \"Resolution\": \"string\", \"Parameters\": { \"property1\": \"string\", \"property2\": \"string\" } } Get Resolved Stream Request GET /api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Streams/{streamId}/Resolved  api v1 Tenants {tenantId} Namespaces {namespaceId} Streams {streamId} Resolved Parameters string tenantId string namespaceId string streamId Response Status Code Body Type Description 200 SdsResolvedStream None 400 ErrorResponseBody None 401 ErrorResponseBody None 403 ErrorResponseBody None 404 ErrorResponseBody None 500 ErrorResponseBody None 503 ErrorResponseBody None Example response body 200 Response ( SdsResolvedStream ) { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"TypeId\": \"string\", \"Indexes\": [ { \"SdsTypePropertyId\": \"string\" } ], \"InterpolationMode\": 0, \"ExtrapolationMode\": 0, \"PropertyOverrides\": [ { \"SdsTypePropertyId\": \"string\", \"Uom\": \"string\", \"InterpolationMode\": 0 } ], \"Resolved\": true, \"Type\": { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": \"Empty\", \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": 0, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": \"[\u003cSdsType\u003e]\", \"Properties\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"Order\": 0, \"IsKey\": true, \"FixedSize\": 0, \"SdsType\": \"\u003cSdsType\u003e\", \"Value\": null, \"Uom\": \"string\", \"InterpolationMode\": 0, \"IsQuality\": true } ], \"BaseType\": \"\u003cSdsType\u003e\", \"DerivedTypes\": \"[\u003cSdsType\u003e]\", \"InterpolationMode\": 0, \"ExtrapolationMode\": 0 } ], \"Properties\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"Order\": 0, \"IsKey\": true, \"FixedSize\": 0, \"SdsType\": \"\u003cSdsResolvedType\u003e\", \"Value\": null, \"Uom\": \"string\", \"InterpolationMode\": \"Continuous\", \"IsQuality\": true } ], \"BaseType\": { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": 0, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": \"[\u003cSdsType\u003e]\", \"Properties\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"Order\": 0, \"IsKey\": true, \"FixedSize\": 0, \"SdsType\": \"\u003cSdsType\u003e\", \"Value\": null, \"Uom\": \"string\", \"InterpolationMode\": 0, \"IsQuality\": true } ], \"BaseType\": \"\u003cSdsType\u003e\", \"DerivedTypes\": \"[\u003cSdsType\u003e]\", \"InterpolationMode\": 0, \"ExtrapolationMode\": 0 }, \"DerivedTypes\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": 0, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": \"[\u003cSdsType\u003e]\", \"Properties\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"Order\": 0, \"IsKey\": true, \"FixedSize\": 0, \"SdsType\": \"\u003cSdsType\u003e\", \"Value\": null, \"Uom\": \"string\", \"InterpolationMode\": 0, \"IsQuality\": true } ], \"BaseType\": \"\u003cSdsType\u003e\", \"DerivedTypes\": \"[\u003cSdsType\u003e]\", \"InterpolationMode\": 0, \"ExtrapolationMode\": 0 } ], \"InterpolationMode\": \"Continuous\", \"ExtrapolationMode\": \"All\" } } Get Resolved Streams Request POST /api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Streams/Bulk/Streams/Resolved  api v1 Tenants {tenantId} Namespaces {namespaceId} Streams Bulk Streams Resolved Parameters string tenantId string namespaceId Response Status Code Body Type Description 207 SdsResolvedStreamsResponse None 400 ErrorResponseBody None 500 ErrorResponseBody None 503 ErrorResponseBody None Example response body 207 Response ( SdsResolvedStreamsResponse ) { \"Data\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"TypeId\": \"string\", \"Indexes\": [ { \"SdsTypePropertyId\": \"string\" } ], \"InterpolationMode\": 0, \"ExtrapolationMode\": 0, \"PropertyOverrides\": [ { \"SdsTypePropertyId\": \"string\", \"Uom\": \"string\", \"InterpolationMode\": 0 } ], \"Resolved\": true, \"Type\": { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": \"Empty\", \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": 0, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": \"[\u003cSdsType\u003e]\", \"Properties\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"Order\": 0, \"IsKey\": true, \"FixedSize\": 0, \"SdsType\": \"\u003cSdsType\u003e\", \"Value\": null, \"Uom\": \"string\", \"InterpolationMode\": \"[\", \"IsQuality\": true } ], \"BaseType\": \"\u003cSdsType\u003e\", \"DerivedTypes\": \"[\u003cSdsType\u003e]\", \"InterpolationMode\": 0, \"ExtrapolationMode\": 0 } ], \"Properties\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"Order\": 0, \"IsKey\": true, \"FixedSize\": 0, \"SdsType\": \"\u003cSdsResolvedType\u003e\", \"Value\": null, \"Uom\": \"string\", \"InterpolationMode\": \"Continuous\", \"IsQuality\": true } ], \"BaseType\": { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": 0, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": \"[\u003cSdsType\u003e]\", \"Properties\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"Order\": 0, \"IsKey\": true, \"FixedSize\": 0, \"SdsType\": null, \"Value\": null, \"Uom\": \"string\", \"InterpolationMode\": null, \"IsQuality\": true } ], \"BaseType\": \"\u003cSdsType\u003e\", \"DerivedTypes\": \"[\u003cSdsType\u003e]\", \"InterpolationMode\": 0, \"ExtrapolationMode\": 0 }, \"DerivedTypes\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": 0, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": \"[\u003cSdsType\u003e]\", \"Properties\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"Order\": 0, \"IsKey\": true, \"FixedSize\": 0, \"SdsType\": \"\u003cSdsType\u003e\", \"Value\": null, \"Uom\": \"string\", \"InterpolationMode\": \"[\", \"IsQuality\": true } ], \"BaseType\": \"\u003cSdsType\u003e\", \"DerivedTypes\": \"[\u003cSdsType\u003e]\", \"InterpolationMode\": 0, \"ExtrapolationMode\": 0 } ], \"InterpolationMode\": \"Continuous\", \"ExtrapolationMode\": \"All\" } } ], \"ChildErrors\": [ { \"OperationId\": \"string\", \"Error\": \"string\", \"Reason\": \"string\", \"Resolution\": \"string\", \"Parameters\": { \"property1\": \"string\", \"property2\": \"string\" }, \"StreamId\": \"string\", \"StatusCode\": 100 } ] } Definitions SdsStream Properties Property Name Data Type Required Nullable Description Id string false true An unique identifier for the SdsStream object Name string false true An optional user-friendly name for the SdsStream object Description string false true A brief description of the SdsStream object TypeId string false true An unique identifier for the SdsType of the SdsStream object Indexes [ SdsStreamIndex ] false true List of SdsStreamIndex s to define secondary indexes for the SdsStream InterpolationMode SdsInterpolationMode false true Defines the SdsInterpolationMode of the SdsStream . Default is null . ExtrapolationMode SdsExtrapolationMode false true Defines the SdsExtrapolationMode of the SdsStream . Default is null . PropertyOverrides [ SdsStreamPropertyOverride ] false true List of SdsStreamPropertyOverrides to define unit of measure and interpolation mode overrides for the SdsStream { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"TypeId\": \"string\", \"Indexes\": [ { \"SdsTypePropertyId\": \"string\" } ], \"InterpolationMode\": 0, \"ExtrapolationMode\": 0, \"PropertyOverrides\": [ { \"SdsTypePropertyId\": \"string\", \"Uom\": \"string\", \"InterpolationMode\": 0 } ] } SdsStreamIndex Indexes speed up and order the results of stream data filtering. SdsStreamIndex or secondary indexes is defined on the stream and is applied to a single property. You can define several secondary indexes. Secondary index values need not be unique. Properties Property Name Data Type Required Nullable Description SdsTypePropertyId string false true An unique identifier for the SdsTypeProperty { \"SdsTypePropertyId\": \"string\" } SdsInterpolationMode Interpolation modes that can be applied to SdsType , SdsTypeProperty , SdsStream , and SdsStreamPropertyOverride objects Enumerated Values Property Value Description Default 0 Interpolation modes that can be applied to SdsType , SdsTypeProperty , SdsStream , and SdsStreamPropertyOverride objects Continuous 0 Interpolation modes that can be applied to SdsType , SdsTypeProperty , SdsStream , and SdsStreamPropertyOverride objects StepwiseContinuousLeading 1 Interpolation modes that can be applied to SdsType , SdsTypeProperty , SdsStream , and SdsStreamPropertyOverride objects StepwiseContinuousTrailing 2 Interpolation modes that can be applied to SdsType , SdsTypeProperty , SdsStream , and SdsStreamPropertyOverride objects Discrete 3 Interpolation modes that can be applied to SdsType , SdsTypeProperty , SdsStream , and SdsStreamPropertyOverride objects ContinuousNullableLeading 4 Interpolation modes that can be applied to SdsType , SdsTypeProperty , SdsStream , and SdsStreamPropertyOverride objects ContinuousNullableTrailing 5 Interpolation modes that can be applied to SdsType , SdsTypeProperty , SdsStream , and SdsStreamPropertyOverride objects SdsExtrapolationMode Defines how a stream responds to requests with indexes that precede or follow all data in the stream. Behavior also depends on the SdsInterpolationMode for a stream. If SdsInterpolationMode is set to Discrete , extrapolation will not occur. If SdsInterpolationMode is set to ContinuousNullableLeading or ContinuousNullableTrailing , default values will be returned instead of actual data. Enumerated Values Property Value Description All 0 Defines how a stream responds to requests with indexes that precede or follow all data in the stream. Behavior also depends on the SdsInterpolationMode for a stream. If SdsInterpolationMode is set to Discrete , extrapolation will not occur. If SdsInterpolationMode is set to ContinuousNullableLeading or ContinuousNullableTrailing , default values are returned instead of actual data. None 1 Defines how a stream responds to requests with indexes that precede or follow all data in the stream. Behavior also depends on the SdsInterpolationMode for a stream. If SdsInterpolationMode is set to Discrete , extrapolation will not occur. If SdsInterpolationMode is set to ContinuousNullableLeading or ContinuousNullableTrailing , default values are returned instead of actual data. Forward 2 Defines how a stream responds to requests with indexes that precede or follow all data in the stream. Behavior also depends on the SdsInterpolationMode for a stream. If SdsInterpolationMode is set to Discrete , extrapolation will not occur. If SdsInterpolationMode is set to ContinuousNullableLeading or ContinuousNullableTrailing , default values are returned instead of actual data. Backward 3 Defines how a stream responds to requests with indexes that precede or follow all data in the stream. Behavior also depends on the SdsInterpolationMode for a stream. If SdsInterpolationMode is set to Discrete , extrapolation will not occur. If SdsInterpolationMode is set to ContinuousNullableLeading or ContinuousNullableTrailing , default values are returned instead of actual data. SdsStreamPropertyOverride Properties Property Name Data Type Required Nullable Description SdsTypePropertyId string false true An unique identifier for the SdsTypeProperty object that needs to be overridden Uom string false true The Id , name, or abbreviation of the unit of measure to be applied to the SdsTypeProperty InterpolationMode SdsInterpolationMode false true Defines the InterpolationMode of the SdsTypeProperty { \"SdsTypePropertyId\": \"string\", \"Uom\": \"string\", \"InterpolationMode\": 0 } ErrorResponseBody Contains the error message format that follows the AVEVA Data Hub error standards Properties Property Name Data Type Required Nullable Description OperationId string false true Operation unique identifier of action that caused the error Error string false true Error description Reason string false true Reason for the error Resolution string false true Resolution to resolve the error Parameters object false true Ids or values that are creating or are affected by the error { \"OperationId\": \"string\", \"Error\": \"string\", \"Reason\": \"string\", \"Resolution\": \"string\", \"Parameters\": { \"property1\": \"string\", \"property2\": \"string\" } } SdsType A contract defining the type of data to read or write in a SdsStream Properties Property Name Data Type Required Nullable Description Id string false true A unique identifier for the SdsType object Name string false true An optional user-friendly name for the SdsType object Description string false true A brief description of the SdsType object SdsTypeCode SdsTypeCode false false The SdsTypeCode of the SdsType object IsGenericType boolean false false A boolean value indicating whether the current SdsType is a generic type This property is only used when using templates or generics. It will be automatically set if the SdsType is generated using SdsTypeBuilder . For further information on generics, please refer, https://docs.microsoft.com/en-us/dotnet/csharp/programming-guide/generics/index https:  docs.microsoft.com en-us dotnet csharp programming-guide generics index IsReferenceType boolean false false Enabling this property preserves objects as references during serialization/de-serialization serialization de-serialization of the SdsType data while using the SdsFormatter . This property behaves similar to the IsReference property for DataContractSerializer and is only valid for serialization if SdsFormatter is used. GenericArguments [ SdsType ] false true Contains the parameterized SdsTypes of the current generic SdsType . This property is only used when using templates or generics. It will be automatically set if the SdsType is generated using SdsTypeBuilder . For further information on generics, please refer to https://docs.microsoft.com/en-us/dotnet/csharp/programming-guide/generics/index https:  docs.microsoft.com en-us dotnet csharp programming-guide generics index Properties [ SdsTypeProperty ] false true List of SdsTypeProperty s of the SdsType BaseType SdsType false true Defines the base type of the SdsType . Commonly used by SdsTypeBuilder to generate an SdsType from contracts not explicitly defined and maintained by the user. DerivedTypes [ SdsType ] false true List of SdsTypes that should be recognized by SdsFormatter during serialization/de-serialization. serialization de-serialization. This property behaves similar to KnownTypeAttribute attribute for DataContractSerializer and only valid for serialization if SdsFormatter is used. InterpolationMode SdsInterpolationMode false false Defines the SdsInterpolationMode of the SdsType . This property is only valid for the root SdsType and invalid for SdsTypes of SdsTypeProperty s. ExtrapolationMode SdsExtrapolationMode false false Defines the SdsExtrapolationMode of the SdsType . This property is only valid for the root SdsType and invalid for SdsTypes of SdsTypeProperty s. { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": 0, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": 0, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": null, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": [ {} ], \"Properties\": [ {} ], \"BaseType\": null, \"DerivedTypes\": [ {} ], \"InterpolationMode\": null, \"ExtrapolationMode\": null } ], \"Properties\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"Order\": 0, \"IsKey\": true, \"FixedSize\": 0, \"SdsType\": null, \"Value\": null, \"Uom\": \"string\", \"InterpolationMode\": null, \"IsQuality\": true } ], \"BaseType\": { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": null, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": [ null ], \"Properties\": [ null ], \"BaseType\": null, \"DerivedTypes\": [ null ], \"InterpolationMode\": null, \"ExtrapolationMode\": null }, \"DerivedTypes\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": null, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": [ {} ], \"Properties\": [ {} ], \"BaseType\": null, \"DerivedTypes\": [ {} ], \"InterpolationMode\": null, \"ExtrapolationMode\": null } ], \"InterpolationMode\": 0, \"ExtrapolationMode\": 0 } ], \"Properties\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"Order\": 0, \"IsKey\": true, \"FixedSize\": 0, \"SdsType\": { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": null, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": [ null ], \"Properties\": [ null ], \"BaseType\": null, \"DerivedTypes\": [ null ], \"InterpolationMode\": null, \"ExtrapolationMode\": null }, \"Value\": null, \"Uom\": \"string\", \"InterpolationMode\": 0, \"IsQuality\": true } ], \"BaseType\": { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": 0, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": null, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": [ null ], \"Properties\": [ null ], \"BaseType\": null, \"DerivedTypes\": [ null ], \"InterpolationMode\": null, \"ExtrapolationMode\": null } ], \"Properties\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"Order\": 0, \"IsKey\": true, \"FixedSize\": 0, \"SdsType\": null, \"Value\": null, \"Uom\": \"string\", \"InterpolationMode\": null, \"IsQuality\": true } ], \"BaseType\": { \"Id\": null, \"Name\": null, \"Description\": null, \"SdsTypeCode\": null, \"IsGenericType\": null, \"IsReferenceType\": null, \"GenericArguments\": null, \"Properties\": null, \"BaseType\": null, \"DerivedTypes\": null, \"InterpolationMode\": null, \"ExtrapolationMode\": null }, \"DerivedTypes\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": null, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": [ null ], \"Properties\": [ null ], \"BaseType\": null, \"DerivedTypes\": [ null ], \"InterpolationMode\": null, \"ExtrapolationMode\": null } ], \"InterpolationMode\": 0, \"ExtrapolationMode\": 0 }, \"DerivedTypes\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": 0, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": null, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": [ {} ], \"Properties\": [ {} ], \"BaseType\": null, \"DerivedTypes\": [ {} ], \"InterpolationMode\": null, \"ExtrapolationMode\": null } ], \"Properties\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"Order\": 0, \"IsKey\": true, \"FixedSize\": 0, \"SdsType\": null, \"Value\": null, \"Uom\": \"string\", \"InterpolationMode\": null, \"IsQuality\": true } ], \"BaseType\": { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": null, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": [ null ], \"Properties\": [ null ], \"BaseType\": null, \"DerivedTypes\": [ null ], \"InterpolationMode\": null, \"ExtrapolationMode\": null }, \"DerivedTypes\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": null, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": [ {} ], \"Properties\": [ {} ], \"BaseType\": null, \"DerivedTypes\": [ {} ], \"InterpolationMode\": null, \"ExtrapolationMode\": null } ], \"InterpolationMode\": 0, \"ExtrapolationMode\": 0 } ], \"InterpolationMode\": 0, \"ExtrapolationMode\": 0 } SdsTypeCode Enumerated Values Property Value Empty 0 Object 1 Boolean 3 Char 4 SByte 5 Byte 6 Int16 7 UInt16 8 Int32 9 UInt32 10 Int64 11 UInt64 12 Single 13 Double 14 Decimal 15 DateTime 16 String 18 Guid 19 DateTimeOffset 20 TimeSpan 21 Version 22 NullableBoolean 103 NullableChar 104 NullableSByte 105 NullableByte 106 NullableInt16 107 NullableUInt16 108 NullableInt32 109 NullableUInt32 110 NullableInt64 111 NullableUInt64 112 NullableSingle 113 NullableDouble 114 NullableDecimal 115 NullableDateTime 116 NullableGuid 119 NullableDateTimeOffset 120 NullableTimeSpan 121 BooleanArray 203 CharArray 204 SByteArray 205 ByteArray 206 Int16Array 207 UInt16Array 208 Int32Array 209 UInt32Array 210 Int64Array 211 UInt64Array 212 SingleArray 213 DoubleArray 214 DecimalArray 215 DateTimeArray 216 StringArray 218 GuidArray 219 DateTimeOffsetArray 220 TimeSpanArray 221 VersionArray 222 Array 400 IList 401 IDictionary 402 IEnumerable 403 SdsType 501 SdsTypeProperty 502 SdsStreamView 503 SdsStreamViewProperty 504 SdsStreamViewMap 505 SdsStreamViewMapProperty 506 SdsStream 507 SdsStreamIndex 508 SdsTable 509 SdsColumn 510 SdsValues 511 SdsObject 512 SByteEnum 605 ByteEnum 606 Int16Enum 607 UInt16Enum 608 Int32Enum 609 UInt32Enum 610 Int64Enum 611 UInt64Enum 612 NullableSByteEnum 705 NullableByteEnum 706 NullableInt16Enum 707 NullableUInt16Enum 708 NullableInt32Enum 709 NullableUInt32Enum 710 NullableInt64Enum 711 NullableUInt64Enum 712 SdsTypeProperty A contract defining a property of a SdsType Properties Property Name Data Type Required Nullable Description Id string false true An unique identifier for the SdsTypeProperty object Name string false true An optional user-friendly name for the SdsTypeProperty object Description string false true A brief description of the SdsTypeProperty object Order int32 false false The order used for comparison among SdsTypeProperty s if a compound index is specified for SdsType IsKey boolean false false A boolean value indicating whether the current SdsTypeProperty must be used for indexing Used in combination with property to enable compound indexing FixedSize int32 false false An optional property specifying the length of string. SdsType SdsType false true SdsType of the current SdsTypeProperty Value any false true An enum value of the current SdsTypeProperty . Uom string false true Indicates the Unit of Measure of the current SdsTypeProperty InterpolationMode SdsInterpolationMode false true An InterpolationMode that overrides the root SdsType \u0027s InterpolationMode for this SdsTypeProperty IsQuality boolean false false Indicates whether this property marks data quality { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"Order\": 0, \"IsKey\": true, \"FixedSize\": 0, \"SdsType\": { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": 0, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": null, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": [ null ], \"Properties\": [ null ], \"BaseType\": null, \"DerivedTypes\": [ null ], \"InterpolationMode\": null, \"ExtrapolationMode\": null } ], \"Properties\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"Order\": 0, \"IsKey\": true, \"FixedSize\": 0, \"SdsType\": null, \"Value\": null, \"Uom\": \"string\", \"InterpolationMode\": null, \"IsQuality\": true } ], \"BaseType\": { \"Id\": null, \"Name\": null, \"Description\": null, \"SdsTypeCode\": null, \"IsGenericType\": null, \"IsReferenceType\": null, \"GenericArguments\": null, \"Properties\": null, \"BaseType\": null, \"DerivedTypes\": null, \"InterpolationMode\": null, \"ExtrapolationMode\": null }, \"DerivedTypes\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": null, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": [ null ], \"Properties\": [ null ], \"BaseType\": null, \"DerivedTypes\": [ null ], \"InterpolationMode\": null, \"ExtrapolationMode\": null } ], \"InterpolationMode\": 0, \"ExtrapolationMode\": 0 }, \"Value\": null, \"Uom\": \"string\", \"InterpolationMode\": 0, \"IsQuality\": true } SdsResolvedStream A contract defining resolution of SdsStream Properties Property Name Data Type Required Nullable Description Id string false true A unique identifier for the SdsStream object Name string false true An optional user-friendly name for the SdsStream object Description string false true A brief description of the SdsStream object TypeId string false true A unique identifier for the SdsType of the SdsStream object Indexes [ SdsStreamIndex ] false true List of SdsStreamIndex s to define secondary indexes for the SdsStream InterpolationMode SdsInterpolationMode false true Defines the SdsInterpolationMode of the SdsStream . Default is null . ExtrapolationMode SdsExtrapolationMode false true Defines the SdsExtrapolationMode of the SdsStream . Default is null . PropertyOverrides [ SdsStreamPropertyOverride ] false true List of SdsStreamPropertyOverride s to define unit of measure and interpolation mode overrides for the SdsStream Resolved boolean false false None Type SdsResolvedType false true A contract defining the type of data to read or write in a SdsResolvedStream { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"TypeId\": \"string\", \"Indexes\": [ { \"SdsTypePropertyId\": \"string\" } ], \"InterpolationMode\": 0, \"ExtrapolationMode\": 0, \"PropertyOverrides\": [ { \"SdsTypePropertyId\": \"string\", \"Uom\": \"string\", \"InterpolationMode\": 0 } ], \"Resolved\": true, \"Type\": { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": \"Empty\", \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": 0, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": \"[\u003cSdsType\u003e]\", \"Properties\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"Order\": 0, \"IsKey\": true, \"FixedSize\": 0, \"SdsType\": \"\u003cSdsType\u003e\", \"Value\": null, \"Uom\": \"string\", \"InterpolationMode\": 0, \"IsQuality\": true } ], \"BaseType\": \"\u003cSdsType\u003e\", \"DerivedTypes\": \"[\u003cSdsType\u003e]\", \"InterpolationMode\": 0, \"ExtrapolationMode\": 0 } ], \"Properties\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"Order\": 0, \"IsKey\": true, \"FixedSize\": 0, \"SdsType\": \"\u003cSdsResolvedType\u003e\", \"Value\": null, \"Uom\": \"string\", \"InterpolationMode\": \"Continuous\", \"IsQuality\": true } ], \"BaseType\": { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": 0, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": \"[\u003cSdsType\u003e]\", \"Properties\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"Order\": 0, \"IsKey\": true, \"FixedSize\": 0, \"SdsType\": \"\u003cSdsType\u003e\", \"Value\": null, \"Uom\": \"string\", \"InterpolationMode\": 0, \"IsQuality\": true } ], \"BaseType\": \"\u003cSdsType\u003e\", \"DerivedTypes\": \"[\u003cSdsType\u003e]\", \"InterpolationMode\": 0, \"ExtrapolationMode\": 0 }, \"DerivedTypes\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": 0, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": \"[\u003cSdsType\u003e]\", \"Properties\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"Order\": 0, \"IsKey\": true, \"FixedSize\": 0, \"SdsType\": \"\u003cSdsType\u003e\", \"Value\": null, \"Uom\": \"string\", \"InterpolationMode\": 0, \"IsQuality\": true } ], \"BaseType\": \"\u003cSdsType\u003e\", \"DerivedTypes\": \"[\u003cSdsType\u003e]\", \"InterpolationMode\": 0, \"ExtrapolationMode\": 0 } ], \"InterpolationMode\": \"Continuous\", \"ExtrapolationMode\": \"All\" } } SdsResolvedType A contract defining the type of data to read or write in a SdsResolvedStream Properties Property Name Data Type Required Nullable Description Id string false true A unique identifier for the SdsType object Name string false true An optional user-friendly name for the SdsType object Description string false true A brief description of the SdsType object SdsTypeCode SdsTypeCode2 false false None IsGenericType boolean false false A boolean value indicating whether the current SdsType is a generic type This property is only used when using templates or generics. It will be automatically set if the SdsType is generated using SdsTypeBuilder . For more information on generics, refer to Generic classes and methods IsReferenceType boolean false false Enabling this property preserves objects as references during serialization/de-serialization serialization de-serialization of the SdsType data while using the SdsFormatter This property behaves similar to IsReference property for DataContractSerializer and is only valid for serialization if SdsFormatter is used. GenericArguments [ SdsType ] false true Contains the parameterized SdsTypes of the current generic SdsType This property is only used when using templates or generics. It will be automatically set if the SdsType is generated using SdsTypeBuilder . For more information on generics, refer to Generic classes and methods Properties [ SdsResolvedTypeProperty ] false true [A contract defining the type of data to read or write in a SdsResolvedType ] BaseType SdsType false true Defines the base type of the SdsType Commonly used by SdsTypeBuilder to generate SdsType from contracts not explicitly defined and maintained by the user. DerivedTypes [ SdsType ] false true List of SdsTypes that should be recognized by SdsFormatter during serialization/de-serialization. serialization de-serialization. This property behaves similar to KnownTypeAttribute attribute for DataContractSerializer and only valid for serialization if SdsFormatter is used. InterpolationMode SdsInterpolationMode2 false false Interpolation modes that can be applied to SdsType , SdsTypeProperty , SdsStream , and SdsStreamPropertyOverride objects ExtrapolationMode SdsExtrapolationMode2 false false Defines how a stream responds to requests with indexes that precede or follow all data in the stream. Behavior also depends on the SdsInterpolationMode for a stream. If SdsInterpolationMode is set to Discrete , extrapolation will not occur. If SdsInterpolationMode is set to ContinuousNullableLeading or ContinuousNullableTrailing , default values will be returned instead of actual data. { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": \"Empty\", \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": 0, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": \"[\u003cSdsType\u003e]\", \"Properties\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"Order\": 0, \"IsKey\": true, \"FixedSize\": 0, \"SdsType\": \"\u003cSdsType\u003e\", \"Value\": null, \"Uom\": \"string\", \"InterpolationMode\": 0, \"IsQuality\": true } ], \"BaseType\": \"\u003cSdsType\u003e\", \"DerivedTypes\": \"[\u003cSdsType\u003e]\", \"InterpolationMode\": 0, \"ExtrapolationMode\": 0 } ], \"Properties\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"Order\": 0, \"IsKey\": true, \"FixedSize\": 0, \"SdsType\": \"\u003cSdsResolvedType\u003e\", \"Value\": null, \"Uom\": \"string\", \"InterpolationMode\": \"Continuous\", \"IsQuality\": true } ], \"BaseType\": { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": 0, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": \"[\u003cSdsType\u003e]\", \"Properties\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"Order\": 0, \"IsKey\": true, \"FixedSize\": 0, \"SdsType\": \"\u003cSdsType\u003e\", \"Value\": null, \"Uom\": \"string\", \"InterpolationMode\": 0, \"IsQuality\": true } ], \"BaseType\": \"\u003cSdsType\u003e\", \"DerivedTypes\": \"[\u003cSdsType\u003e]\", \"InterpolationMode\": 0, \"ExtrapolationMode\": 0 }, \"DerivedTypes\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": 0, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": \"[\u003cSdsType\u003e]\", \"Properties\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"Order\": 0, \"IsKey\": true, \"FixedSize\": 0, \"SdsType\": \"\u003cSdsType\u003e\", \"Value\": null, \"Uom\": \"string\", \"InterpolationMode\": 0, \"IsQuality\": true } ], \"BaseType\": \"\u003cSdsType\u003e\", \"DerivedTypes\": \"[\u003cSdsType\u003e]\", \"InterpolationMode\": 0, \"ExtrapolationMode\": 0 } ], \"InterpolationMode\": \"Continuous\", \"ExtrapolationMode\": \"All\" } SdsResolvedTypeProperty A contract defining the type of data to read or write in a SdsResolvedType Properties Property Name Data Type Required Nullable Description Id string false true A unique identifier for the SdsTypeProperty object Name string false true An optional user-friendly name for the SdsTypeProperty object Description string false true A brief description of the SdsTypeProperty object Order int32 false false The order used for comparison among SdsTypeProperty s if a compound index is specified for SdsType IsKey boolean false false A boolean value indicating whether the current SdsTypeProperty must be used for indexing. Used in combination with property to enable compound indexing. FixedSize int32 false false An optional property specifying the length of string. SdsType SdsResolvedType false true A contract defining the type of data to read or write in a SdsResolvedStream Value any false true An enum value of the current SdsTypeProperty. Uom string false true Indicates the Unit of Measure of the current SdsTypeProperty InterpolationMode SdsInterpolationMode2 false true Interpolation modes that can be applied to SdsType , SdsTypeProperty , SdsStream , and SdsStreamPropertyOverride objects IsQuality boolean false false Indicates whether this property marks data quality { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"Order\": 0, \"IsKey\": true, \"FixedSize\": 0, \"SdsType\": { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": \"Empty\", \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": 0, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": \"[\u003cSdsType\u003e]\", \"Properties\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"Order\": 0, \"IsKey\": true, \"FixedSize\": 0, \"SdsType\": \"\u003cSdsType\u003e\", \"Value\": null, \"Uom\": \"string\", \"InterpolationMode\": 0, \"IsQuality\": true } ], \"BaseType\": \"\u003cSdsType\u003e\", \"DerivedTypes\": \"[\u003cSdsType\u003e]\", \"InterpolationMode\": 0, \"ExtrapolationMode\": 0 } ], \"Properties\": \"[\u003cSdsResolvedTypeProperty\u003e]\", \"BaseType\": { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": 0, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": \"[\u003cSdsType\u003e]\", \"Properties\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"Order\": 0, \"IsKey\": true, \"FixedSize\": 0, \"SdsType\": \"\u003cSdsType\u003e\", \"Value\": null, \"Uom\": \"string\", \"InterpolationMode\": 0, \"IsQuality\": true } ], \"BaseType\": \"\u003cSdsType\u003e\", \"DerivedTypes\": \"[\u003cSdsType\u003e]\", \"InterpolationMode\": 0, \"ExtrapolationMode\": 0 }, \"DerivedTypes\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": 0, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": \"[\u003cSdsType\u003e]\", \"Properties\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"Order\": 0, \"IsKey\": true, \"FixedSize\": 0, \"SdsType\": \"\u003cSdsType\u003e\", \"Value\": null, \"Uom\": \"string\", \"InterpolationMode\": 0, \"IsQuality\": true } ], \"BaseType\": \"\u003cSdsType\u003e\", \"DerivedTypes\": \"[\u003cSdsType\u003e]\", \"InterpolationMode\": 0, \"ExtrapolationMode\": 0 } ], \"InterpolationMode\": \"Continuous\", \"ExtrapolationMode\": \"All\" }, \"Value\": null, \"Uom\": \"string\", \"InterpolationMode\": \"Continuous\", \"IsQuality\": true } SdsInterpolationMode2 Interpolation modes that can be applied to SdsType , SdsTypeProperty , SdsStream , and SdsStreamPropertyOverride objects Enumerated Values Property Value Default Continuous Continuous Continuous StepwiseContinuousLeading StepwiseContinuousLeading StepwiseContinuousTrailing StepwiseContinuousTrailing Discrete Discrete ContinuousNullableLeading ContinuousNullableLeading ContinuousNullableTrailing ContinuousNullableTrailing SdsTypeCode2 Enumerated Values Property Value Empty Empty Object Object Boolean Boolean Char Char SByte SByte Byte Byte Int16 Int16 UInt16 UInt16 Int32 Int32 UInt32 UInt32 Int64 Int64 UInt64 UInt64 Single Single Double Double Decimal Decimal DateTime DateTime String String Guid Guid DateTimeOffset DateTimeOffset TimeSpan TimeSpan Version Version NullableBoolean NullableBoolean NullableChar NullableChar NullableSByte NullableSByte NullableByte NullableByte NullableInt16 NullableInt16 NullableUInt16 NullableUInt16 NullableInt32 NullableInt32 NullableUInt32 NullableUInt32 NullableInt64 NullableInt64 NullableUInt64 NullableUInt64 NullableSingle NullableSingle NullableDouble NullableDouble NullableDecimal NullableDecimal NullableDateTime NullableDateTime NullableGuid NullableGuid NullableDateTimeOffset NullableDateTimeOffset NullableTimeSpan NullableTimeSpan BooleanArray BooleanArray CharArray CharArray SByteArray SByteArray ByteArray ByteArray Int16Array Int16Array UInt16Array UInt16Array Int32Array Int32Array UInt32Array UInt32Array Int64Array Int64Array UInt64Array UInt64Array SingleArray SingleArray DoubleArray DoubleArray DecimalArray DecimalArray DateTimeArray DateTimeArray StringArray StringArray GuidArray GuidArray DateTimeOffsetArray DateTimeOffsetArray TimeSpanArray TimeSpanArray VersionArray VersionArray Array Array IList IList IDictionary IDictionary IEnumerable IEnumerable SdsType SdsType SdsTypeProperty SdsTypeProperty SdsStreamView SdsStreamView SdsStreamViewProperty SdsStreamViewProperty SdsStreamViewMap SdsStreamViewMap SdsStreamViewMapProperty SdsStreamViewMapProperty SdsStream SdsStream SdsStreamIndex SdsStreamIndex SdsTable SdsTable SdsColumn SdsColumn SdsValues SdsValues SdsObject SdsObject SByteEnum SByteEnum ByteEnum ByteEnum Int16Enum Int16Enum UInt16Enum UInt16Enum Int32Enum Int32Enum UInt32Enum UInt32Enum Int64Enum Int64Enum UInt64Enum UInt64Enum NullableSByteEnum NullableSByteEnum NullableByteEnum NullableByteEnum NullableInt16Enum NullableInt16Enum NullableUInt16Enum NullableUInt16Enum NullableInt32Enum NullableInt32Enum NullableUInt32Enum NullableUInt32Enum NullableInt64Enum NullableInt64Enum NullableUInt64Enum NullableUInt64Enum SdsExtrapolationMode2 Defines how a stream responds to requests with indexes that precede or follow all data in the stream. Behavior also depends on the SdsInterpolationMode for a stream. If SdsInterpolationMode is set to Discrete , extrapolation will not occur. If SdsInterpolationMode is set to ContinuousNullableLeading or ContinuousNullableTrailing , default values will be returned instead of actual data. Enumerated Values Property Value All All None None Forward Forward Backward Backward SdsResolvedStreamsResponse A contract defining bulk response of SdsResolvedStream Properties Property Name Data Type Required Nullable Description Data [ SdsResolvedStream ] false true [A contract defining resolution of SdsStream ] ChildErrors [ SdsResolvedStreamErrorResponseBody ] false true [The error response contains details on the cause of stream resolution failure and resolution of the error.] { \"Data\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"TypeId\": \"string\", \"Indexes\": [ { \"SdsTypePropertyId\": \"string\" } ], \"InterpolationMode\": 0, \"ExtrapolationMode\": 0, \"PropertyOverrides\": [ { \"SdsTypePropertyId\": \"string\", \"Uom\": \"string\", \"InterpolationMode\": 0 } ], \"Resolved\": true, \"Type\": { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": \"Empty\", \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": 0, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": \"[\u003cSdsType\u003e]\", \"Properties\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"Order\": 0, \"IsKey\": true, \"FixedSize\": 0, \"SdsType\": \"\u003cSdsType\u003e\", \"Value\": null, \"Uom\": \"string\", \"InterpolationMode\": \"[\", \"IsQuality\": true } ], \"BaseType\": \"\u003cSdsType\u003e\", \"DerivedTypes\": \"[\u003cSdsType\u003e]\", \"InterpolationMode\": 0, \"ExtrapolationMode\": 0 } ], \"Properties\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"Order\": 0, \"IsKey\": true, \"FixedSize\": 0, \"SdsType\": \"\u003cSdsResolvedType\u003e\", \"Value\": null, \"Uom\": \"string\", \"InterpolationMode\": \"Continuous\", \"IsQuality\": true } ], \"BaseType\": { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": 0, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": \"[\u003cSdsType\u003e]\", \"Properties\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"Order\": 0, \"IsKey\": true, \"FixedSize\": 0, \"SdsType\": null, \"Value\": null, \"Uom\": \"string\", \"InterpolationMode\": null, \"IsQuality\": true } ], \"BaseType\": \"\u003cSdsType\u003e\", \"DerivedTypes\": \"[\u003cSdsType\u003e]\", \"InterpolationMode\": 0, \"ExtrapolationMode\": 0 }, \"DerivedTypes\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": 0, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": \"[\u003cSdsType\u003e]\", \"Properties\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"Order\": 0, \"IsKey\": true, \"FixedSize\": 0, \"SdsType\": \"\u003cSdsType\u003e\", \"Value\": null, \"Uom\": \"string\", \"InterpolationMode\": \"[\", \"IsQuality\": true } ], \"BaseType\": \"\u003cSdsType\u003e\", \"DerivedTypes\": \"[\u003cSdsType\u003e]\", \"InterpolationMode\": 0, \"ExtrapolationMode\": 0 } ], \"InterpolationMode\": \"Continuous\", \"ExtrapolationMode\": \"All\" } } ], \"ChildErrors\": [ { \"OperationId\": \"string\", \"Error\": \"string\", \"Reason\": \"string\", \"Resolution\": \"string\", \"Parameters\": { \"property1\": \"string\", \"property2\": \"string\" }, \"StreamId\": \"string\", \"StatusCode\": 100 } ] } SdsResolvedStreamErrorResponseBody The error response contains details on the cause of stream resolution failure and resolution of the error. Properties Property Name Data Type Required Nullable Description OperationId string false true Operation unique identifier of action that caused the error Error string false true Error description Reason string false true Reason for the error Resolution string false true Resolution to resolve the error Parameters object false true Id s or values that are creating or are affected by the error StreamId string false true None StatusCode HttpStatusCode false false None { \"OperationId\": \"string\", \"Error\": \"string\", \"Reason\": \"string\", \"Resolution\": \"string\", \"Parameters\": { \"property1\": \"string\", \"property2\": \"string\" }, \"StreamId\": \"string\", \"StatusCode\": 100 } List Summaries Returns summary intervals between a specified start and end index. Index types that cannot be interpolated do not support summary requests. Strings are an example of indexes that cannot be interpolated. Summaries are not supported for streams with compound indexes. Interpolating between two indexes that consist of multiple properties is not defined and results in non-determinant behavior. Request GET /api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Streams/{streamId}/Data/Summaries  api v1 Tenants {tenantId} Namespaces {namespaceId} Streams {streamId} Data Summaries Parameters string tenantId Tenant identifier. string namespaceId Namespace identifier. string streamId Stream identifier. Response Status Code Body Type Description 200 Inline Returns a serialized collection of SdsIntervals. Each SdsInterval has a start, end, and collection of summary values. 400 ErrorResponseBody Missing or invalid inputs 401 ErrorResponseBody Unauthorized 403 ErrorResponseBody Forbidden 404 ErrorResponseBody One of the resources specified was not found 500 ErrorResponseBody An error occurred while processing the request 503 ErrorResponseBody Service Unavailable Example response body 400 Response ( ErrorResponseBody ) { \"OperationId\": \"string\", \"Error\": \"string\", \"Reason\": \"string\", \"Resolution\": \"string\", \"Parameters\": { \"property1\": \"string\", \"property2\": \"string\" } } List Sampled Values Returns representative data sampled by intervals between a specified start and end index. Sampling is driven by a specified property or properties of the stream\u0027s Sds Type. Property types that cannot be interpolated do not support sampling requests. Strings are an example of a property that cannot be interpolated. Request GET /api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Streams/{streamId}/Data/Sampled  api v1 Tenants {tenantId} Namespaces {namespaceId} Streams {streamId} Data Sampled Parameters string tenantId Tenant identifier. string namespaceId Namespace identifier. string streamId Stream identifier. Response Status Code Body Type Description 200 Inline Returns a serialized collection of events 400 ErrorResponseBody Missing or invalid inputs 401 ErrorResponseBody Unauthorized 403 ErrorResponseBody Forbidden 404 ErrorResponseBody One of the resources specified was not found 500 ErrorResponseBody An error occurred while processing the request 503 ErrorResponseBody Service Unavailable Example response body 400 Response ( ErrorResponseBody ) { \"OperationId\": \"string\", \"Error\": \"string\", \"Reason\": \"string\", \"Resolution\": \"string\", \"Parameters\": { \"property1\": \"string\", \"property2\": \"string\" } } Properties Property Name Data Type Required Nullable Description OperationId string false true Operation unique identifier of action that caused the error Error string false true Error description Reason string false true Reason for the error Resolution string false true Resolution to resolve the error Parameters object false true IDs or values that are creating or are affected by the error { \"OperationId\": \"string\", \"Error\": \"string\", \"Reason\": \"string\", \"Resolution\": \"string\", \"Parameters\": { \"property1\": \"string\", \"property2\": \"string\" } } HttpStatusCode Enumerated Values Property Value Continue 100 SwitchingProtocols 101 Processing 102 EarlyHints 103 OK 200 Created 201 Accepted 202 NonAuthoritativeInformation 203 NoContent 204 ResetContent 205 PartialContent 206 MultiStatus 207 AlreadyReported 208 IMUsed 226 MultipleChoices 300 Ambiguous 300 MovedPermanently 301 Moved 301 Found 302 Redirect 302 SeeOther 303 RedirectMethod 303 NotModified 304 UseProxy 305 Unused 306 TemporaryRedirect 307 RedirectKeepVerb 307 PermanentRedirect 308 BadRequest 400 Unauthorized 401 PaymentRequired 402 Forbidden 403 NotFound 404 MethodNotAllowed 405 NotAcceptable 406 ProxyAuthenticationRequired 407 RequestTimeout 408 Conflict 409 Gone 410 LengthRequired 411 PreconditionFailed 412 RequestEntityTooLarge 413 RequestUriTooLong 414 UnsupportedMediaType 415 RequestedRangeNotSatisfiable 416 ExpectationFailed 417 MisdirectedRequest 421 UnprocessableEntity 422 Locked 423 FailedDependency 424 UpgradeRequired 426 PreconditionRequired 428 TooManyRequests 429 RequestHeaderFieldsTooLarge 431 UnavailableForLegalReasons 451 InternalServerError 500 NotImplemented 501 BadGateway 502 ServiceUnavailable 503 GatewayTimeout 504 HttpVersionNotSupported 505 VariantAlsoNegotiates 506 InsufficientStorage 507 LoopDetected 508 NotExtended 510 NetworkAuthenticationRequired 511"
                                                 },
    "content/sds/types/sds-type-api.html":  {
                                                "href":  "content/sds/types/sds-type-api.html",
                                                "title":  "SdsType API",
                                                "keywords":  "SdsType API The REST APIs provide programmatic access to read and write SDS data. The following APIs interact with SdsTypes . For general SdsType information, see Types . List Types Gets a list of SdsType objects. If you do not set the optional parameters, this call returns up to the first 100 SdsTypes . Request GET /api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Types?query={query}\u0026skip={skip}\u0026count={count}\u0026orderby={orderby}  api v1 Tenants {tenantId} Namespaces {namespaceId} Types?query={query}\u0026skip={skip}\u0026count={count}\u0026orderby={orderby} Parameters string tenantId Tenant identifier string namespaceId Namespace identifier [Optional] string query Query identifier. See Search in SDS for information about specifying the search parameter. [Optional] int skip Parameter representing the zero-based offset of the first object to retrieve. If unspecified, a default value of 0 is used. [Optional] int count Parameter representing the maximum number of objects to retrieve. If unspecified, a default value of 100 is used. [Optional] string orderby Parameter representing sorted order. A field name is required. The sorting is based on the stored values for a given field (of type string ). For example, orderby=name would sort the returned results by the name values (ascending by default). Additionally, a value can be provided along with the field name to identify whether to sort ascending or descending, by using values asc or desc , respectively. For example, orderby=name desc would sort the returned results by the name values, descending. If you do not specify a value, the results are not sorted. Response Status Code Body Type Description 200 SdsType [] Returns a list of SdsType objects 401 ErrorResponseBody Unauthorized 403 ErrorResponseBody Forbidden 404 ErrorResponseBody One of the resources specified was not found. 500 ErrorResponseBody An error occurred while processing the request. 503 ErrorResponseBody Service Unavailable Example response body 200 Response HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Id\": \"Simple\", \"Name\": \"Simple\", \"SdsTypeCode\": 1, \"Properties\": [ { \"Id\": \"Time\", \"Name\": \"Time\", \"IsKey\": true, \"SdsType\": { \"Id\": \"19a87a76-614a-385b-ba48-6f8b30ff6ab2\", \"Name\": \"DateTime\", \"SdsTypeCode\": 16 } }, { \"Id\": \"State\", \"Name\": \"State\", \"SdsType\": { \"Id\": \"e20bdd7e-590b-3372-ab39-ff61950fb4f3\", \"Name\": \"State\", \"SdsTypeCode\": 609, \"Properties\": [ { \"Id\": \"Ok\", \"Value\": 0 }, { \"Id\": \"Warning\", \"Value\": 1 }, { \"Id\": \"Alarm\", \"Value\": 2 } ] } }, { \"Id\": \"Measurement\", \"Name\": \"Measurement\", \"SdsType\": { \"Id\": \"6fecef77-20b1-37ae-aa3b-e6bb838d5a86\", \"Name\": \"Double\", \"SdsTypeCode\": 14 } } ] }, ] 401 Response ( ErrorResponseBody ) { \"OperationId\": \"string\", \"Error\": \"string\", \"Reason\": \"string\", \"Resolution\": \"string\", \"Parameters\": { \"property1\": \"string\", \"property2\": \"string\" } } Get Type Gets the specified SdsType Request GET /api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Types/{typeId}  api v1 Tenants {tenantId} Namespaces {namespaceId} Types {typeId} Parameters string tenantId Tenant identifier string namespaceId Namespace identifier string typeId Type identifier Response Status Code Body Type Description 200 SdsType Returns the SdsType 400 ErrorResponseBody Missing or invalid inputs 401 ErrorResponseBody Unauthorized 403 ErrorResponseBody Forbidden 404 ErrorResponseBody One of the resources specified was not found. 500 ErrorResponseBody An error occurred while processing the request. 503 ErrorResponseBody Service Unavailable Example response body 200 Response HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json { \"Id\":\"Simple\", \"Name\":\"Simple\", \"SdsTypeCode\":1, \"Properties\":[ { \"Id\":\"Time\", \"Name\":\"Time\", \"IsKey\":true, \"SdsType\":{ \"Id\":\"19a87a76-614a-385b-ba48-6f8b30ff6ab2\", \"Name\":\"DateTime\", \"SdsTypeCode\":16 } }, { \"Id\":\"State\", \"Name\":\"State\", \"SdsType\":{ \"Id\":\"e20bdd7e-590b-3372-ab39-ff61950fb4f3\", \"Name\":\"State\", \"SdsTypeCode\":609, \"Properties\":[ { \"Id\":\"Ok\", \"Value\":0 }, { \"Id\":\"Warning\", \"Value\":1 }, { \"Id\":\"Alarm\", \"Value\":2 } ] } }, { \"Id\":\"Measurement\", \"Name\":\"Measurement\", \"SdsType\":{ \"Id\":\"6fecef77-20b1-37ae-aa3b-e6bb838d5a86\", \"Name\":\"Double\", \"SdsTypeCode\":14 } } ] } 400 Response ( ErrorResponseBody ) { \"OperationId\": \"string\", \"Error\": \"string\", \"Reason\": \"string\", \"Resolution\": \"string\", \"Parameters\": { \"property1\": \"string\", \"property2\": \"string\" } } Get Or Create Type Creates the specified type. If a type with a matching identifier already exists, SDS compares the existing type with the type that was sent. Request POST /api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Types/{typeId}  api v1 Tenants {tenantId} Namespaces {namespaceId} Types {typeId} Parameters string tenantId Tenant identifier string namespaceId Namespace identifier string typeId Type identifier Response Status Code Body Type Description 200 SdsType Returns the SdsType 201 SdsType Returns the SdsType 302 None The SdsType already exists 400 ErrorResponseBody Missing or invalid inputs 401 ErrorResponseBody Unauthorized 403 ErrorResponseBody Forbidden 404 ErrorResponseBody One of the resources specified was not found. 409 ErrorResponseBody Conflict 500 ErrorResponseBody An error occurred while processing the request. 503 ErrorResponseBody Service Unavailable Example response body 200 Response ( SdsType ) { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": 0, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": 0, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": null, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": [ {} ], \"Properties\": [ {} ], \"BaseType\": null, \"DerivedTypes\": [ {} ], \"InterpolationMode\": null, \"ExtrapolationMode\": null } ], \"Properties\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"Order\": 0, \"IsKey\": true, \"FixedSize\": 0, \"SdsType\": null, \"Value\": null, \"Uom\": \"string\", \"InterpolationMode\": null, \"IsQuality\": true } ], \"BaseType\": { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": null, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": [ null ], \"Properties\": [ null ], \"BaseType\": null, \"DerivedTypes\": [ null ], \"InterpolationMode\": null, \"ExtrapolationMode\": null }, \"DerivedTypes\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": null, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": [ {} ], \"Properties\": [ {} ], \"BaseType\": null, \"DerivedTypes\": [ {} ], \"InterpolationMode\": null, \"ExtrapolationMode\": null } ], \"InterpolationMode\": 0, \"ExtrapolationMode\": 0 } ], \"Properties\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"Order\": 0, \"IsKey\": true, \"FixedSize\": 0, \"SdsType\": { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": null, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": [ null ], \"Properties\": [ null ], \"BaseType\": null, \"DerivedTypes\": [ null ], \"InterpolationMode\": null, \"ExtrapolationMode\": null }, \"Value\": null, \"Uom\": \"string\", \"InterpolationMode\": 0, \"IsQuality\": true } ], \"BaseType\": { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": 0, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": null, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": [ null ], \"Properties\": [ null ], \"BaseType\": null, \"DerivedTypes\": [ null ], \"InterpolationMode\": null, \"ExtrapolationMode\": null } ], \"Properties\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"Order\": 0, \"IsKey\": true, \"FixedSize\": 0, \"SdsType\": null, \"Value\": null, \"Uom\": \"string\", \"InterpolationMode\": null, \"IsQuality\": true } ], \"BaseType\": { \"Id\": null, \"Name\": null, \"Description\": null, \"SdsTypeCode\": null, \"IsGenericType\": null, \"IsReferenceType\": null, \"GenericArguments\": null, \"Properties\": null, \"BaseType\": null, \"DerivedTypes\": null, \"InterpolationMode\": null, \"ExtrapolationMode\": null }, \"DerivedTypes\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": null, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": [ null ], \"Properties\": [ null ], \"BaseType\": null, \"DerivedTypes\": [ null ], \"InterpolationMode\": null, \"ExtrapolationMode\": null } ], \"InterpolationMode\": 0, \"ExtrapolationMode\": 0 }, \"DerivedTypes\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": 0, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": null, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": [ {} ], \"Properties\": [ {} ], \"BaseType\": null, \"DerivedTypes\": [ {} ], \"InterpolationMode\": null, \"ExtrapolationMode\": null } ], \"Properties\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"Order\": 0, \"IsKey\": true, \"FixedSize\": 0, \"SdsType\": null, \"Value\": null, \"Uom\": \"string\", \"InterpolationMode\": null, \"IsQuality\": true } ], \"BaseType\": { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": null, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": [ null ], \"Properties\": [ null ], \"BaseType\": null, \"DerivedTypes\": [ null ], \"InterpolationMode\": null, \"ExtrapolationMode\": null }, \"DerivedTypes\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": null, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": [ {} ], \"Properties\": [ {} ], \"BaseType\": null, \"DerivedTypes\": [ {} ], \"InterpolationMode\": null, \"ExtrapolationMode\": null } ], \"InterpolationMode\": 0, \"ExtrapolationMode\": 0 } ], \"InterpolationMode\": 0, \"ExtrapolationMode\": 0 } 201 Response HTTP/1.1 HTTP 1.1 201 Content-Type: application/json application json { \"Id\": \"Simple\", \"Name\": \"Simple\", \"Description\": null, \"SdsTypeCode\": 1, \"IsGenericType\": false, \"IsReferenceType\": false, \"GenericArguments\": null, \"Properties\": [ { \"Id\": \"Time\", \"Name\": \"Time\", \"Description\": null, \"Order\": 0, \"IsKey\": true, \"FixedSize\": 0, \"SdsType\": { \"Id\": \"19a87a76-614a-385b-ba48-6f8b30ff6ab2\", \"Name\": \"DateTime\", \"Description\": null, \"SdsTypeCode\": 16, \"IsGenericType\": false, \"IsReferenceType\": false, \"GenericArguments\": null, \"Properties\": null, \"BaseType\": null, \"DerivedTypes\": null, \"InterpolationMode\": 0, \"ExtrapolationMode\": 0 }, \"Value\": null, \"Uom\": null, \"InterpolationMode\": null }, { \"Id\": \"State\", \"Name\": \"State\", \"Description\": null, \"Order\": 0, \"IsKey\": false, \"FixedSize\": 0, \"SdsType\": { \"Id\": \"e20bdd7e-590b-3372-ab39-ff61950fb4f3\", \"Name\": \"State\", \"Description\": null, \"SdsTypeCode\": 609, \"IsGenericType\": false, \"IsReferenceType\": false, \"GenericArguments\": null, \"Properties\": [ { \"Id\": \"Ok\", \"Name\": null, \"Description\": null, \"Order\": 0, \"IsKey\": false, \"FixedSize\": 0, \"SdsType\": null, \"Value\": 0, \"Uom\": null, \"InterpolationMode\": null }, { \"Id\": \"Warning\", \"Name\": null, \"Description\": null, \"Order\": 0, \"IsKey\": false, \"FixedSize\": 0, \"SdsType\": null, \"Value\": 1, \"Uom\": null, \"InterpolationMode\": null }, { \"Id\": \"Alarm\", \"Name\": null, \"Description\": null, \"Order\": 0, \"IsKey\": false, \"FixedSize\": 0, \"SdsType\": null, \"Value\": 2, \"Uom\": null, \"InterpolationMode\": null } ], \"BaseType\": null, \"DerivedTypes\": null, \"InterpolationMode\": 0, \"ExtrapolationMode\": 0 }, \"Value\": null, \"Uom\": null, \"InterpolationMode\": null }, { \"Id\": \"Measurement\", \"Name\": \"Measurement\", \"Description\": null, \"Order\": 0, \"IsKey\": false, \"FixedSize\": 0, \"SdsType\": { \"Id\": \"6fecef77-20b1-37ae-aa3b-e6bb838d5a86\", \"Name\": \"Double\", \"Description\": null, \"SdsTypeCode\": 14, \"IsGenericType\": false, \"IsReferenceType\": false, \"GenericArguments\": null, \"Properties\": null, \"BaseType\": null, \"DerivedTypes\": null, \"InterpolationMode\": 0, \"ExtrapolationMode\": 0 }, \"Value\": null, \"Uom\": null, \"InterpolationMode\": null } ], \"BaseType\": null, \"DerivedTypes\": null, \"InterpolationMode\": 0, \"ExtrapolationMode\": 0 } Delete Type Deletes a type from the specified tenant and namespace. Note that a type cannot be deleted if any streams, stream views, or other types reference it. Request DELETE /api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Types/{typeId}  api v1 Tenants {tenantId} Namespaces {namespaceId} Types {typeId} Parameters string tenantId Tenant identifier string namespaceId Namespace identifier string typeId Type identifier Response Status Code Body Type Description 204 None The SdsType was successfully deleted 400 ErrorResponseBody Missing or invalid inputs 401 ErrorResponseBody Unauthorized 403 ErrorResponseBody Forbidden 404 ErrorResponseBody One of the resources specified was not found. 409 ErrorResponseBody Conflict 500 ErrorResponseBody An error occurred while processing the request. 503 ErrorResponseBody Service Unavailable Example response body 400 Response ( ErrorResponseBody ) { \"OperationId\": \"string\", \"Error\": \"string\", \"Reason\": \"string\", \"Resolution\": \"string\", \"Parameters\": { \"property1\": \"string\", \"property2\": \"string\" } } Get Type Reference Count Returns a dictionary mapping the object name to the number of references held by streams, stream views and parent types for the specified type. Request GET /api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Types/{typeId}/ReferenceCount  api v1 Tenants {tenantId} Namespaces {namespaceId} Types {typeId} ReferenceCount Parameters string tenantId Tenant identifier string namespaceId Namespace identifier string typeId Type identifier Response Status Code Body Type Description 200 Inline Returns a dictionary mapping with object name to number of references 400 ErrorResponseBody Missing or invalid inputs 401 ErrorResponseBody Unauthorized 403 ErrorResponseBody Forbidden 404 ErrorResponseBody One of the resources specified was not found. 500 ErrorResponseBody An error occurred while processing the request. 503 ErrorResponseBody Service Unavailable Example response body 200 Response { \"SdsStream\":3, \"SdsStreamView\":2, \"SdsType\":1 } 400 Response ( ErrorResponseBody ) { \"OperationId\": \"string\", \"Error\": \"string\", \"Reason\": \"string\", \"Resolution\": \"string\", \"Parameters\": { \"property1\": \"string\", \"property2\": \"string\" } } Definitions SdsType A contract defining the type of data to read or write in a SdsStream Properties Property Name Data Type Required Nullable Description Id string false true A unique identifier for the SdsType object Name string false true An optional user-friendly name for the SdsType object Description string false true A brief description of the SdsType object SdsTypeCode SdsTypeCode false false The SdsTypeCode of the SdsType object IsGenericType boolean false false A boolean value indicating whether the current SdsType is a generic type. This property is only used when using templates or generics. It is automatically set if the SdsType is generated using SdsTypeBuilder . For further information on generics, refer to Generic classes and methods . IsReferenceType boolean false false Enabling this property preserves objects as references during serialization/de-serialization serialization de-serialization of the SdsType data while using the SdsFormatter . This property behaves similar to IsReference property for DataContractSerializer and is only valid for serialization if SdsFormatter is used. GenericArguments [SdsType] false true Contains the parameterized SdsTypes of the current generic SdsType . This property is only used when using templates or generics. It is automatically set if the SdsType is generated using SdsTypeBuilder . For further information on generics,refer to Generic classes and methods . Properties [ SdsTypeProperty ] false true List of SdsTypeProperty s of the SdsType BaseType SdsType false true Defines the base type of the SdsType . Commonly used by SdsTypeBuilder to generate SdsType from contracts not explicitly defined and maintained by the user. DerivedTypes [SdsType] false true List of SdsTypes that should be recognized by SdsFormatter during serialization/de-serialization. serialization de-serialization. This property behaves similar to KnownTypeAttribute attribute for DataContractSerializer and only valid for serialization if SdsFormatter is used. InterpolationMode SdsInterpolationMode false false Defines the SdsInterpolationMode of the SdsType . This property is only valid for the root SdsType and invalid for SdsTypes of SdsTypeProperty s. ExtrapolationMode SdsExtrapolationMode false false Defines the SdsExtrapolationMode of the SdsType . This property is only valid for the root SdsType and invalid for SdsTypes of SdsTypeProperty s. { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": 0, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": 0, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": null, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": [ {} ], \"Properties\": [ {} ], \"BaseType\": null, \"DerivedTypes\": [ {} ], \"InterpolationMode\": null, \"ExtrapolationMode\": null } ], \"Properties\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"Order\": 0, \"IsKey\": true, \"FixedSize\": 0, \"SdsType\": null, \"Value\": null, \"Uom\": \"string\", \"InterpolationMode\": null, \"IsQuality\": true } ], \"BaseType\": { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": null, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": [ null ], \"Properties\": [ null ], \"BaseType\": null, \"DerivedTypes\": [ null ], \"InterpolationMode\": null, \"ExtrapolationMode\": null }, \"DerivedTypes\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": null, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": [ {} ], \"Properties\": [ {} ], \"BaseType\": null, \"DerivedTypes\": [ {} ], \"InterpolationMode\": null, \"ExtrapolationMode\": null } ], \"InterpolationMode\": 0, \"ExtrapolationMode\": 0 } ], \"Properties\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"Order\": 0, \"IsKey\": true, \"FixedSize\": 0, \"SdsType\": { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": null, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": [ null ], \"Properties\": [ null ], \"BaseType\": null, \"DerivedTypes\": [ null ], \"InterpolationMode\": null, \"ExtrapolationMode\": null }, \"Value\": null, \"Uom\": \"string\", \"InterpolationMode\": 0, \"IsQuality\": true } ], \"BaseType\": { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": 0, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": null, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": [ null ], \"Properties\": [ null ], \"BaseType\": null, \"DerivedTypes\": [ null ], \"InterpolationMode\": null, \"ExtrapolationMode\": null } ], \"Properties\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"Order\": 0, \"IsKey\": true, \"FixedSize\": 0, \"SdsType\": null, \"Value\": null, \"Uom\": \"string\", \"InterpolationMode\": null, \"IsQuality\": true } ], \"BaseType\": { \"Id\": null, \"Name\": null, \"Description\": null, \"SdsTypeCode\": null, \"IsGenericType\": null, \"IsReferenceType\": null, \"GenericArguments\": null, \"Properties\": null, \"BaseType\": null, \"DerivedTypes\": null, \"InterpolationMode\": null, \"ExtrapolationMode\": null }, \"DerivedTypes\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": null, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": [ null ], \"Properties\": [ null ], \"BaseType\": null, \"DerivedTypes\": [ null ], \"InterpolationMode\": null, \"ExtrapolationMode\": null } ], \"InterpolationMode\": 0, \"ExtrapolationMode\": 0 }, \"DerivedTypes\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": 0, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": null, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": [ {} ], \"Properties\": [ {} ], \"BaseType\": null, \"DerivedTypes\": [ {} ], \"InterpolationMode\": null, \"ExtrapolationMode\": null } ], \"Properties\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"Order\": 0, \"IsKey\": true, \"FixedSize\": 0, \"SdsType\": null, \"Value\": null, \"Uom\": \"string\", \"InterpolationMode\": null, \"IsQuality\": true } ], \"BaseType\": { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": null, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": [ null ], \"Properties\": [ null ], \"BaseType\": null, \"DerivedTypes\": [ null ], \"InterpolationMode\": null, \"ExtrapolationMode\": null }, \"DerivedTypes\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": null, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": [ {} ], \"Properties\": [ {} ], \"BaseType\": null, \"DerivedTypes\": [ {} ], \"InterpolationMode\": null, \"ExtrapolationMode\": null } ], \"InterpolationMode\": 0, \"ExtrapolationMode\": 0 } ], \"InterpolationMode\": 0, \"ExtrapolationMode\": 0 } SdsTypeCode Enumerated Values Property Value Empty 0 Object 1 Boolean 3 Char 4 SByte 5 Byte 6 Int16 7 UInt16 8 Int32 9 UInt32 10 Int64 11 UInt64 12 Single 13 Double 14 Decimal 15 DateTime 16 String 18 Guid 19 DateTimeOffset 20 TimeSpan 21 Version 22 NullableBoolean 103 NullableChar 104 NullableSByte 105 NullableByte 106 NullableInt16 107 NullableUInt16 108 NullableInt32 109 NullableUInt32 110 NullableInt64 111 NullableUInt64 112 NullableSingle 113 NullableDouble 114 NullableDecimal 115 NullableDateTime 116 NullableGuid 119 NullableDateTimeOffset 120 NullableTimeSpan 121 BooleanArray 203 CharArray 204 SByteArray 205 ByteArray 206 Int16Array 207 UInt16Array 208 Int32Array 209 UInt32Array 210 Int64Array 211 UInt64Array 212 SingleArray 213 DoubleArray 214 DecimalArray 215 DateTimeArray 216 StringArray 218 GuidArray 219 DateTimeOffsetArray 220 TimeSpanArray 221 VersionArray 222 Array 400 IList 401 IDictionary 402 IEnumerable 403 SdsType 501 SdsTypeProperty 502 SdsStreamView 503 SdsStreamViewProperty 504 SdsStreamViewMap 505 SdsStreamViewMapProperty 506 SdsStream 507 SdsStreamIndex 508 SdsTable 509 SdsColumn 510 SdsValues 511 SdsObject 512 SByteEnum 605 ByteEnum 606 Int16Enum 607 UInt16Enum 608 Int32Enum 609 UInt32Enum 610 Int64Enum 611 UInt64Enum 612 NullableSByteEnum 705 NullableByteEnum 706 NullableInt16Enum 707 NullableUInt16Enum 708 NullableInt32Enum 709 NullableUInt32Enum 710 NullableInt64Enum 711 NullableUInt64Enum 712 SdsTypeProperty A contract defining a property of a SdsType Properties Property Name Data Type Required Nullable Description Id string false true An unique identifier for the SdsTypeProperty object Name string false true An optional user-friendly name for the SdsTypeProperty object Description string false true A brief description of the SdsTypeProperty object Order int32 false false The order used for comparison among SdsTypeProperty s if a compound index is specified for SdsType IsKey boolean false false A boolean value indicating whether the current SdsTypeProperty must be used for indexing. Used in combination with property to enable compound indexing FixedSize int32 false false An optional property specifying the length of string Exclusively used for that is of SdsType SdsType false true SdsType of the current SdsTypeProperty Value any false true An enum value of the current SdsTypeProperty . Uom string false true Indicates the Unit of Measure of the current SdsTypeProperty InterpolationMode SdsInterpolationMode false true An InterpolationMode that overrides the root SdsType \u0027s InterpolationMode for this SdsTypeProperty IsQuality boolean false false Indicates whether this property marks data quality { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"Order\": 0, \"IsKey\": true, \"FixedSize\": 0, \"SdsType\": { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": 0, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": null, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": [ null ], \"Properties\": [ null ], \"BaseType\": null, \"DerivedTypes\": [ null ], \"InterpolationMode\": null, \"ExtrapolationMode\": null } ], \"Properties\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"Order\": 0, \"IsKey\": true, \"FixedSize\": 0, \"SdsType\": null, \"Value\": null, \"Uom\": \"string\", \"InterpolationMode\": null, \"IsQuality\": true } ], \"BaseType\": { \"Id\": null, \"Name\": null, \"Description\": null, \"SdsTypeCode\": null, \"IsGenericType\": null, \"IsReferenceType\": null, \"GenericArguments\": null, \"Properties\": null, \"BaseType\": null, \"DerivedTypes\": null, \"InterpolationMode\": null, \"ExtrapolationMode\": null }, \"DerivedTypes\": [ { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SdsTypeCode\": null, \"IsGenericType\": true, \"IsReferenceType\": true, \"GenericArguments\": [ null ], \"Properties\": [ null ], \"BaseType\": null, \"DerivedTypes\": [ null ], \"InterpolationMode\": null, \"ExtrapolationMode\": null } ], \"InterpolationMode\": 0, \"ExtrapolationMode\": 0 }, \"Value\": null, \"Uom\": \"string\", \"InterpolationMode\": 0, \"IsQuality\": true } SdsInterpolationMode Interpolation modes that can be applied to SdsType , SdsTypeProperty , SdsStream , and SdsStreamPropertyOverride objects Enumerated Values Property Value Default 0 Continuous 0 StepwiseContinuousLeading 1 StepwiseContinuousTrailing 2 Discrete 3 ContinuousNullableLeading 4 ContinuousNullableTrailing 5 SdsExtrapolationMode Defines how a stream responds to requests with indexes that precede or follow all data in the stream. Behavior also depends on the SdsInterpolationMode for a stream. If you set SdsInterpolationMode to Discrete , extrapolation does not occur. If you set SdsInterpolationMode to ContinuousNullableLeading or ContinuousNullableTrailing , default values are returned instead of actual data. Enumerated Values Property Value All 0 None 1 Forward 2 Backward 3 ErrorResponseBody Contains the error message format that follows the AVEVA Data Hub error standards Properties Property Name Data Type Required Nullable Description OperationId string false true Operation unique identifier of action that caused the error Error string false true Error description Reason string false true Reason for the error Resolution string false true Resolution to resolve the error Parameters object false true Ids or values that are creating or are affected by the error { \"OperationId\": \"string\", \"Error\": \"string\", \"Reason\": \"string\", \"Resolution\": \"string\", \"Parameters\": { \"property1\": \"string\", \"property2\": \"string\" } }"
                                            },
    "content/sds/types/sds-type-code.html":  {
                                                 "href":  "content/sds/types/sds-type-code.html",
                                                 "title":  "SdsTypeCode",
                                                 "keywords":  "SdsTypeCode The SdsTypeCode is a numeric identifier used by SDS to identify SdsTypes. An SdsTypeCode exists for every supported type. Atomic types, such as strings, floats, and arrays, are defined entirely by the SdsTypeCode. Atomic types do not need fields to define the type. Types requiring additional definition, such as enums and objects, are identified using a generic SdsTypeCode, such as ByteEnum, Int32Enum, NullableInt32Enum, or Object, plus additional SdsProperty fields. Supported Types The following tables lists types that are supported and defined by the SdsTypeCode. Type SdsTypeCode Array 400 Boolean 3 BooleanArray 203 Byte 6 ByteArray 206 ByteEnum 606 Char 4 CharArray 204 DateTime 16 DateTimeArray 216 DateTimeOffset 20 DateTimeOffsetArray 220 DBNull 2 Decimal 15 DecimalArray 215 Double 14 DoubleArray 214 Empty 0 Guid 19 GuidArray 219 IDictionary 402 IEnumerable 403 IList 401 Int16 7 Int16Array 207 Int16Enum 607 Int32 9 Int32Array 209 Int32Enum 609 Int64 11 Int64Array 211 Int64Enum 611 NullableBoolean 103 NullableByte 106 NullableByteEnum 706 NullableChar 104 NullableDateTime 116 NullableDateTimeOffset 120 NullableDecimal 115 NullableDouble 114 NullableGuid 119 NullableInt16 107 NullableInt16Enum 707 NullableInt32 109 NullableInt32Enum 709 NullableInt64 111 NullableInt64Enum 711 NullableSByte 105 NullableSByteEnum 705 NullableSingle 113 NullableTimeSpan 121 NullableUInt16 108 NullableUInt16Enum 708 NullableUInt32 110 NullableUInt32Enum 710 NullableUInt64 112 NullableUInt64Enum 712 Object 1 SByte 5 SByteArray 205 SByteEnum 605 Single 13 SingleArray 213 String 18 StringArray 218 TimeSpan 21 TimeSpanArray 221 UInt16 8 UInt16Array 208 UInt16Enum 608 UInt32 10 UInt32Array 210 UInt32Enum 610 UInt64 12 UInt64Array 212 UInt64Enum 612 Version 22 VersionArray 222"
                                             },
    "content/sds/types/sds-type-examples.html":  {
                                                     "href":  "content/sds/types/sds-type-examples.html",
                                                     "title":  "Examples of SdsTypes",
                                                     "keywords":  "Examples of SdsTypes The examples in this section refer to the following types and are defined in Python and JavaScript samples. In the sample code, SdsType , SdsTypeProperty , and SdsTypeCode are defined as in the code snippets shown here: Python class SdsTypeCode(Enum): Empty = 0 Object = 1 DBNull = 2 Boolean = 3 Char = 4 ... class SdsTypeProperty(object): \"\"\"SDS type property definition\"\"\" def __init__(self): self.__isKey = False @property def Id(self): return self.__id @Id.setter def Id(self, id): self.__id = id ... @property def IsKey(self): return self.__isKey @IsKey.setter def IsKey(self, iskey): self.__isKey = iskey @property def SdsType(self): return self.__SdsType @SdsType.setter def SdsType(self, SdsType): self.__SdsType=SdsType ... class SdsType(object): \"\"\"SDS type definitions\"\"\" def __init__(self): self.SdsTypeCode = SdsTypeCode.Object @property def Id(self): return self.__id @Id.setter def Id(self, id): self.__id = id ... @property def BaseType(self): return self.__baseType @BaseType.setter def BaseType(self, baseType): self.__baseType = baseType @property def SdsTypeCode(self): return self.__typeCode @SdsTypeCode.setter def SdsTypeCode(self, typeCode): self.__typeCode = typeCode @property def Properties(self): return self.__properties @Properties.setter def Properties(self, properties): self.__properties = properties JavaScript SdsTypeCodeMap: { Empty: 0, \"Object\": 1, DBNull: 2, \"Boolean\": 3, Char: 4, ... SdsTypeProperty: function (SdsTypeProperty) { if (SdsTypeProperty.Id) { this.Id = SdsTypeProperty.Id; } if (SdsTypeProperty.Name) { this.Name = SdsTypeProperty.Name; } if (SdsTypeProperty.Description) { this.Description = SdsTypeProperty.Description; } if (SdsTypeProperty.SdsType) { this.SdsType = SdsTypeProperty.SdsType; } if (SdsTypeProperty.IsKey) { this.IsKey = SdsTypeProperty.IsKey; } }, SdsType: function (SdsType) { if (SdsType.Id) { this.Id = SdsType.Id } if (SdsType.Name) { this.Name = SdsType.Name; } if (SdsType.Description) { this.Description = SdsType.Description; } if (SdsType.SdsTypeCode) { this.SdsTypeCode = SdsType.SdsTypeCode; } if (SdsType.Properties) { this.Properties = SdsType.Properties; } }, Working with the following types (both Python and JavaScript classes are shown): Python class State(Enum): Ok = 0 Warning = 1 Alarm = 2 class Simple(object): Time = property(getTime, setTime) def getTime(self): return self.__time def setTime(self, time): self.__time = time State = property(getState, setState) def getState(self): return self.__state def setState(self, state): self.__state = state Measurement = property(getMeasurement, setMeasurement) def getMeasurement(self): return self.__measurement def setMeasurement(self, measurement): self.__measurement = measurement JavaScript var State = { Ok: 0, Warning: 1, Alarm: 2, } var Simple = function () { this.Time = null; this.State = null; this.Measurement = null; } Define the SdsType as follows: Python # Create the properties # Time is the primary key time = SdsTypeProperty() time.Id = \"Time\" time.Name = \"Time\" time.IsKey = True time.SdsType = SdsType() time.SdsType.Id = \"DateTime\" time.SdsType.Name = \"DateTime\" time.SdsType.SdsTypeCode = SdsTypeCode.DateTime # State is not a pre-defined type. A SdsType must be defined to represent the enum stateTypePropertyOk = SdsTypeProperty() stateTypePropertyOk.Id = \"Ok\" stateTypePropertyOk.Value = State.Ok stateTypePropertyWarning = SdsTypeProperty() stateTypePropertyWarning.Id = \"Warning\" stateTypePropertyWarning.Value = State.Warning stateTypePropertyAlarm = SdsTypeProperty() stateTypePropertyAlarm.Id = \"Alarm\" stateTypePropertyAlarm.Value = State.Alarm stateType = SdsType() stateType.Id = \"State\" stateType.Name = \"State\" stateType.Properties = [ stateTypePropertyOk, stateTypePropertyWarning, \\ stateTypePropertyAlarm ] state = SdsTypeProperty() state.Id = \"State\" state.Name = \"State\" state.SdsType = stateType # Value property is a simple non-indexed, pre-defined type value = SdsTypeProperty() value.Id = \"Measurement\" value.Name = \"Measurement\" value.SdsType = SdsType() value.SdsType.Id = \"Double\" value.SdsType.Name = \"Double\" # Create the Simple SdsType simpleType = SdsType() simpleType.Id = \"Simple\" simpleType.Name = \"Simple\" simpleType.Description = \"Basic sample type\" simpleType.SdsTypeCode = SdsTypeCode.Object simpleType.Properties = [ time ] JavaScript //    Time is the primary key var timeProperty = new SdsObjects.SdsTypeProperty({ \"Id\": \"Time\", \"IsKey\": true, \"SdsType\": new SdsObjects.SdsType({ \"Id\": \"dateType\", \"SdsTypeCode\": SdsObjects.SdsTypeCodeMap.DateTime }) }); //    State is not a pre-defined type. An SdsType must be defined to represent the enum var stateTypePropertyOk = new SdsObjects.SdsTypeProperty({ \"Id\": \"Ok\", \"Value\": State.Ok }); var stateTypePropertyWarning = new SdsObjects.SdsTypeProperty({ \"Id\": \"Warning\", \"Value\": State.Warning }); var stateTypePropertyAlarm = new SdsObjects.SdsTypeProperty({ \"Id\": \"Alarm\", \"Value\": State.Alarm }); var stateType = new SdsObjects.SdsType({ \"Id\": \"State\", \"Name\": \"State\", \"SdsTypeCode\": SdsObjects.SdsTypeCodeMap.Int32Enum, \"Properties\": [stateTypePropertyOk, stateTypePropertyWarning, stateTypePropertyAlarm, stateTypePropertyRed] }); //    Measurement property is a simple non-indexed, pre-defined type var measurementProperty = new SdsObjects.SdsTypeProperty({ \"Id\": \"Measurement\", \"Name\": \"Measurement\", \"SdsType\": new SdsObjects.SdsType({ \"Id\": \"doubleType\", \"SdsTypeCode\": SdsObjects.SdsTypeCodeMap.Double }) }); //    Create the Simple SdsType var simpleType = new SdsObjects.SdsType({ \"Id\": \"Simple\", \"Name\": \"Simple\", \"Description\": \"This is a simple SDS type \", \"SdsTypeCode\": SdsObjects.SdsTypeCodeMap.Object, \"Properties\": [timeProperty, stateProperty, measurementProperty] }); Working with a derived class is easy. For the following derived class: class Derived(Simple): @property def Observation(self): return self.__observation @Observation.setter def Observation(self, observation): self.__observation = observation Extend the SdsType as follows: Python # Observation property is a simple non-indexed, standard data type observation = SdsTypeProperty() observation.Id = \"Observation\" observation.Name = \"Observation\" observation.SdsType = SdsType() observation.SdsType.Id = \"String\" observation.SdsType.Name = \"String\" observation.SdsType.SdsTypeCode = SdsTypeCode.String # Create the Derived SdsType derived = SdsType() derived.Id = \"Derived\" derived.Name = \"Derived\" derived.Description = \"Derived sample type\" derived.BaseType = simpleType # Set the base type to the derived type derived.SdsTypeCode = SdsTypeCode.Object derived.Properties = [ observation ] JavaScript var observationProperty = new SdsObjects.SdsTypeProperty({ \"Id\": \"Observation\", \"SdsType\": new SdsObjects.SdsType({ \"Id\": \"strType\", \"SdsTypeCode\": SdsObjects.SdsTypeCodeMap.String }) }); var derivedType = new SdsObjects.SdsType({ \"Id\": \"Derived\", \"Name\": \"Derived\", \"Description\": \" Derived sample type\", \"BaseType\": simpleType, \"SdsTypeCode\": SdsObjects.SdsTypeCodeMap.Object, \"Properties\": [ observationProperty ] });"
                                                 },
    "content/sds/types/sds-type-in-dotnet.html":  {
                                                      "href":  "content/sds/types/sds-type-in-dotnet.html",
                                                      "title":  "SdsTypes in .NET framework",
                                                      "keywords":  "SdsTypes in .NET framework When working in .NET, use the SdsTypeBuilder to create SdsType s. The SdsTypeBuilder eliminates potential errors that can occur when working with SdsTypes manually. There are several ways to work with the SdsTypeBuilder . One is to use the static methods for convenience: public enum State { Ok, Warning, Alarm } public class Simple { [SdsMember(IsKey = true, Order = 0)] public DateTime Time { get; set; } [SdsMember(IsQuality = true)] public State State { get; set; } public Double Measurement { get; set; } } SdsType simpleType = SdsTypeBuilder.CreateSdsType\u003cSimple\u003e(); simpleType.Id = \"Simple\"; simpleType.Name = \"Simple\"; simpleType.Description = \"Basic sample type\"; SdsTypeBuilder recognizes the System.ComponentModel.DataAnnotations.KeyAttribute and its own OSIsoft.Sds.SdsMemberAttribute . When using the SdsMemberAttribute to specify the primary index, set the IsKey to true . The SdsType is created with the following parameters. SdsTypeBuilder automatically generates unique identifiers. Note that the following table contains only a partial list of fields. Field Values Id Simple Name Simple Description Basic sample type Properties Count = 3 [0] Id Time Name Time Description null Order 0 IsKey true SdsType Id c48bfdf5-a271-384b-bf13-bd21d931c1bf Name DateTime Description null Properties null Value null [1] Id State Name State Description null Order 0 IsKey false SdsType Id 02728a4f-4a2d-3588-b669-e08f19c35fe5 Name State Description null Properties Count = 3 [0] Id Name Description Order SdsType Value [1] Id Name Description Order SdsType Value [2] Id Name Description Order SdsType Value Value null [2] Id Measurement Name Measurement Description null Order 0 IsKey false SdsType Id 0f4f147f-4369-3388-8e4b-71e20c96f9ad Name Double Description null Properties null Value null The SdsTypeBuilder also supports derived types. You need not add the base types to SDS before using SdsTypeBuilder . Base types are maintained within the SdsType."
                                                  },
    "content/sds/types/sds-type-reusability.html":  {
                                                        "href":  "content/sds/types/sds-type-reusability.html",
                                                        "title":  "SdsType reusability",
                                                        "keywords":  "SdsType reusability An SdsType can refer other SdsTypes by using their identifiers. This enables type reusability. For example, if there is a common index and value property for a group of types that may have additional properties, you can create a base type with those properties. This is shown in the following example. { \"Id\": \"Simple\", \"Name\": \"Simple\", \"SdsTypeCode\": 1, \"Properties\": [ { \"Id\": \"Time\", \"Name\": \"Time\", \"IsKey\": true, \"SdsType\": { \"SdsTypeCode\": 16 } }, { \"Id\": \"Measurement\", \"Name\": \"Measurement\", \"SdsType\": { \"SdsTypeCode\": 14 } } ] } If a new type should be created with additional properties to the ones above, add a reference to the base type by specifying the base type\u0027s Id , as shown in the following example. { \"Id\": \"Complex\", \"Name\": \"Complex\", \"SdsTypeCode\": 1, \"BaseType\":{ \"Id\":\"Simple\" }, \"Properties\": [ { \"Id\": \"Depth\", \"Name\": \"Depth\", \"SdsType\": { \"SdsTypeCode\": 14 } } ] } The new type may also include the full type definition of the reference type instead of specifying only the Id , as shown in the following example. { \"Id\": \"Complex\", \"Name\": \"Complex\", \"SdsTypeCode\": 1, \"BaseType\":{ \"Id\": \"Simple\", \"Name\": \"Simple\", \"SdsTypeCode\": 1, \"Properties\": [ { \"Id\": \"Time\", \"Name\": \"Time\", \"IsKey\": true, \"SdsType\": { \"SdsTypeCode\": 16 } }, { \"Id\": \"Measurement\", \"Name\": \"Measurement\", \"SdsType\": { \"SdsTypeCode\": 14 } } ] }, \"Properties\": [ { \"Id\": \"Depth\", \"Name\": \"Depth\", \"SdsType\": { \"SdsTypeCode\": 14 } } ] } If you send the full definition, the referenced types (base type Simple in the example above) should match the actual type initially created. If you send the full definition and the referenced types do not exist, SDS creates them automatically. Further type creations can reference them as demonstrated above. Note: When trying to get types back from SDS, the results will also include types that were automatically created by SDS. Base types and properties of type Object, Enum, and user-defined collections such as Array, List, and Dictionary are treated as referenced types. Note that you cannot create streams using these referenced types. If a stream of particular type is to be created, the type should contain at least one property with a valid index type as described in Indexes . The index property may also be in the base type as shown in the example above. If needed, the base type\u0027s Id can be changed to be more meaningful. You can do this using any programming language. The following is a .NET example. public class Basic { [SdsMember(IsKey = true, Order = 0)] public DateTime Time { get; set; } public double Temperature { get; set; } } public class EngineMonitor : Basic { public double PistonSpeed { get; set; } } public class WindShieldMonitor : Basic { public double Luminance { get; set; } } SdsType engineType = SdsTypeBuilder.CreateSdsType\u003cEngineMonitor\u003e(); engineType.Id = \"Engine\"; engineType.BaseType.Id = \"Basic\"; SdsType windShieldType = SdsTypeBuilder.CreateSdsType\u003cWindShieldMonitor\u003e(); windShieldType.Id = \"WindShield\"; windShieldType.BaseType.Id = \"Basic\";"
                                                    },
    "content/sds/types/sds-types-outside-dotnet.html":  {
                                                            "href":  "content/sds/types/sds-types-outside-dotnet.html",
                                                            "title":  "SdsTypes outside of .NET framework",
                                                            "keywords":  "SdsTypes outside of .NET framework You can manually build types when .NET SdsTypeBuilder is unavailable. In the following examples, types are built and defined in Python and Javascript. For more detailed examples, see the AVEVA Data Hub code samples in GitHub for Python and JavaScript . For samples in other languages, go to [AVEVA Data Hub code samples in GitHub]( https://github.com/osisoft/OSI-Samples-AVEVA https:  github.com osisoft OSI-Samples-AVEVA Data Hub/blob/master/docs/SDS_WAVEFORM_README.md). Hub blob master docs SDS_WAVEFORM_README.md). SdsTypeCode, SdsTypeProperty, and SdsType Python JavaScript class SdsTypeCode(Enum): Empty = 0 Object = 1 DBNull = 2 Boolean = 3 Char = 4 ... class SdsTypeProperty(object): \"\"\"SDS type property definition\"\"\" def __init__(self): self.__isKey = False @property def Id(self): return self.__id @Id.setter def Id(self, id): self.__id = id ... @property def IsKey(self): return self.__isKey @IsKey.setter def IsKey(self, iskey): self.__isKey = iskey @property def SdsType(self): return self.__SdsType @SdsType.setter def SdsType(self, SdsType): self.__SdsType=SdsType ... class SdsType(object): \"\"\"SDS type definitions\"\"\" def __init__(self): self.SdsTypeCode = SdsTypeCode.Object @property def Id(self): return self.__id @Id.setter def Id(self, id): self.__id = id ... @property def BaseType(self): return self.__baseType @BaseType.setter def BaseType(self, baseType): self.__baseType = baseType @property def SdsTypeCode(self): return self.__typeCode @SdsTypeCode.setter def SdsTypeCode(self, typeCode): self.__typeCode = typeCode @property def Properties(self): return self.__properties @Properties.setter def Properties(self, properties): self.__properties = properties SdsTypeCodeMap: { Empty: 0, \"Object\": 1, DBNull: 2, \"Boolean\": 3, Char: 4, ... SdsTypeProperty: function (SdsTypeProperty) { if (SdsTypeProperty.Id) { this.Id = SdsTypeProperty.Id; } if (SdsTypeProperty.Name) { this.Name = SdsTypeProperty.Name; } if (SdsTypeProperty.Description) { this.Description = SdsTypeProperty.Description; } if (SdsTypeProperty.SdsType) { this.SdsType = SdsTypeProperty.SdsType; } if (SdsTypeProperty.IsKey) { this.IsKey = SdsTypeProperty.IsKey; } }, SdsType: function (SdsType) { if (SdsType.Id) { this.Id = SdsType.Id } if (SdsType.Name) { this.Name = SdsType.Name; } if (SdsType.Description) { this.Description = SdsType.Description; } if (SdsType.SdsTypeCode) { this.SdsTypeCode = SdsType.SdsTypeCode; } if (SdsType.Properties) { this.Properties = SdsType.Properties; } }, Enum State and type Simple Python JavaScript class State(Enum): Ok = 0 Warning = 1 Alarm = 2 class Simple(object): Time = property(getTime, setTime) def getTime(self): return self.__time def setTime(self, time): self.__time = time State = property(getState, setState) def getState(self): return self.__state def setState(self, state): self.__state = state Measurement = property(getMeasurement, setMeasurement) def getMeasurement(self): return self.__measurement def setMeasurement(self, measurement): self.__measurement = measurement var State = { Ok: 0, Warning: 1, Alarm: 2, } var Simple = function () { this.Time = null; this.State = null; this.Measurement = null; } Defining and creating types Python JavaScript # Create the properties # Time is the primary index time = SdsTypeProperty() time.Id = \"Time\" time.Name = \"Time\" time.IsKey = True time.SdsType = SdsType() time.SdsType.Id = \"DateTime\" time.SdsType.Name = \"DateTime\" time.SdsType.SdsTypeCode = SdsTypeCode.DateTime # State is not a pre-defined type. SdsType must be defined to represent the enum stateTypePropertyOk = SdsTypeProperty() stateTypePropertyOk.Id = \"Ok\" stateTypePropertyOk.Value = State.Ok stateTypePropertyWarning = SdsTypeProperty() stateTypePropertyWarning.Id = \"Warning\" stateTypePropertyWarning.Value = State.Warning stateTypePropertyAlarm = SdsTypeProperty() stateTypePropertyAlarm.Id = \"Alarm\" stateTypePropertyAlarm.Value = State.Alarm stateType = SdsType() stateType.Id = \"State\" stateType.Name = \"State\" stateType.Properties = [ stateTypePropertyOk, stateTypePropertyWarning, \\ stateTypePropertyAlarm ] state = SdsTypeProperty() state.Id = \"State\" state.Name = \"State\" state.SdsType = stateType # Value property is a simple non-indexed, pre-defined type value = SdsTypeProperty() value.Id = \"Measurement\" value.Name = \"Measurement\" value.SdsType = SdsType() value.SdsType.Id = \"Double\" value.SdsType.Name = \"Double\" # Create the Simple SdsType simpleType = SdsType() simpleType.Id = \"Simple\" simpleType.Name = \"Simple\" simpleType.Description = \"Basic sample type\" simpleType.SdsTypeCode = SdsTypeCode.Object simpleType.Properties = [ time ] //    Time is the primary index var timeProperty = new SdsObjects.SdsTypeProperty({ \"Id\": \"Time\", \"IsKey\": true, \"SdsType\": new SdsObjects.SdsType({ \"Id\": \"dateType\", \"SdsTypeCode\": SdsObjects.SdsTypeCodeMap.DateTime }) }); //    State is not a pre-defined type. An SdsType must be defined to represent the enum var stateTypePropertyOk = new SdsObjects.SdsTypeProperty({ \"Id\": \"Ok\", \"Value\": State.Ok }); var stateTypePropertyWarning = new SdsObjects.SdsTypeProperty({ \"Id\": \"Warning\", \"Value\": State.Warning }); var stateTypePropertyAlarm = new SdsObjects.SdsTypeProperty({ \"Id\": \"Alarm\", \"Value\": State.Alarm }); var stateType = new SdsObjects.SdsType({ \"Id\": \"State\", \"Name\": \"State\", \"SdsTypeCode\": SdsObjects.SdsTypeCodeMap.Int32Enum, \"Properties\": [stateTypePropertyOk, stateTypePropertyWarning, stateTypePropertyAlarm, stateTypePropertyRed] }); //    Measurement property is a simple non-indexed, pre-defined type var measurementProperty = new SdsObjects.SdsTypeProperty({ \"Id\": \"Measurement\", \"Name\": \"Measurement\", \"SdsType\": new SdsObjects.SdsType({ \"Id\": \"doubleType\", \"SdsTypeCode\": SdsObjects.SdsTypeCodeMap.Double }) }); //    Create the Simple SdsType var simpleType = new SdsObjects.SdsType({ \"Id\": \"Simple\", \"Name\": \"Simple\", \"Description\": \"This is a simple SDS type \", \"SdsTypeCode\": SdsObjects.SdsTypeCodeMap.Object, \"Properties\": [timeProperty, stateProperty, measurementProperty] }); Working with a derived class is easy. For example, this is a derived class: class Derived(Simple): @property def Observation(self): return self.__observation @Observation.setter def Observation(self, observation): self.__observation = observation Extending the types Python JavaScript # Observation property is a simple non-indexed, standard data type observation = SdsTypeProperty() observation.Id = \"Observation\" observation.Name = \"Observation\" observation.SdsType = SdsType() observation.SdsType.Id = \"String\" observation.SdsType.Name = \"String\" observation.SdsType.SdsTypeCode = SdsTypeCode.String # Create the derived SdsType derived = SdsType() derived.Id = \"Derived\" derived.Name = \"Derived\" derived.Description = \"Derived sample type\" derived.BaseType = simpleType # Set the base type to the derived type derived.SdsTypeCode = SdsTypeCode.Object derived.Properties = [ observation ] var observationProprety = new SdsObjects.SdsTypeProperty({ \"Id\": \"Observation\", \"SdsType\": new SdsObjects.SdsType({ \"Id\": \"strType\", \"SdsTypeCode\": SdsObjects.SdsTypeCodeMap.String }) }); var derivedType = new SdsObjects.SdsType({ \"Id\": \"Derived\", \"Name\": \"Derived\", \"Description\": \" Derived sample type\", \"BaseType\": simpleType, \"SdsTypeCode\": SdsObjects.SdsTypeCodeMap.Object, \"Properties\": [ observationProprety ] });"
                                                        },
    "content/sds/units-of-measure.html":  {
                                              "href":  "content/sds/units-of-measure.html",
                                              "title":  "Units of measure",
                                              "keywords":  "Units of measure SDS provides a collection of built-in units of measure (Uom). These units of measure can be associated with SdsStreams and SdsTypes to provide unit information for stream data that model measurable quantities. If data has unit information associated with it, SDS supports unit conversions when retrieving data. For more information, see Read data . Since a unit of measurement (such as meter) defines the magnitude of a quantity (such as Length), SDS represents this through two objects: SdsUom and SdsUomQuantity . SdsUom An SdsUom represents a single unit of measure, such as \u0027meter\u0027. The following table shows the required and optional SdsUom fields. Property Type Optionality Description Example Id String Required Unique identifier for the unit of measure. meters per second Abbreviation String Optional Abbreviation for the unit of measure. m/s m s Name String Optional Full name for the unit of measure. Meters per second DisplayName String Optional Friendly display name for the unit of measure. meters per second QuantityId String Required The identifier for the quantity that this unit of measure quantifies. Velocity ConversionFactor Double Required Used for unit conversions. When a value of this unit is multiplied by the ConversionFactor and then incremented by the ConversionOffset , the value in terms of the base unit of the corresponding quantity is returned. 1.0 ConversionOffset Double Required Used for unit conversions. See details for ConversionFactor . 0.0 SdsUomQuantity An SdsUomQuantity represents a single measurable quantity, such as length. The following table shows the required and optional SdsUomQuantity fields. Property Type Optionality Description Example Id String Required Unique identifier for the quantity. Velocity Name String Optional Full name for the quantity. Velocity BaseUom SdsUom Required The base unit of measure for this quantity. All other Uoms measuring this quantity will have ConversionFactors and ConversionOffsets relative to the BaseUom . SdsUom representing \"meters per second\" Dimensions short[] Optional Reserved for internal use. Represents the seven base SI dimensions: Length, Mass, Time, Electric Current, Thermodynamic Temperature, Amount of Substance, and Luminous Density. [1,0,-1,0,0,0,0] Associating a unit of measure with a type At type creation, SdsUom can be associated with an SdsTypeProperty . For types API, see Types . Associating a unit of measure with a stream At stream creation, you can override any unit of measure associated with an SdsTypeProperty belonging to the type of the stream. This enables the reuse of a type that may have default unit information associated with it already. For streams API, see Streams ."
                                          },
    "content/sds/uom/sds-uom-api.html":  {
                                             "href":  "content/sds/uom/sds-uom-api.html",
                                             "title":  "SdsUom API",
                                             "keywords":  "SdsUom API The REST APIs provide programmatic access to read and write SDS data. The APIs in this section interact with SdsUoms. For more information, see SdsUom . List Units of Measure Returns a list of all available units of measure in the system Request GET /api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Units?skip={skip}\u0026count={count}  api v1 Tenants {tenantId} Namespaces {namespaceId} Units?skip={skip}\u0026count={count} Parameters string tenantId Tenant identifier string namespaceId Namespace identifier [Optional] int skip Parameter representing the zero-based offset of the first object to retrieve. If unspecified, a default value of 0 is used. [Optional] int count Parameter representing the maximum number of objects to retrieve. If unspecified, a default value of 100 is used. Response Status Code Body Type Description 200 SdsUom [] Returns a list of SdsUom objects 400 ErrorResponseBody One of the resources specified was invalid or missing. 401 ErrorResponseBody Unauthorized 403 ErrorResponseBody Forbidden 404 ErrorResponseBody One of the resources specified was not found. 500 ErrorResponseBody An error occurred while processing the request. 503 ErrorResponseBody Service Unavailable Example response body 200 Response HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Id\":\"count\", \"Abbreviation\":\"count\", \"Name\":\"count\", \"DisplayName\":\"count\", \"QuantityId\":\"Quantity\", \"ConversionFactor\":1 }, { \"Id\":\"Ampere hour\", \"Abbreviation\":\"Ah\", \"Name\":\"Ampere hour\", \"DisplayName\":\"Ampere hour\", \"QuantityId\":\"Electric Charge\", \"ConversionFactor\":3600 }, { \"Id\":\"coulomb\", \"Abbreviation\":\"C\", \"Name\":\"coulomb\", \"DisplayName\":\"coulomb\", \"QuantityId\":\"Electric Charge\", \"ConversionFactor\":1 } ] 400 Response ( ErrorResponseBody ) { \"OperationId\": \"string\", \"Error\": \"string\", \"Reason\": \"string\", \"Resolution\": \"string\", \"Parameters\": { \"property1\": \"string\", \"property2\": \"string\" } } Get Unit of Measure Returns the unit of measure corresponding to the specified uomId within a given namespace Request GET /api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Units/{uomId}  api v1 Tenants {tenantId} Namespaces {namespaceId} Units {uomId} Parameters string tenantId Tenant identifier string namespaceId Namespace identifier string uomId The unit of measure identifier Response Status Code Body Type Description 200 SdsUom Returns the SdsUom 400 ErrorResponseBody One of the resources specified was invalid or missing. 401 ErrorResponseBody Unauthorized 403 ErrorResponseBody Forbidden 404 ErrorResponseBody One of the resources specified was not found. 500 ErrorResponseBody An error occurred while processing the request. 503 ErrorResponseBody Service Unavailable Example response body 200 Response HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json { \"Id\":\"ounce\", \"Abbreviation\":\"oz\", \"Name\":\"ounce\", \"DisplayName\":\"ounce\", \"QuantityId\":\"Mass\", \"ConversionFactor\":0.028349523 } 400 Response ( ErrorResponseBody ) { \"OperationId\": \"string\", \"Error\": \"string\", \"Reason\": \"string\", \"Resolution\": \"string\", \"Parameters\": { \"property1\": \"string\", \"property2\": \"string\" } } Definitions SdsUom DataContract representing a unit of measure Properties Property Name Data Type Required Nullable Description Id string false true Unique identifier for this unit of measure Abbreviation string false true Abbreviation for this unit of measure Name string false true Full name for this unit of measure DisplayName string false true Display name for this unit of measure QuantityId string false true The Id of the quantity that this unit is a measure of ConversionFactor double false false When a value of this unit is multiplied by the ConversionFactor and then incremented by the ConversionOffset , the value in terms of the base unit of the corresponding quantity is returned. ConversionOffset double false false When a value of this unit is multiplied by the ConversionFactor and then incremented by the ConversionOffset , the value in terms of the base unit of the corresponding quantity is returned. { \"Id\": \"string\", \"Abbreviation\": \"string\", \"Name\": \"string\", \"DisplayName\": \"string\", \"QuantityId\": \"string\", \"ConversionFactor\": 0, \"ConversionOffset\": 0 } ErrorResponseBody Contains the error message format that follows the AVEVA Data Hub error standards Properties Property Name Data Type Required Nullable Description OperationId string false true Operation unique identifier of action that caused the error Error string false true Error description Reason string false true Reason for the error Resolution string false true Resolution to resolve the error Parameters object false true IDs or values that are creating or are affected by the error { \"OperationId\": \"string\", \"Error\": \"string\", \"Reason\": \"string\", \"Resolution\": \"string\", \"Parameters\": { \"property1\": \"string\", \"property2\": \"string\" } }"
                                         },
    "content/sds/uom/sds-uom-quantity-api.html":  {
                                                      "href":  "content/sds/uom/sds-uom-quantity-api.html",
                                                      "title":  "SdsUomQuantity API",
                                                      "keywords":  "SdsUomQuantity API The REST APIs provide programmatic access to read and write SDS data. The APIs in this section interact with SdsUomQuantity s. For more information, see SdsUomQuantity . List Quantities Returns a list of all quantities available within a given namespace Request GET /api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Quantities?skip={skip}\u0026count={count}  api v1 Tenants {tenantId} Namespaces {namespaceId} Quantities?skip={skip}\u0026count={count} Parameters string tenantId Tenant identifier string namespaceId Namespace identifier [Optional] int skip Parameter representing the zero-based offset of the first object to retrieve. If unspecified, a default value of 0 is used. [Optional] int count Parameter representing the maximum number of objects to retrieve. If unspecified, a default value of 100 is used. Response Status Code Body Type Description 200 SdsUomQuantity [] Returns a list of SdsUomQuantity objects 400 ErrorResponseBody Missing or invalid inputs 401 ErrorResponseBody Unauthorized 403 ErrorResponseBody Forbidden 404 ErrorResponseBody One of the resources specified was not found. 500 ErrorResponseBody An error occurred while processing the request. 503 ErrorResponseBody Service Unavailable Example response body 200 Response HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Id\":\"Angular Velocity\", \"Name\":\"Angular Velocity\", \"BaseUom\":{ \"Id\":\"radian per second\", \"Abbreviation\":\"rad/s\", \"Abbreviation\":\"rad s\", \"Name\":\"radian per second\", \"DisplayName\":\"radian per second\", \"QuantityId\":\"Angular Velocity\", \"ConversionFactor\":1 }, \"Dimensions\":[ 0, 0, -1, 0, 0, 0, 0 ] }, { \"Id\":\"Area\", \"Name\":\"Area\", \"BaseUom\":{ \"Id\":\"square meter\", \"Abbreviation\":\"m2\", \"Name\":\"square meter\", \"DisplayName\":\"square meter\", \"QuantityId\":\"Area\", \"ConversionFactor\":1 }, \"Dimensions\":[ 2, 0, 0, 0, 0, 0, 0 ] } ] 400 Response ( ErrorResponseBody ) { \"OperationId\": \"string\", \"Error\": \"string\", \"Reason\": \"string\", \"Resolution\": \"string\", \"Parameters\": { \"property1\": \"string\", \"property2\": \"string\" } } Get Quantity Returns the quantity corresponding to the specified quantityId within a given namespace Request GET /api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Quantities/{quantityId}  api v1 Tenants {tenantId} Namespaces {namespaceId} Quantities {quantityId} Parameters string tenantId Tenant identifier string namespaceId Namespace identifier string quantityId The quantity identifier Response Status Code Body Type Description 200 SdsUomQuantity Returns the SdsUomQuantity 400 ErrorResponseBody Missing or invalid inputs 401 ErrorResponseBody Unauthorized 403 ErrorResponseBody Forbidden 404 ErrorResponseBody One of the resources specified was not found. 500 ErrorResponseBody An error occurred while processing the request. 503 ErrorResponseBody Service Unavailable Example response body 200 Response HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json { \"Id\":\"Length\", \"Name\":\"Length\", \"BaseUom\":{ \"Id\":\"meter\", \"Abbreviation\":\"m\", \"Name\":\"meter\", \"DisplayName\":\"meter\", \"QuantityId\":\"Length\", \"ConversionFactor\":1 }, \"Dimensions\":[ 1, 0, 0, 0, 0, 0, 0 ] } 400 Response ( ErrorResponseBody ) { \"OperationId\": \"string\", \"Error\": \"string\", \"Reason\": \"string\", \"Resolution\": \"string\", \"Parameters\": { \"property1\": \"string\", \"property2\": \"string\" } } Get Quantity Unit of Measure Returns the unit of measure associated with the specified uomId belonging to the quantity with the specified quantityId Request GET /api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Quantities/{quantityId}/Units/{uomId}  api v1 Tenants {tenantId} Namespaces {namespaceId} Quantities {quantityId} Units {uomId} Parameters string tenantId Tenant identifier string namespaceId Namespace identifier string quantityId The quantity identifier string uomId The unit of measure identifier Response Status Code Body Type Description 200 SdsUom Returns the SdsUom 400 ErrorResponseBody Missing or invalid inputs 401 ErrorResponseBody Unauthorized 403 ErrorResponseBody Forbidden 404 ErrorResponseBody One of the resources specified was not found. 500 ErrorResponseBody An error occurred while processing the request. 503 ErrorResponseBody Service Unavailable Example response body 200 Response HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json { \"Id\": \"mile\", \"Abbreviation\": \"mi\", \"Name\": \"mile\", \"DisplayName\": \"mile\", \"QuantityId\": \"Length\", \"ConversionFactor\": 1609.344 } 400 Response ( ErrorResponseBody ) { \"OperationId\": \"string\", \"Error\": \"string\", \"Reason\": \"string\", \"Resolution\": \"string\", \"Parameters\": { \"property1\": \"string\", \"property2\": \"string\" } } List Quantity Units of Measure Returns the list of units of measure that belongs to the quantity with the specified quantityId Request GET /api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Quantities/{quantityId}/Units  api v1 Tenants {tenantId} Namespaces {namespaceId} Quantities {quantityId} Units Parameters string tenantId Tenant identifier string namespaceId Namespace identifier string quantityId The quantity identifier Response Status Code Body Type Description 200 SdsUom [] Returns a list of SdsUom objects 400 ErrorResponseBody Missing or invalid inputs 401 ErrorResponseBody Unauthorized 403 ErrorResponseBody Forbidden 404 ErrorResponseBody One of the resources specified was not found. 500 ErrorResponseBody An error occurred while processing the request. 503 ErrorResponseBody Service Unavailable Example response body 200 Response HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Id\":\"milliampere\", \"Abbreviation\":\"mA\", \"Name\":\"milliampere\", \"DisplayName\":\"milliampere\", \"QuantityId\":\"Electric Current\", \"ConversionFactor\":0.001 }, { \"Id\":\"ampere\", \"Abbreviation\":\"A\", \"Name\":\"ampere\", \"DisplayName\":\"ampere\", \"QuantityId\":\"Electric Current\", \"ConversionFactor\":1 } ] 400 Response ( ErrorResponseBody ) { \"OperationId\": \"string\", \"Error\": \"string\", \"Reason\": \"string\", \"Resolution\": \"string\", \"Parameters\": { \"property1\": \"string\", \"property2\": \"string\" } } Definitions SdsUomQuantity DataContract representing a measurable quantity. For example, A unit of measure \u0027meter\u0027 would measure a quantity \u0027length\u0027 Properties Property Name Data Type Required Nullable Description Id string false true Unique identifier for this quantity Name string false true Friendly name for this quantity BaseUom SdsUom false true The base unit of measurement for this quantity. All other uom\u0027s measuring this quantity will be calculated relative to the base uom. Dimensions [integer] false true Powers of each dimension that define this quantity. This array must have a length of 7. Dimensions represented in index order are Length, Mass, Time, Electric Current, Thermodynamic Temperature, Amount of Substance, and Luminous Density. For example, A velocity quantity would have values [1,0,-1,0,0,0,0] for (length)/(time) (length) (time) { \"Id\": \"string\", \"Name\": \"string\", \"BaseUom\": { \"Id\": \"string\", \"Abbreviation\": \"string\", \"Name\": \"string\", \"DisplayName\": \"string\", \"QuantityId\": \"string\", \"ConversionFactor\": 0, \"ConversionOffset\": 0 }, \"Dimensions\": [ 0 ] } SdsUom DataContract representing a unit of measure Properties Property Name Data Type Required Nullable Description Id string false true Unique identifier for this unit of measure Abbreviation string false true Abbreviation for this unit of measure Name string false true Full name for this unit of measure DisplayName string false true Display name for this unit of measure QuantityId string false true The Id of the quantity that this unit is a measure of ConversionFactor double false false When a value of this unit is multiplied by the ConversionFactor and then incremented by the ConversionOffset , the value in terms of the base unit of the corresponding quantity is returned. ConversionOffset double false false When a value of this unit is multiplied by the ConversionFactor and then incremented by the ConversionOffset , the value in terms of the base unit of the corresponding quantity is returned. { \"Id\": \"string\", \"Abbreviation\": \"string\", \"Name\": \"string\", \"DisplayName\": \"string\", \"QuantityId\": \"string\", \"ConversionFactor\": 0, \"ConversionOffset\": 0 } ErrorResponseBody Contains the error message format that follows the AVEVA Data Hub error standards Properties Property Name Data Type Required Nullable Description OperationId string false true Operation unique identifier of action that caused the error Error string false true Error description Reason string false true Reason for the error Resolution string false true Resolution to resolve the error Parameters object false true IDs or values that are creating or are affected by the error. { \"OperationId\": \"string\", \"Error\": \"string\", \"Reason\": \"string\", \"Resolution\": \"string\", \"Parameters\": { \"property1\": \"string\", \"property2\": \"string\" } }"
                                                  },
    "content/sds/uom/supported-uom.html":  {
                                               "href":  "content/sds/uom/supported-uom.html",
                                               "title":  "Supported units of measure",
                                               "keywords":  "Supported units of measure The following table shows the supported units of measure. Supported units of measure are read-only. Uom Id Abbreviation Quantity Id Conversion Factor Conversion Offset micrometer ??m Length 0.000001 0 nanometer nm Length 0.000000001 0 cubic foot per minute ft3/min ft3 min Volume Flow Rate 0.000471947443 0 hectopascal hPa Pressure 100 0 millibar mbar Pressure 100 0 milliwatt hour mWh Energy 3.60 0 newton meter Nm Energy 1 0 microgram per cubic meter ??g/m3 ??g m3 Density 0.000000001 0 millimeter per square second mm/s2 mm s2 Acceleration 0.001 0 meter per square second m/s2 m s2 Acceleration 1 0 decisiemens per meter dS/m dS m Conductivity 0.1 0 siemens per meter S/m S m Conductivity 1 0 microsiemens per centimeter ??S/cm ??S cm Conductivity 0.0001 0 count per liter Count/L Count L Quantity per Volume 0.001 0 count per cubic meter Count/m3 Count m3 Quantity per Volume 1 0 watt-hours per mile Wh/mi Wh mi Energy per Length 0.44704 0 joule per meter J/m J m Energy per Length 1 0 inverse centimeter cm-1 Reciprocal length 100 0 inverse meter m-1 Reciprocal length 1 0 lux Lx Illuminance 1 0 watt per square meter W/m2 W m2 Irradiance 1 0 count count Quantity 1 0 Ampere hour Ah Electric Charge 3600 0 coulomb C Electric Charge 1 0 kilogram per second kg/s kg s Mass Flow Rate 1 0 long ton per day lton/d lton d Mass Flow Rate 0.011759802 0 million pound per day MMlb/d MMlb d Mass Flow Rate 5.24991169 0 short ton per day ston/d ston d Mass Flow Rate 0.010499823 0 thousand pound per day klb/d klb d Mass Flow Rate 0.005249912 0 gram per second g/s g s Mass Flow Rate 0.001 0 pound per second lb/s lb s Mass Flow Rate 0.45359237 0 tonne per day t/d t d Mass Flow Rate 0.011574074 0 long ton lton Mass 1016.046909 0 million pound MM lb Mass 453592.37 0 ounce oz Mass 0.028349523 0 short ton ston Mass 907.18474 0 thousand pound klb Mass 453.59237 0 ton ton Mass 907.18474 0 gram g Mass 0.001 0 milligram mg Mass 1.00E-06 0 pound lb Mass 0.45359237 0 tonne t Mass 1000 0 kilogram kg Mass 1 0 second s Time 1 0 hour h Time 3600 0 day d Time 86400 0 month month Time 2628000 0 week week Time 604800 0 year yr Time 31536000 0 minute min Time 60 0 dyne dyne Force 1.00E-05 0 kilogram-force kgf Force 9.80665 0 pound-force lbf Force 4.448221615 0 newton N Force 1 0 watt W Power 1 0 million British thermal unit per day MM Btu/d Btu d Power 12211.29459 0 million British thermal unit per hour MM Btu/h Btu h Power 293071.0702 0 gigawatt GW Power 1000000000 0 megawatt MW Power 1000000 0 British thermal unit per hour Btu/h Btu h Power 0.29307107 0 calorie per second cal/s cal s Power 4.1868 0 horsepower hp Power 745.6998716 0 joule per second J/s J s Power 1 0 kilowatt kW Power 1000 0 megajoule per hour MJ/h MJ h Power 277.7777778 0 million calorie per hour MMcal/h MMcal h Power 1163 0 mole per second mol/s mol s Molar Flow Rate 1 0 gram mole per second gmol/s gmol s Molar Flow Rate 1 0 kilogram mole per second kmol/s kmol s Molar Flow Rate 1000 0 pound mole per second lbmol/s lbmol s Molar Flow Rate 453.59237 0 meter m Length 1 0 centimeter cm Length 0.01 0 inch in Length 0.0254 0 International nautical mile nmi Length 1852 0 kilometer km Length 1000 0 millimeter mm Length 0.001 0 foot ft Length 0.3048 0 mile mi Length 1609.344 0 sixteenth of an inch sxi Length 0.0015875 0 yard yd Length 0.9144 0 candela cd Luminous Intensity 1 0 meter per second m/s m s Speed 1 0 centimeter per second cm/s cm s Speed 0.01 0 foot per second ft/s ft s Speed 0.3048 0 International nautical mile per hour nmi/h nmi h Speed 0.514444444 0 kilometer per hour km/h km h Speed 0.277777778 0 mile per hour mi/h mi h Speed 0.44704 0 revolution per minute rpm Angular Velocity 0.104719755 0 radian per second rad/s rad s Angular Velocity 1 0 barrel per day bbl/d bbl d Volume Flow Rate 1.84E-06 0 cubic centimeter per second cm3/s cm3 s Volume Flow Rate 1.00E-06 0 cubic foot per second ft3/s ft3 s Volume Flow Rate 0.028316847 0 cubic meter per hour m3/h m3 h Volume Flow Rate 0.000277778 0 Imperial gallon per minute Imp gal/min gal min Volume Flow Rate 7.58E-05 0 liter per second L/s L s Volume Flow Rate 0.001 0 US gallon per minute US gal/min gal min Volume Flow Rate 6.31E-05 0 cubic meter per second m3/s m3 s Volume Flow Rate 1 0 pascal Pa Pressure 1 0 atmosphere atm Pressure 101325 0 bar bar Pressure 100000 0 inches of mercury inHg Pressure 3386.388158 0 kilogram-force per square centimeter kgf/cm2 kgf cm2 Pressure 98066.5 0 kilogram-force per square meter kgf/m2 kgf m2 Pressure 9.80665 0 kilopascal kPa Pressure 1000 0 millimeter of mercury mmHg Pressure 133.3223684 0 newton per square meter N/m2 N m2 Pressure 1 0 pound-force per square inch psi Pressure 6894.757293 0 pound-force per square inch (customary) psia Pressure 6894.757293 0 torr torr Pressure 133.3223684 0 square meter m2 Area 1 0 square foot ft2 Area 0.09290304 0 acre acre Area 4046.856422 0 square mile mi2 Area 2589988.11 0 square yard yd2 Area 0.83612736 0 hectare ha Area 10000 0 square centimeter cm2 Area 0.0001 0 square inch in2 Area 0.00064516 0 square kilometer km2 Area 1000000 0 square millimeter mm2 Area 1.00E-06 0 yobibyte YiB Computer Storage 1.21E+24 0 zebibyte ZiB Computer Storage 1.18E+21 0 exbibyte EiB Computer Storage 1.15E+18 0 pebibyte PiB Computer Storage 1.13E+15 0 tebibyte TiB Computer Storage 1.10E+12 0 gibibyte GiB Computer Storage 1073741824 0 mebibyte MiB Computer Storage 1048576 0 kibibyte KiB Computer Storage 1024 0 yottabyte YB Computer Storage 1.00E+24 0 zettabyte ZB Computer Storage 1.00E+21 0 exabyte EB Computer Storage 1.00E+18 0 petabyte PB Computer Storage 1.00E+15 0 terabyte TB Computer Storage 1.00E+12 0 gigabyte GB Computer Storage 1000000000 0 megabyte MB Computer Storage 1000000 0 kilobyte kB Computer Storage 1000 0 byte B Computer Storage 1 0 kelvin K Temperature 1 0 degree Celsius ??C Temperature 1 273.15 degree Rankine ??R Temperature 0.555555556 -2.56E-13 degree Fahrenheit ??F Temperature 0.555555556 255.3722222 milliampere mA Electric Current 0.001 0 ampere A Electric Current 1 0 joule per gram J/g J g Specific Energy 1000 0 joule per kilogram J/kg J kg Specific Energy 1 0 British thermal unit per pound Btu/lb Btu lb Specific Energy 2326 0 kilocalorie per kilogram kcal/kg kcal kg Specific Energy 4186.8 0 kilojoule per kilogram kJ/kg kJ kg Specific Energy 1000 0 kilojoule per pound kJ/lb kJ lb Specific Energy 2204.622622 0 British thermal unit per degree Rankine Btu/??R Btu ??R Entropy and Heat Capacity 1899.100535 0 British thermal unit per degree Fahrenheit Btu/??F Btu ??F Entropy and Heat Capacity 1899.100535 0 kilojoule per kelvin kJ/K kJ K Entropy and Heat Capacity 1000 0 joule per kelvin J/K J K Entropy and Heat Capacity 1 0 cubic foot per pound ft3/lb ft3 lb Specific Volume 0.062427961 0 cubic centimeter per gram cm3/g cm3 g Specific Volume 0.001 0 cubic meter per kilogram m3/kg m3 kg Specific Volume 1 0 hertz Hz Frequency 1 0 mole mol Amount of Substance 1 0 gram mole gmol Amount of Substance 1 0 kilogram mole kmol Amount of Substance 1000 0 pound mole lbmol Amount of Substance 453.59237 0 percent % Ratio 1 0 parts per billion ppb Ratio 1.00E-07 0 parts per million ppm Ratio 0.0001 0 ohm ?? Electric Resistance 1 0 gram per gram mole g/gmol g gmol Molecular Weight 0.001 0 pound per pound mole lb/lbmol lb lbmol Molecular Weight 0.001 0 kilogram per mole kg/mol kg mol Molecular Weight 1 0 kilogram per kilogram mole kg/kmol kg kmol Molecular Weight 0.001 0 British thermal unit per pound degree Rankine Btu/(lb Btu (lb ??R) Specific Entropy and Specific Heat Capacity 4186.8 0 British thermal unit per pound degree Fahrenheit Btu/(lb Btu (lb ??F) Specific Entropy and Specific Heat Capacity 4186.8 0 joule per gram kelvin J/(g J (g K) Specific Entropy and Specific Heat Capacity 1000 0 kilojoule per kilogram kelvin kJ/(kg kJ (kg K) Specific Entropy and Specific Heat Capacity 1000 0 joule per kilogram kelvin J/(kg J (kg K) Specific Entropy and Specific Heat Capacity 1 0 kilovolt kV Electric Potential 1000 0 millivolt mV Electric Potential 0.001 0 megavolt MV Electric Potential 1000000 0 volt V Electric Potential 1 0 joule J Energy 1 0 gigawatt hour GWh Energy 3.60E+12 0 megawatt hour MWh Energy 3600000000 0 watt hour Wh Energy 3600 0 British thermal unit Btu Energy 1055.055853 0 calorie cal Energy 4.1868 0 gigajoule GJ Energy 1000000000 0 kilojoule kJ Energy 1000 0 kilowatt hour kWh Energy 3600000 0 megajoule MJ Energy 1000000 0 watt second Ws Energy 1 0 kilocalorie kcal Energy 4186.8 0 million calorie MMcal Energy 4186800 0 million British thermal unit MM Btu Energy 1055055853 0 acre foot acre ft Volume 1233.481838 0 million imperial gallon Imp Mgal Volume 4546.09 0 thousand imperial gallon Imp kgal Volume 4.54609 0 barrel bbl Volume 0.158987295 0 Imperial gallon Imp gal Volume 0.00454609 0 million US gallon US Mgal Volume 3785.411784 0 thousand US gallon US kgal Volume 3.785411784 0 cubic centimeter cm3 Volume 1.00E-06 0 cubic foot ft3 Volume 0.028316847 0 kiloliter kL Volume 1 0 liter L Volume 0.001 0 megaliter M L Volume 1000 0 milliliter mL Volume 1.00E-06 0 thousand cubic meter k m3 Volume 1000 0 US gallon US gal Volume 0.003785412 0 million barrel MMbbl Volume 158987.2949 0 thousand barrel kbbl Volume 158.9872949 0 cubic meter m3 Volume 1 0 kilogram per cubic meter kg/m3 kg m3 Density 1 0 gram per liter g/L g L Density 1 0 kilogram per liter kg/L kg L Density 1000 0 pound per barrel lb/bbl lb bbl Density 2.853010174 0 pound per cubic foot lb/ft3 lb ft3 Density 16.01846337 0 pound per US gallon lb/US lb US gal Density 119.8264273 0 tonne per cubic meter t/m3 t m3 Density 1000 0 radian rad Plane Angle 1 0 degree ?? Plane Angle 0.017453293 0 revolution r Plane Angle 6.283185307 0 pascal second Pa*s Dynamic Viscosity 1 0 poise P Dynamic Viscosity 0.1 0 delta degree Fahrenheit delta ??F Temperature (Delta) 0.555555556 0 delta degree Rankine delta ??R Temperature (Delta) 0.555555556 0 delta kelvin delta K Temperature (Delta) 1 0 delta degree Celsius delta ??C Temperature (Delta) 1 0"
                                           },
    "content/sds/uom/supported-uom-quantities.html":  {
                                                          "href":  "content/sds/uom/supported-uom-quantities.html",
                                                          "title":  "Supported quantities",
                                                          "keywords":  "Supported quantities The following table shows supported quantities and their base unit of measures. Supported quantities are read-only. Quantity Id Base Uom Id Acceleration meter per square second Angular Velocity radian per second Area square meter Computer Storage byte Conductivity siemens per meter Density kilogram per cubic meter Dynamic Viscosity pascal second Electric Charge coulomb Electric Current ampere Electric Potential volt Electric Resistance ohm Energy joule Energy per Length joule per meter Entropy and Heat Capacity joule per kelvin Force newton Frequency hertz Illuminance lux Irradiance watt per square meter Length meter Luminous Intensity candela Mass kilogram Mass Flow Rate kilogram per second Molar Flow Rate mole per second Molecular Weight kilogram per mole Amount of Substance mole Plane Angle radian Power watt Pressure pascal Quantity count Quantity per Volume count per cubic meter Ratio percent Reciprocal length inverse meter Specific Energy joule per kilogram Specific Entropy and Specific Heat Capacity joule per kilogram kelvin Specific Volume cubic meter per kilogram Speed meter per second Temperature kelvin Temperature (Delta) delta kelvin Time second Volume cubic meter Volume Flow Rate cubic meter per second"
                                                      },
    "content/sds/views/get-started-with-stream-views.html":  {
                                                                 "href":  "content/sds/views/get-started-with-stream-views.html",
                                                                 "title":  "Get started with SdsStreamViews",
                                                                 "keywords":  "Get started with SdsStreamViews To work with stream views, you first need to have types, streams, and streams data defined. The following is a simplified procedure for working with a stream view. For code examples, see Work with SdsStreamViews outside of .NET framework and Work with SdsStreamViews in .NET framework . Create a type that will be the source type. Create a stream that is of the type defined in the previous step. Write data into the stream that was created in the previous step. Read data from the stream to verify. Create another type that will be the target type. Create a stream view using the source type (step 1) and the target type (step 5). The mapping between the source and the target type happens automatically if you do not specify it in SdsStreamViewProperty . Get SdsStreamViewMap to see how properties are mapped. Read data from the stream with the stream view to verify. For more information, see Reading with SdsStreamViews . Work with SdsStreamViews outside of .NET framework When working with stream views, invoke HTTP directly or use some of the sample code. Both Python and JavaScript use SdsStreamView definitions. The following JSON is a simple mapping between a source type with identifier Simple and a target type with identifier Simple1 . { \"Id\":\"StreamView\", \"Name\":\"StreamView\", \"SourceTypeId\":\"Simple\", \"TargetTypeId\":\"Simple1\" } The following is the corresponding SdsStreamViewMap . { \"SourceTypeId\":\"Simple\", \"TargetTypeId\":\"Simple1\", \"Properties\":[ { \"SourceId\":\"Time\", \"TargetId\":\"Time\" }, { \"SourceId\":\"State\", \"TargetId\":\"State\" }, { \"SourceId\":\"Measurement\", \"TargetId\":\"Value\", \"Mode\":4 } ] } Work with SdsStreamViews in .NET framework When working in .NET, use the SDS client libraries method ISdsMetadataService . Given the following: public enum State { Ok, Warning, Alarm } public class Simple { [SdsMember(IsKey = true, Order = 0)] public DateTime Time { get; set; } public State State { get; set; } public double Measurement { get; set; } } SdsType simpleType = SdsTypeBuilder.CreateSdsType\u003cSimple\u003e(); simpleType.Id = \"Simple\"; simpleType.Name = \"Simple\"; await config.GetOrCreateTypeAsync(simpleType); SdsStream simpleStream = await config.GetOrCreateStreamAsync(new SdsStream() { Id = \"Simple\", Name = \"Simple\", TypeId = simpleType.Id }); DateTime start = new DateTime(2017, 4, 1).ToUniversalTime(); for (int i = 0; i \u003c 10; i++) { Simple value = new Simple() { Time = start + TimeSpan.FromMinutes(i), State = State.Warning, Measurement = i }; await client.InsertValueAsync(simpleStream.Id, value); } IEnumerable\u003cSimple\u003e simpleValues = await client.GetWindowValuesAsync\u003cSimple\u003e(simpleStream.Id, start.ToString(\"o\"), start.Add(TimeSpan.FromMinutes(10)).ToString(\"o\")); foreach (Simple value in simpleValues) Console.WriteLine($\"{value.Time}: {value.State}, {value.Measurement}\"); //    The example displays the following output: //    4 /   1 /   2017 7:00:00 AM: Warning, 0 //    4 /   1 /   2017 7:01:00 AM: Warning, 1 //    4 /   1 /   2017 7:02:00 AM: Warning, 2 //    4 /   1 /   2017 7:03:00 AM: Warning, 3 //    4 /   1 /   2017 7:04:00 AM: Warning, 4 //    4 /   1 /   2017 7:05:00 AM: Warning, 5 //    4 /   1 /   2017 7:06:00 AM: Warning, 6 //    4 /   1 /   2017 7:07:00 AM: Warning, 7 //    4 /   1 /   2017 7:08:00 AM: Warning, 8 //    4 /   1 /   2017 7:09:00 AM: Warning, 9 To map the Measurement property to a property in the same location of the same type, allow SDS to automatically determine mapping. public class Simple1 { [SdsMember(IsKey = true, Order = 0)] public DateTime Time { get; set; } public State State { get; set; } public double Value { get; set; } } SdsType simple1Type = SdsTypeBuilder.CreateSdsType\u003cSimple1\u003e(); simple1Type.Id = \"Simple1\"; simple1Type.Name = \"Simple1\"; simple1Type = await config.GetOrCreateTypeAsync(simple1Type); SdsStreamView view = new SdsStreamView() { Id = \"StreamView\", Name = \"StreamView\", SourceTypeId = simpleType.Id, TargetTypeId = simple1Type.Id, }; view = await config.GetOrCreateStreamViewAsync(view); SdsStreamViewMap map = await config.GetStreamViewMapAsync(view.Id); Console.WriteLine($\"{map.SourceTypeId} to {map.TargetTypeId}\"); for (int i = 0; i \u003c map.Properties.Count; i++) Console.WriteLine($\"\\t{i}) {map.Properties[i].SourceId} to {map.Properties[i].TargetId} - {map.Properties[i].Mode}\"); Console.WriteLine(); IEnumerable\u003cSimple1\u003e simple1Values = await client.GetWindowValuesAsync\u003cSimple1\u003e(simpleStream.Id, start.ToString(\"o\"), start.Add(TimeSpan.FromMinutes(10)).ToString(\"o\"), view.Id); foreach (Simple1 value in simple1Values) Console.WriteLine($\"{value.Time}: {value.State}, {value.Value}\"); //    The example displays the following output: //    Simple to Simple1 //    0) Time to Time - None //    1) State to State - None //    2) Measurement to Value - FieldRename //    //    4 /   1 /   2017 7:00:00 AM: Warning, 0 //    4 /   1 /   2017 7:01:00 AM: Warning, 1 //    4 /   1 /   2017 7:02:00 AM: Warning, 2 //    4 /   1 /   2017 7:03:00 AM: Warning, 3 //    4 /   1 /   2017 7:04:00 AM: Warning, 4 //    4 /   1 /   2017 7:05:00 AM: Warning, 5 //    4 /   1 /   2017 7:06:00 AM: Warning, 6 //    4 /   1 /   2017 7:07:00 AM: Warning, 7 //    4 /   1 /   2017 7:08:00 AM: Warning, 8 //    4 /   1 /   2017 7:09:00 AM: Warning, 9 The SdsStreamViewMap shows that SDS was able to determine that mapping from Measurement to Value resulted in renaming. SDS can also determine property mapping of the same name but different type. Note that the location of the Measurement property is different, yet it is still mapped. public class Simple2 { [SdsMember(IsKey = true, Order = 0)] public DateTime Time { get; set; } public int Measurement { get; set; } public State State { get; set; } } SdsType simple2Type = SdsTypeBuilder.CreateSdsType\u003cSimple2\u003e(); simple2Type.Id = \"Simple2\"; simple2Type.Name = \"Simple2\"; simple2Type = await config.GetOrCreateTypeAsync(simple2Type); SdsStreamView view = new SdsStreamView() { Id = \"StreamView1\", Name = \"StreamView1\", SourceTypeId = simpleType.Id, TargetTypeId = simple2Type.Id, }; view = await config.GetOrCreateStreamViewAsync(view); SdsStreamViewMap map = await config.GetStreamViewMapAsync(view.Id); Console.WriteLine($\"{map.SourceTypeId} to {map.TargetTypeId}\"); for (int i = 0; i \u003c map.Properties.Count; i++) Console.WriteLine($\"\\t{i}) {map.Properties[i].SourceId} to {map.Properties[i].TargetId} - {map.Properties[i].Mode}\"); Console.WriteLine(); IEnumerable\u003cSimple2\u003e simple2Values = await client.GetWindowValuesAsync\u003cSimple2\u003e(simpleStream.Id, start.ToString(\"o\"), start.Add(TimeSpan.FromMinutes(10)).ToString(\"o\"), view.Id); foreach (Simple2 value in simple2Values) Console.WriteLine($\"{value.Time}: {value.State}, {value.Measurement}\"); //The   The example displays the following output: //    Simple to Simple2 //    0) Time to Time - None //    1) State to State - None //    2) Measurement to Measurement - FieldConversion //    //    4 /   1 /   2017 7:00:00 AM: Warning, 0 //    4 /   1 /   2017 7:01:00 AM: Warning, 1 //    4 /   1 /   2017 7:02:00 AM: Warning, 2 //    4 /   1 /   2017 7:03:00 AM: Warning, 3 //    4 /   1 /   2017 7:04:00 AM: Warning, 4 //    4 /   1 /   2017 7:05:00 AM: Warning, 5 //    4 /   1 /   2017 7:06:00 AM: Warning, 6 //    4 /   1 /   2017 7:07:00 AM: Warning, 7 //    4 /   1 /   2017 7:08:00 AM: Warning, 8 //    4 /   1 /   2017 7:09:00 AM: Warning, 9 The SdsStreamViewMap shows that the source Measurement floating point is converted to integer in the target. When neither the field name, the field type, or location matches, SDS does not determine mapping. The source is eliminated and the target is added and assigned the default value. public class Simple3 { [SdsMember(IsKey = true, Order = 0)] public DateTime Time { get; set; } public State State { get; set; } public int Value { get; set; } } SdsType simple3Type = SdsTypeBuilder.CreateSdsType\u003cSimple3\u003e(); simple3Type.Id = \"Simple3\"; simple3Type.Name = \"Simple3\"; simple3Type = await config.GetOrCreateTypeAsync(simple3Type); SdsStreamView view = new SdsStreamView() { Id = \"StreamView2\", Name = \"StreamView2\", SourceTypeId = simpleType.Id, TargetTypeId = simple3Type.Id, }; view = await config.GetOrCreateStreamViewAsync(view); SdsStreamViewMap map = await config.GetStreamViewMapAsync(view.Id); Console.WriteLine($\"{map.SourceTypeId} to {map.TargetTypeId}\"); for (int i = 0; i \u003c map.Properties.Count; i++) Console.WriteLine($\"\\t{i}) {map.Properties[i].SourceId} to {map.Properties[i].TargetId} - {map.Properties[i].Mode}\"); Console.WriteLine(); IEnumerable\u003cSimple3\u003e simple3Values = await client.GetWindowValuesAsync\u003cSimple3\u003e(simpleStream.Id, start.ToString(\"o\"), start.Add(TimeSpan.FromMinutes(10)).ToString(\"o\"), view.Id); foreach (Simple3 value in simple3Values) Console.WriteLine($\"{value.Time}: {value.State}, {value.Value}\"); //The   The example displays the following output: //    Simple to Simple3 //    0) Time to Time - None //    1) State to State - None //    2) Measurement to -FieldRemove //    3) to Value -FieldAdd //    //    4 /   1 /   2017 7:00:00 AM: Warning, 0 //    4 /   1 /   2017 7:01:00 AM: Warning, 0 //    4 /   1 /   2017 7:02:00 AM: Warning, 0 //    4 /   1 /   2017 7:03:00 AM: Warning, 0 //    4 /   1 /   2017 7:04:00 AM: Warning, 0 //    4 /   1 /   2017 7:05:00 AM: Warning, 0 //    4 /   1 /   2017 7:06:00 AM: Warning, 0 //    4 /   1 /   2017 7:07:00 AM: Warning, 0 //    4 /   1 /   2017 7:08:00 AM: Warning, 0 //    4 /   1 /   2017 7:09:00 AM: Warning, 0 To map when SDS cannot determine mapping, use SdsStreamView Properties . SdsStreamView view = new SdsStreamView() { Id = \"SteamView3\", Name = \"StreamView3\", SourceTypeId = simpleType.Id, TargetTypeId = simple3Type.Id, Properties = new List\u003cSdsStreamViewProperty\u003e() { new SdsStreamViewProperty() { SourceId = \"Time\", TargetId = \"Time\" }, new SdsStreamViewProperty() { SourceId = \"State\", TargetId = \"State\" }, new SdsStreamViewProperty() { SourceId = \"Measurement\", TargetId = \"Value\" } } }; view = await config.GetOrCreateStreamViewAsync(view); SdsStreamViewMap map = await config.GetStreamViewMapAsync(view.Id); Console.WriteLine($\"{map.SourceTypeId} to {map.TargetTypeId}\"); for (int i = 0; i \u003c map.Properties.Count; i++) Console.WriteLine($\"\\t{i}) {map.Properties[i].SourceId} to {map.Properties[i].TargetId} - {map.Properties[i].Mode}\"); Console.WriteLine(); IEnumerable\u003cSimple3\u003e simple3Values = await client.GetWindowValuesAsync\u003cSimple3\u003e(simpleStream.Id, start.ToString(\"o\"), start.Add(TimeSpan.FromMinutes(10)).ToString(\"o\"), view.Id); foreach (Simple3 value in simple3Values) Console.WriteLine($\"{value.Time}: {value.State}, {value.Value}\"); //The   The example displays the following output: //    Simple to Simple3 //    0) Time to Time - None //    1) State to State - None //    2) Measurement to Value - FieldRename, FieldConversion //    //    4 /   1 /   2017 7:00:00 AM: Warning, 0 //    4 /   1 /   2017 7:01:00 AM: Warning, 1 //    4 /   1 /   2017 7:02:00 AM: Warning, 2 //    4 /   1 /   2017 7:03:00 AM: Warning, 3 //    4 /   1 /   2017 7:04:00 AM: Warning, 4 //    4 /   1 /   2017 7:05:00 AM: Warning, 5 //    4 /   1 /   2017 7:06:00 AM: Warning, 6 //    4 /   1 /   2017 7:07:00 AM: Warning, 7 //    4 /   1 /   2017 7:08:00 AM: Warning, 8 //    4 /   1 /   2017 7:09:00 AM: Warning, 9"
                                                             },
    "content/sds/views/sds-stream-view-api.html":  {
                                                       "href":  "content/sds/views/sds-stream-view-api.html",
                                                       "title":  "SdsStreamView API",
                                                       "keywords":  "SdsStreamView API The REST APIs provide programmatic access to read and write SDS data. The APIs in this section interact with SdsStreamViews. For general SdsStreamView information, see Stream views . List Stream Views Returns a list of SdsStreamView Request GET /api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/StreamViews?query={query}\u0026skip={skip}\u0026count={count}\u0026orderby={orderby}  api v1 Tenants {tenantId} Namespaces {namespaceId} StreamViews?query={query}\u0026skip={skip}\u0026count={count}\u0026orderby={orderby} Parameters string tenantId Tenant identifier string namespaceId Namespace identifier [Optional] string query Query identifier. See Search in SDS for information about specifying the search parameter. [Optional] int skip Parameter representing the zero-based offset of the first object to retrieve. If unspecified, a default value of 0 is used. [Optional] int count Parameter representing the maximum number of objects to retrieve. If unspecified, a default value of 100 is used. [Optional] string orderby Parameter representing sorted order. A field name is required. The sorting is based on the stored values for a given field (of type string ). For example, orderby=name would sort the returned results by the name values (ascending by default). Additionally, a value can be provided along with the field name to identify whether to sort ascending or descending, by using values asc or desc , respectively. For example, orderby=name desc sorts the returned results by descending name values. If you do not specify a value, the results are not sorted. Response Status Code Body Type Description 200 SdsStreamView [] Returns a list of SdsStreamView objects 400 ErrorResponseBody Missing or invalid inputs 401 ErrorResponseBody Unauthorized 403 ErrorResponseBody Forbidden 404 ErrorResponseBody One of the resources specified was not found. 500 ErrorResponseBody An error occurred while processing the request. 503 ErrorResponseBody Service Unavailable Example response body 200 Response HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Id\":\"StreamView\", \"Name\":\"StreamView\", \"SourceTypeId\":\"Simple\", \"TargetTypeId\":\"Simple3\" }, { \"Id\":\"StreamViewWithProperties\", \"Name\":\"StreamViewWithProperties\", \"SourceTypeId\":\"Simple\", \"TargetTypeId\":\"Simple3\", \"Properties\":[ { \"SourceId\":\"Time\", \"TargetId\":\"Time\" }, { \"SourceId\":\"State\", \"TargetId\":\"State\" }, { \"SourceId\":\"Measurement\", \"TargetId\":\"Value\" } ] } ] 400 Response ( ErrorResponseBody ) { \"OperationId\": \"string\", \"Error\": \"string\", \"Reason\": \"string\", \"Resolution\": \"string\", \"Parameters\": { \"property1\": \"string\", \"property2\": \"string\" } } Get Stream View Returns the SdsStreamView Request GET /api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/StreamViews/{streamViewId}  api v1 Tenants {tenantId} Namespaces {namespaceId} StreamViews {streamViewId} Parameters string tenantId Tenant identifier string namespaceId Namespace identifier string streamViewId Stream view identifier Response Status Code Body Type Description 200 SdsStreamView Returns the SdsStreamView 400 ErrorResponseBody Missing or invalid inputs 401 ErrorResponseBody Unauthorized 403 ErrorResponseBody Forbidden 404 ErrorResponseBody One of the resources specified was not found. 500 ErrorResponseBody An error occurred while processing the request. 503 ErrorResponseBody Service Unavailable Example response body 200 Response HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json { \"Id\":\"StreamView\", \"Name\":\"StreamView\", \"SourceTypeId\":\"Simple\", \"TargetTypeId\":\"Simple3\", \"Properties\":[ { \"SourceId\":\"Time\", \"TargetId\":\"Time\" }, { \"SourceId\":\"State\", \"TargetId\":\"State\" }, { \"SourceId\":\"Measurement\", \"TargetId\":\"Value\" } ] } 400 Response ( ErrorResponseBody ) { \"OperationId\": \"string\", \"Error\": \"string\", \"Reason\": \"string\", \"Resolution\": \"string\", \"Parameters\": { \"property1\": \"string\", \"property2\": \"string\" } } Get Or Create Stream View If an SdsStreamView with a matching identifier already exists, the stream view passed in is compared with the existing stream view. If the stream views are identical, a Found (302) status is returned. If the stream views are different, the Conflict (409) error is returned. If no matching identifier is found, the specified stream view is created. Request POST /api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/StreamViews/{streamViewId}  api v1 Tenants {tenantId} Namespaces {namespaceId} StreamViews {streamViewId} Parameters string tenantId Tenant identifier string namespaceId Namespace identifier string streamViewId Stream view identifier Response Status Code Body Type Description 200 SdsStreamView Returns the SdsStreamView 302 SdsStreamView Found a Stream View 400 ErrorResponseBody Missing or invalid inputs 401 ErrorResponseBody Unauthorized 403 ErrorResponseBody Forbidden 404 ErrorResponseBody One of the resources specified was not found. 409 ErrorResponseBody Conflict 500 ErrorResponseBody An error occurred while processing the request. 503 ErrorResponseBody Service Unavailable Example response body 200 Response ( SdsStreamView ) { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SourceTypeId\": \"string\", \"TargetTypeId\": \"string\", \"Properties\": [ { \"SourceId\": \"string\", \"TargetId\": \"string\", \"SdsStreamView\": { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SourceTypeId\": \"string\", \"TargetTypeId\": \"string\", \"Properties\": [ null ] } } ] } Create Or Update Stream View Creates or updates the definition of a SdsStreamView Request PUT /api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/StreamViews/{streamViewId}  api v1 Tenants {tenantId} Namespaces {namespaceId} StreamViews {streamViewId} Parameters string tenantId Tenant identifier string namespaceId Namespace identifier string streamViewId Stream view identifier Response Status Code Body Type Description 200 SdsStreamView Returns the SdsStreamView 400 ErrorResponseBody Missing or invalid inputs 401 ErrorResponseBody Unauthorized 403 ErrorResponseBody Forbidden 404 ErrorResponseBody One of the resources specified was not found. 409 ErrorResponseBody Conflict 500 ErrorResponseBody An error occurred while processing the request. 503 ErrorResponseBody Service Unavailable Example response body 200 Response ( SdsStreamView ) { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SourceTypeId\": \"string\", \"TargetTypeId\": \"string\", \"Properties\": [ { \"SourceId\": \"string\", \"TargetId\": \"string\", \"SdsStreamView\": { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SourceTypeId\": \"string\", \"TargetTypeId\": \"string\", \"Properties\": [ null ] } } ] } Delete Stream View Deletes a stream view from the specified tenant and namespace Request DELETE /api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/StreamViews/{streamViewId}  api v1 Tenants {tenantId} Namespaces {namespaceId} StreamViews {streamViewId} Parameters string tenantId Tenant identifier string namespaceId Namespace identifier string streamViewId Stream view identifier Response Status Code Body Type Description 204 None SdsStreamView was successfully deleted. 400 ErrorResponseBody Missing or invalid inputs 401 ErrorResponseBody Unauthorized 403 ErrorResponseBody Forbidden 404 ErrorResponseBody One of the resources specified was not found. 500 ErrorResponseBody An error occurred while processing the request. 503 ErrorResponseBody Service Unavailable Example response body 400 Response ( ErrorResponseBody ) { \"OperationId\": \"string\", \"Error\": \"string\", \"Reason\": \"string\", \"Resolution\": \"string\", \"Parameters\": { \"property1\": \"string\", \"property2\": \"string\" } } Get Stream View Map Returns the SdsStreamViewMap corresponding to the specified streamViewId Request GET /api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/StreamViews/{streamViewId}/Map  api v1 Tenants {tenantId} Namespaces {namespaceId} StreamViews {streamViewId} Map Parameters string tenantId Tenant identifier string namespaceId Namespace identifier string streamViewId Stream view identifier Response Status Code Body Type Description 200 SdsStreamViewMap Returns the SdsStreamViewMap 400 ErrorResponseBody Missing or invalid inputs 401 ErrorResponseBody Unauthorized 403 ErrorResponseBody Forbidden 404 ErrorResponseBody One of the resources specified was not found. 500 ErrorResponseBody An error occurred while processing the request. 503 ErrorResponseBody Service Unavailable Example response body 200 Response HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json { \"SourceTypeId\": \"Simple\", \"TargetTypeId\": \"Simple3\", \"Properties\": [ { \"SourceId\": \"Time\", \"TargetId\": \"Time\" }, { \"SourceId\": \"Measurement\", \"TargetId\": \"Value\", \"Mode\": 20 }, { \"SourceId\": \"State\", \"Mode\": 2 }, { \"TargetId\": \"State\", \"Mode\": 1 } ] } 400 Response ( ErrorResponseBody ) { \"OperationId\": \"string\", \"Error\": \"string\", \"Reason\": \"string\", \"Resolution\": \"string\", \"Parameters\": { \"property1\": \"string\", \"property2\": \"string\" } } Definitions SdsStreamView A contract defining the stream view Properties Property Name Data Type Required Nullable Description Id string false true A unique identifier for the SdsStreamView Name string false true An optional user-friendly name for the SdsStreamView Description string false true A brief description of the SdsStreamView SourceTypeId string false true Identifier of the SdsType of the SdsStream TargetTypeId string false true Identifier of the SdsType to convert events to Properties [ SdsStreamViewProperty ] false true List of SdsStreamViewProperty { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SourceTypeId\": \"string\", \"TargetTypeId\": \"string\", \"Properties\": [ { \"SourceId\": \"string\", \"TargetId\": \"string\", \"SdsStreamView\": { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SourceTypeId\": \"string\", \"TargetTypeId\": \"string\", \"Properties\": [ null ] } } ] } SdsStreamViewProperty A contract defining the stream view property Properties Property Name Data Type Required Nullable Description SourceId string false true Identifier of the SdsTypeProperty from the source SdsType Properties list TargetId string false true Identifier of the SdsTypeProperty from the target SdsType Properties list SdsStreamView SdsStreamView false true Additional mapping instructions for derived types { \"SourceId\": \"string\", \"TargetId\": \"string\", \"SdsStreamView\": { \"Id\": \"string\", \"Name\": \"string\", \"Description\": \"string\", \"SourceTypeId\": \"string\", \"TargetTypeId\": \"string\", \"Properties\": [ { \"SourceId\": \"string\", \"TargetId\": \"string\", \"SdsStreamView\": null } ] } } ErrorResponseBody Contains the error message format that follows the AVEVA Data Hub error standards Properties Property Name Data Type Required Nullable Description OperationId string false true Operation unique identifier of action that caused the error Error string false true Error description Reason string false true Reason for the error Resolution string false true Resolution to resolve the error Parameters object false true IDs or values that are creating or are affected by the error { \"OperationId\": \"string\", \"Error\": \"string\", \"Reason\": \"string\", \"Resolution\": \"string\", \"Parameters\": { \"property1\": \"string\", \"property2\": \"string\" } } SdsStreamViewMap A contract defining the stream view map Properties Property Name Data Type Required Nullable Description SourceTypeId string false true Identifier of the SdsType of the SdsStream TargetTypeId string false true Identifier of the SdsType to convert events to Properties [ SdsStreamViewMapProperty ] false true Identifier of the SdsStreamViewMapProperty from the target SdsType Properties list { \"SourceTypeId\": \"string\", \"TargetTypeId\": \"string\", \"Properties\": [ { \"SourceId\": \"string\", \"TargetId\": \"string\", \"Mode\": 0, \"StreamViewMap\": { \"SourceTypeId\": \"string\", \"TargetTypeId\": \"string\", \"Properties\": [ null ] } } ] } SdsStreamViewMapProperty A contract defining stream view map property Properties Property Name Data Type Required Nullable Description SourceId string false true Identifier of the SdsType of the SdsStream TargetId string false true Identifier of the SdsType to convert events to Mode SdsStreamViewMode false false Aggregate of actions applied to the properties. SdsStreamViewMode are combined via binary arithmetic. StreamViewMap SdsStreamViewMap false true Mapping for derived types { \"SourceId\": \"string\", \"TargetId\": \"string\", \"Mode\": 0, \"StreamViewMap\": { \"SourceTypeId\": \"string\", \"TargetTypeId\": \"string\", \"Properties\": [ { \"SourceId\": \"string\", \"TargetId\": \"string\", \"Mode\": null, \"StreamViewMap\": null } ] } } SdsStreamViewMode Enumerated Values Property Value None 0 FieldAdd 1 FieldRemove 2 FieldRename 4 FieldMove 8 FieldConversion 16 InvalidFieldConversion 32"
                                                   },
    "content/sds/write-data.html":  {
                                        "href":  "content/sds/write-data.html",
                                        "title":  "Write data",
                                        "keywords":  "Write data The SDS REST APIs provide programmatic access to write data to SDS. All writes rely on a stream???s key or primary index. The primary index determines the order of events in the stream. Secondary indexes are updated, but they do not contribute to the request. All references to indexes are to the primary index. When working in .NET, convenient SDS Client libraries are available. The ISdsDataService interface, accessed using the SdsService.GetDataService() helper, defines the available functions. Single stream writes The following methods support writing a single or multiple values: Insert Values inserts a collection of events. Patch Values updates specific fields for a collection of events. Replace Values replaces a collection of events. Remove Values deletes the events based on the request parameters. Update Values add or replaces a collection of events. Request body format With the exception of Remove Values , all single stream write calls require a request body containing the events to insert or modify. The events must be formatted as a serialized JSON array of the stream\u0027s type. JSON arrays are comma-delimited lists of a type enclosed within square brackets. The following code shows a list of three WaveData events that are properly formatted for insertion. For the complete example, see the [AVEVA Data Hub-Samples]( https://github.com/osisoft/AVEVA https:  github.com osisoft AVEVA Data Hub-Samples). [ { \"Order\":2, \"Tau\":0.25722883666666846, \"Radians\":1.6162164471269089, \"Sin\":1.9979373673043652, \"Cos\":-0.090809010174665111, \"Tan\":-44.003064529862513, \"Sinh\":4.8353589272389, \"Cosh\":5.2326566823391856, \"Tanh\":1.8481468289554672 }, { \"Order\":4, \"Tau\":0.25724560000002383, \"Radians\":1.6163217742567466, \"Sin\":1.9979277915696148, \"Cos\":-0.091019446679060964, \"Tan\":-43.901119254534827, \"Sinh\":4.8359100947709592, \"Cosh\":5.233166005842703, \"Tanh\":1.8481776000882766 }, { \"Order\":6, \"Tau\":0.25724560000002383, \"Radians\":1.6163217742567466, \"Sin\":1.9979277915696148, \"Cos\":-0.091019446679060964, \"Tan\":-43.901119254534827, \"Sinh\":4.8359100947709592, \"Cosh\":5.233166005842703, \"Tanh\":1.8481776000882766 } ] You can serialize your data using one of many available JSON serializers available at Introducing JSON . Indexes SDS writes rely on the primary index for positioning within streams and locating existing events. Most writes use the index as specified by the value; however, for deletes, indexes are specified as strings in the URI, or, when using the SDS Client Libraries, the index may be passed as-is to DELETE methods that take the index type as a generic argument. For more details about working with indexes, see Indexes . To specify compound indexes in the URI, specify each field that composes the index, in the specified order, separated by the pipe character, ???|???."
                                    },
    "content/sds/write-data/response-format-write.html":  {
                                                              "href":  "content/sds/write-data/response-format-write.html",
                                                              "title":  "Response format for write APIs",
                                                              "keywords":  "Response format for write APIs The format of the response is specified in the API call. For write APIs, the supported response formats are: JSON - The default response format for SDS, which is used in all examples in this documentation. Default JSON responses do not include any values that are equal to the default value for their type. Verbose JSON - Verbose has no impact on writes because writes return only error messages. To specify verbose JSON return, add the header Accept-Verbosity with a value of verbose to the request. SDS - To specify SDS format, set the Accept header in the request to application/sds application sds ."
                                                          },
    "content/sds/write-data/writing-data-api.html":  {
                                                         "href":  "content/sds/write-data/writing-data-api.html",
                                                         "title":  "Write data API",
                                                         "keywords":  "Write data API The following example API calls show different methods for writing data. Example type, stream, and data Many of the API methods described below contain example requests and responses in JSON to highlight usage and specific behaviors. The following type, stream, and data are used in the examples. Example type SimpleType is an SdsType with a single index and two additional properties. This type is defined below in .NET, Python, and JavaScript: .NET Python JavaScript public enum State { Ok, Warning, Alarm } public class SimpleType { [SdsMember(IsKey = true, Order = 0) ] public DateTime Time { get; set; } public State State { get; set; } [SdsMember(Uom = \"meter\")] public Double Measurement { get; set; } } class State(Enum): Ok = 0 Warning = 1 Alarm = 2 class SimpleType(object): Time = property(getTime, setTime) def getTime(self): return self.__time def setTime(self, time): self.__time = time State = property(getState, setState) def getState(self): return self.__state def setState(self, state): self.__state = state Measurement = property(getValue, setValue) def getValue(self): return self.__measurement def setValue(self, measurement): self.__measurement = measurement var State = { Ok: 0, Warning: 1, Alarm: 2, } var SimpleType = function () { this.Time = null; this.State = null; this.Value = null; } Example stream Simple is an SdsStream of type SimpleType . Example data Simple has stored values as follows: 11/23/2017 11 23 2017 12:00:00 PM: Ok 0 11/23/2017 11 23 2017 1:00:00 PM: Ok 10 11/23/2017 11 23 2017 2:00:00 PM: Ok 20 11/23/2017 11 23 2017 3:00:00 PM: Ok 30 11/23/2017 11 23 2017 4:00:00 PM: Ok 40 All times are represented at offset 0, GMT. Insert Values Inserts data into the specified stream. Returns an error if data is already present at the index of any event. Request POST api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Streams/{streamId}/Data api v1 Tenants {tenantId} Namespaces {namespaceId} Streams {streamId} Data Parameters string tenantId Tenant identifier string namespaceId Namespace identifier string streamId Stream identifier Request body A serialized list of one or more events of the stream type Example request The following request is used to insert events into stream Simple of SimpleType : POST api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Streams/{streamId}/Data api v1 Tenants {tenantId} Namespaces {namespaceId} Streams {streamId} Data Example request body The request body specifies the values to insert. [ { \"Time\": \"2017-11-23T17:00:00Z\", \"State\": 0, \"Measurement\": 50 }, { \"Time\": \"2017-11-23T18:00:00Z\", \"State\": 0, \"Measurement\": 60 } ] Response Status Code Body Type Description 204 None Specified values were successfully added 400 ErrorResponseBody Missing or invalid inputs 401 ErrorResponseBody Unauthorized 403 ErrorResponseBody Forbidden 404 ErrorResponseBody One of the resources specified was not found. 409 ErrorResponseBody Conflict 500 ErrorResponseBody An error occurred while processing the request. 503 ErrorResponseBody Service Unavailable Example response body 400 Response ( ErrorResponseBody ) { \"OperationId\": \"string\", \"Error\": \"string\", \"Reason\": \"string\", \"Resolution\": \"string\", \"Parameters\": { \"property1\": \"string\", \"property2\": \"string\" } } Note: This request will return an error if an event already exists for any index in the request. If any individual index encounters a problem, the entire operation is rolled back and no insertions are made. The streamId and index that caused the issue are included in the error response. Update Values Writes one or more events to the specified stream Request PUT api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Streams/{streamId}/Data api v1 Tenants {tenantId} Namespaces {namespaceId} Streams {streamId} Data Parameters string tenantId Tenant identifier string namespaceId Namespace identifier string streamId Stream identifier Request body A serialized list of one or more events of the stream type Response Status Code Body Type Description 204 None Specified values were successfully updated 400 ErrorResponseBody Missing or invalid inputs 401 ErrorResponseBody Unauthorized 403 ErrorResponseBody Forbidden 404 ErrorResponseBody One of the resources specified was not found. 500 ErrorResponseBody An error occurred while processing the request. 503 ErrorResponseBody Service Unavailable Example response body 400 Response ( ErrorResponseBody ) { \"OperationId\": \"string\", \"Error\": \"string\", \"Reason\": \"string\", \"Resolution\": \"string\", \"Parameters\": { \"property1\": \"string\", \"property2\": \"string\" } } Note: This request performs an insert or a replace depending on whether an event already exists at the event indexes. If any item fails to write, the entire operation is rolled back and no events are written to the stream. The index that caused the issue is included in the error response. Replace Values Writes one or more events over existing events in the specified stream Request PUT api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Streams/{streamId}/Data api v1 Tenants {tenantId} Namespaces {namespaceId} Streams {streamId} Data ?allowCreate=false Parameters string tenantId Tenant identifier string namespaceId Namespace identifier string streamId Stream identifier Request Body A serialized list of one or more events of the stream type Response Status Code Body Type Description 204 None Specified values were successfully deleted 400 ErrorResponseBody Missing or invalid inputs 401 ErrorResponseBody Unauthorized 403 ErrorResponseBody Forbidden 404 ErrorResponseBody One of the resources specified was not found. 500 ErrorResponseBody An error occurred while processing the request. 503 ErrorResponseBody Service Unavailable Example response body 400 Response ( ErrorResponseBody ) { \"OperationId\": \"string\", \"Error\": \"string\", \"Reason\": \"string\", \"Resolution\": \"string\", \"Parameters\": { \"property1\": \"string\", \"property2\": \"string\" } } Note: This request returns an error if the stream does not have an event to be replaced at the specified index. If any individual event fails to be replaced, the entire operation is rolled back and no replaces are performed. The index that caused the issue and the streamId are included in the error response. Patch Values Modifies the specified stream event(s). Patching affects only the data item parameters that are included in the call. Request PATCH api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Streams/{streamId}/Data api v1 Tenants {tenantId} Namespaces {namespaceId} Streams {streamId} Data ?select={selectExpression} Parameters string tenantId Tenant identifier string namespaceId Namespace identifier string streamId Stream identifier string selectExpression Comma separated list of strings that indicates the event fields that will be changed in stream events Request body A serialized collection of one or more patch property events Response The response includes a status code Example request Let\u0027s say that you have a stream Simple of SimpleType . To change one property ( Measurement ) for one event, you can use the request with the body below: PATCH api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Streams/Simple/Data api v1 Tenants {tenantId} Namespaces {namespaceId} Streams Simple Data ?select=measurement Example request body [ { \"Time\":\"2017-11-23T12:00:00Z\", \"Measurement\":500.0 } ] This request only changes the Measurement value at the specified event index. Note: Patching is used to patch the events of the selected fields for one or more events in the stream. Only the fields indicated in selectExpression are modified. The events to be modified are indicated by the index value of each entry in the collection. If there is a problem patching any individual event, the entire operation is rolled back and the error will indicate the streamId and index of the problem. Remove Values There are two options for specifying which events to remove from a stream: Index Collection : One or more indexes can be specified in the request. Window : A window can be specified with a start index and end index. Index Collection Removes the event at each index from the specified stream. Different overloads are available to make it easier to indicate the index where you want to remove a data event. Request DELETE api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Streams/{streamId}/Data api v1 Tenants {tenantId} Namespaces {namespaceId} Streams {streamId} Data ?index={index}[\u0026index={index}???] Parameters string tenantId Tenant identifier string namespaceId Namespace identifier string streamId Stream identifier string index One or more indexes of events to remove Response Status Code Body Type Description 204 None Specified values were successfully deleted 400 ErrorResponseBody Missing or invalid inputs 401 ErrorResponseBody Unauthorized 403 ErrorResponseBody Forbidden 404 ErrorResponseBody One of the resources specified was not found. 500 ErrorResponseBody An error occurred while processing the request. 503 ErrorResponseBody Service Unavailable Example response body 400 Response ( ErrorResponseBody ) { \"OperationId\": \"string\", \"Error\": \"string\", \"Reason\": \"string\", \"Resolution\": \"string\", \"Parameters\": { \"property1\": \"string\", \"property2\": \"string\" } } Note: If any individual event fails to be removed, the entire operation is rolled back and no events are removed. The streamId and index that caused the issue are included in the error response. If you attempt to remove events at indexes that have no events, an error is returned. If this occurs, you can use Window request format to remove any events from a specified \"window\" of indexes, which will not return an error if no data is found. Window Removes events at and between the start index and end index Request DELETE api/v1/Tenants/{tenantId}/Namespaces/{namespaceId}/Streams/{streamId}/Data api v1 Tenants {tenantId} Namespaces {namespaceId} Streams {streamId} Data ?startIndex={startIndex}\u0026endIndex={endIndex} Parameters string tenantId Tenant identifier string namespaceId Namespace identifier string streamId Stream identifier string startIndex The index defining the beginning of the window string endIndex The index defining the end of the window Response Status Code Body Type Description 204 None Specified values were successfully deleted 400 ErrorResponseBody Missing or invalid inputs 401 ErrorResponseBody Unauthorized 403 ErrorResponseBody Forbidden 404 ErrorResponseBody One of the resources specified was not found. 500 ErrorResponseBody An error occurred while processing the request. 503 ErrorResponseBody Service Unavailable Example response body 400 Response ( ErrorResponseBody ) { \"OperationId\": \"string\", \"Error\": \"string\", \"Reason\": \"string\", \"Resolution\": \"string\", \"Parameters\": { \"property1\": \"string\", \"property2\": \"string\" } } Note: If any individual event fails to be removed, the entire operation is rolled back and no removes are done. Definitions ErrorResponseBody Contains the error message format that follows the AVEVA Data Hub error standards Properties Property Name Data Type Required Nullable Description OperationId string false true Operation unique identifier of action that caused the error Error string false true Error description Reason string false true Reason for the error Resolution string false true Resolution to resolve the error Parameters object false true Ids or values that are creating or are affected by the error { \"OperationId\": \"string\", \"Error\": \"string\", \"Reason\": \"string\", \"Resolution\": \"string\", \"Parameters\": { \"property1\": \"string\", \"property2\": \"string\" } }"
                                                     },
    "content/security/security-overview.html":  {
                                                    "href":  "content/security/security-overview.html",
                                                    "title":  "Security",
                                                    "keywords":  "Security Consider the following when determining Edge Data Store security practices. REST APIs EDS supports REST APIs for configuration, data reading through SDS, and data writing through OMF and SDS. EDS provides only localhost access to REST APIs, which means any code that reads or writes to the REST APIs must reside on the device on which EDS is running. To keep EDS secure, only administrators should have access to devices where EDS is installed. REST access is through HTTP. The default port is 5590. The port number can be changed during installation or configuration. URLs must be of the form http://localhost:{port}/ http:  localhost:{port}  or http://127.0.0.1:{port}/ http:  127.0.0.1:{port}  . Note: Do not use the host name or IP Address in the URL. Note: Docker users must use the \"host\" networking mode for the container. For information about using EDS with Docker, see Install Edge Data Store using Docker . Data egress Use HTTPS to write data to AVEVA Data Hub or OSIsoft PI Web API; writing to either of these destinations is not limited to the local device. EDS adapters The Modbus TCP EDS adapter and the OPC UA EDS adapter access remote data sources through binary protocols. The Modbus TCP EDS adapter does not currently support transport layer security between the adapter and the data source, which means that the Modbus traffic will be unprotected. If needed, use other measures to secure this traffic such as air-gapped control network, VPN connection, SSH tunnel, etc. Secure storage Sensitive information such as passwords and client secrets are saved in configuration files in an encrypted form. Only the EDS runtime can properly store and retrieve these protected data items. Note: Do not manually edit configuration files. Altering encrypted values will cause failures. Unencrypted values for sensitive information are only available when you provide them to the system through the REST API, such as with initial configuration or update. From that point forward, the unencrypted values are not available, either in the configuration files or through the REST API. The REST API will only return a placeholder string for such values. Use caution when submitting sensitive data items. For example, remove any temporary file containing unencrypted credentials used to submit configuration to the REST API from the system. Service and file system security The installer creates a specific user account that the Edge Data Store service runs under. This account is only used for running the service. For example, it cannot be used for interactive sessions. Do not configure this service account; modifying the service configuration in this respect could cause system failure. The EDS binary files, configuration files, and data files are configured by the installer and runtime to allow appropriate access by the service account. Do not modify the permission and ownership assignments for these files as failures could occur. Consider a third party encryption-at-rest technique for your data storage. This security measure protects your data in the case the device storage is physically stolen, lost, or otherwise falls into the wrong hands. On Linux, EDS is compatible with whole disk encryption systems such as LUKS or partial encryption systems such as eCryptfs . On Windows, EDS is compatible with whole disk encryption solutions such as BitLocker and Windows EFS ."
                                                },
    "content/shared-content/README.html":  {
                                               "href":  "content/shared-content/README.html",
                                               "title":  "content/shared-content subtree",
                                               "keywords":  "content/shared-content content shared-content subtree All content in this directory is consumed from the PI Adapter framework repository as a subtree. This repo contains shared files that are used in all adapter documentation. Within Edge Data Store, files from the PI Adapter repo are reused in the modbus and opc-ua portions of the TOC, as Modbus and OPC-UA are dependent on referencing a version of the framework. Subtree wiki article To update the subtree To update content in content/shared-content content shared-content , perform the following actions from a command prompt session: Enter the following command. git subtree pull --prefix content/shared-content content shared-content https://github.com/osisoft/PI-Adapter https:  github.com osisoft PI-Adapter main --squash Resolve any conflicts and complete the merge. Which branch of the PI Adapter framework is EDS consuming? It\u0027s currently consuming main . It should continue to consume main until another use case arises. What if I want to consume a different branch of the framework? If you want to swap out the most recent version of the PI Adapter framework for a specific version, update the branch parameter in the git subtree pull command: git subtree pull --prefix content/shared-content content shared-content https://github.com/osisoft/PI-Adapter https:  github.com osisoft PI-Adapter \u003cCUSTOM_BRANCH\u003e --squash"
                                           },
    "content/shared-content/shared-content/_includes/inline/component-id.html":  {
                                                                                     "href":  "content/shared-content/shared-content/_includes/inline/component-id.html",
                                                                                     "title":  "",
                                                                                     "keywords":  ""
                                                                                 },
    "content/shared-content/shared-content/_includes/inline/component-type.html":  {
                                                                                       "href":  "content/shared-content/shared-content/_includes/inline/component-type.html",
                                                                                       "title":  "",
                                                                                       "keywords":  ""
                                                                                   },
    "content/shared-content/shared-content/_includes/inline/docker-image.html":  {
                                                                                     "href":  "content/shared-content/shared-content/_includes/inline/docker-image.html",
                                                                                     "title":  "",
                                                                                     "keywords":  "\u003cscript_name.sh\u003e"
                                                                                 },
    "content/shared-content/shared-content/_includes/inline/framework-version.html":  {
                                                                                          "href":  "content/shared-content/shared-content/_includes/inline/framework-version.html",
                                                                                          "title":  "",
                                                                                          "keywords":  "\u003cx.x\u003e"
                                                                                      },
    "content/shared-content/shared-content/_includes/inline/installer-name.html":  {
                                                                                       "href":  "content/shared-content/shared-content/_includes/inline/installer-name.html",
                                                                                       "title":  "",
                                                                                       "keywords":  "\u003cINSTALLER_NAME\u003e"
                                                                                   },
    "content/shared-content/shared-content/_includes/inline/product-name.html":  {
                                                                                     "href":  "content/shared-content/shared-content/_includes/inline/product-name.html",
                                                                                     "title":  "",
                                                                                     "keywords":  "\u003cPRODUCT_NAME\u003e"
                                                                                 },
    "content/shared-content/shared-content/_includes/inline/product-protocol.html":  {
                                                                                         "href":  "content/shared-content/shared-content/_includes/inline/product-protocol.html",
                                                                                         "title":  "",
                                                                                         "keywords":  ""
                                                                                     },
    "content/shared-content/shared-content/_includes/inline/product-version.html":  {
                                                                                        "href":  "content/shared-content/shared-content/_includes/inline/product-version.html",
                                                                                        "title":  "",
                                                                                        "keywords":  "\u003cPRODUCT_VERSION\u003e"
                                                                                    },
    "content/shared-content/shared-content/_includes/inline/startup-script.html":  {
                                                                                       "href":  "content/shared-content/shared-content/_includes/inline/startup-script.html",
                                                                                       "title":  "",
                                                                                       "keywords":  "\u003cscript_name.sh\u003e"
                                                                                   },
    "content/shared-content/shared-content/_includes/inline/symantic-version.html":  {
                                                                                         "href":  "content/shared-content/shared-content/_includes/inline/symantic-version.html",
                                                                                         "title":  "",
                                                                                         "keywords":  "\u003cx.x.x.x\u003e"
                                                                                     },
    "content/shared-content/shared-content/configuration/buffering.html":  {
                                                                               "href":  "content/shared-content/shared-content/configuration/buffering.html",
                                                                               "title":  "Buffering",
                                                                               "keywords":  "Buffering You can configure PI adapters to buffer data egressed from the adapter to endpoints. Configure buffering through the buffering configuration parameters in the system configuration. Note: OSIsoft recommends that you do not modify the default buffering location unless it is necessary. Changes to the buffering configuration parameters only take effect during adapter service startup. Configure buffering Complete the following steps to configure buffering. Use the PUT method in conjunction with the http://localhost:5590/api/v1/configuration/system/buffering http:  localhost:5590 api v1 configuration system buffering REST endpoint to initialize the configuration. Using a text editor, create an empty text file. Copy and paste an example configuration for buffering into the file. For sample JSON, see Examples - Retrieve the buffering configuration . Update the example JSON parameters for your environment. For a table of all available parameters, see Buffering parameters . Save the file. For example, as ConfigureBuffering.json . Open a command line session. Change directory to the location of ConfigureBuffering.json . Enter the following cURL command (which uses the PUT method) to initialize the buffering configuration. curl -d \"@ConfigureBuffering.json\" -H \"Content-Type: application/json\" application json\" -X PUT \"http://localhost:5590/api/v1/configuration/system/buffering\" \"http:  localhost:5590 api v1 configuration system buffering\" Note: If you installed the adapter to listen on a non-default port, update 5590 to the port number in use. For a list of other REST operations you can perform, like updating or replacing a buffering configuration, see REST URLs . Buffering schema The full schema definition for the system buffering is in the System_Buffering_schema.json file located in one of the following folders: Windows : %ProgramFiles%\\OSIsoft\\Adapters\\\u003cAdapterName\u003e\\Schemas Linux : /opt/OSIsoft/Adapters/\u003cAdapterName\u003e/Schemas  opt OSIsoft Adapters \u003cAdapterName\u003e Schemas Buffering parameters The following parameters are available for configuring buffering: Parameter Required Type Description EnablePersistentBuffering Optional boolean Enables or disables on-disk buffering Allowed value: true or false Default value: true Note: If you disable persistent buffering, in-memory buffering is used. On-disk and in-memory buffering are limited by value in the MaxBufferSizeMB property. MaxBufferSizeMB Optional integer Defines the maximum size of the buffer that is persisted on disk 1 or used in memory 2 . The unit is specified in MB (1 Megabyte = 1048576 bytes). Consider the capacity and the type of storage medium to determine a suitable value for this parameter. Minimum value: 1 Maximum value: 2147483647 Default value: 1024 Note: The MaxBufferSizeMB property is applied to each configured endpoint. For example, if you set the MaxBufferSizeMB to 1024 and you configured the adapter to send data to two endpoints (for example, PI Server and AVEVA Data Hub), the total maximum resources used for buffering will be 2048 . The health endpoint is an exception fixed at 20 MB. BufferLocation Required string Defines the location of the buffer files. Absolute paths are required. Consider the access-control list (ACL) when you set this parameter. BufferLocation is used to buffer files when EnablePersistentBuffering is true . Allowed value: Valid path to a folder location in the file system Default value: Windows: %ProgramData%\\OSIsoft\\Adapters\\{AdapterInstance}\\Buffers Linux: /usr/share/OSIsoft/Adapters/{AdapterInstance}/Buffers  usr share OSIsoft Adapters {AdapterInstance} Buffers 1 Buffering to disk - disk is only used if required; Data is only written to the disk buffer if it is queued in the memory buffer for more than 5 seconds. The MaxBufferSizeMB is applied per configured endpoint, except the health endpoint. An adapter creates 20 MB buffer files that are stored in BufferLocation . When MaxBufferSizeMB is reached, the oldest buffer file is deleted and a new buffer file is created. The health endpoint is fixed at 20 MB. When the health endpoint buffer file becomes full, a new buffer file is created and the previous buffer file is deleted. Note: The following rules apply in case of an error when creating a new buffer file: Attempt to delete oldest buffer file and retry. If unable to buffer, errors are logged to indicate data loss. If a buffer file is corrupted, an attempt is made to recover individual records and any failure to recover records is logged. 2 Buffering only to memory : The MaxBufferSizeMB is applied per configured endpoint except the health endpoint. When MaxBufferSizeMB is reached, the oldest messages in the memory buffer are removed. Depending on the size of a new message, several old messages may be removed. The health endpoint is fixed at 20 MB. When the health endpoint buffer file becomes full, the oldest messages in the memory buffer are removed and new messages are added. Examples The following examples are buffering configurations made through the curl REST client. Retrieve the buffering configuration curl -X GET \"http://localhost:5590/api/v1/configuration/system/buffering\" \"http:  localhost:5590 api v1 configuration system buffering\" Sample output: { \"bufferLocation\": \"C:/ProgramData/OSIsoft/Adapters/\u003cAdapterName\u003e/Buffers\", \"C: ProgramData OSIsoft Adapters \u003cAdapterName\u003e Buffers\", \"maxBufferSizeMB\": 1024, \"enablePersistentBuffering\": true } 200 OK response indicates success. Update MaxBufferSizeMb parameter curl -d \"{ \\\"MaxBufferSizeMB\\\": 100 }\" -H \"Content-Type: application/json\" application json\" -X PATCH \"http://localhost:5590/api/v1/configuration/system/buffering\" \"http:  localhost:5590 api v1 configuration system buffering\" 204 No Content response indicates success. REST URLs Relative URL HTTP verb Action api/v1/configuration/system/buffering api v1 configuration system buffering GET Gets the buffering configuration api/v1/configuration/system/buffering api v1 configuration system buffering PUT Replaces the existing buffering configuration api/v1/configuration/system/buffering api v1 configuration system buffering PATCH Update parameter, partial configuration"
                                                                           },
    "content/shared-content/shared-content/configuration/data-filters.html":  {
                                                                                  "href":  "content/shared-content/shared-content/configuration/data-filters.html",
                                                                                  "title":  "Data filters",
                                                                                  "keywords":  "Data filters You can configure PI adapters to perform data filtering to save network bandwidth. Every data item in the data selection configuration can be assigned the Id of a data filter. The adapter filters data for those data items based on the data filter configuration. Note: If you enable data filters and data quality changes, both the old and current data quality values are passed on. Configure data filters Complete the following steps to configure data filters. Use the PUT method in conjunction with the http://localhost:5590/api/v1/configuration/\u003cComponentId\u003e/DataFilters http:  localhost:5590 api v1 configuration \u003cComponentId\u003e DataFilters REST endpoint to initialize the configuration. Using a text editor, create an empty text file. Copy and paste an example configuration for data filters into the file. For sample JSON, see Data filters example . Update the example JSON parameters for your environment. For a table of all available parameters, see Data filters parameters . Save the file. For example, as ConfigureDataFilters.json . Open a command line session. Change directory to the location of ConfigureDataFilters.json . Enter the following curl command or EdgeCmd to initialize the data filters configuration. curl Tab Text 2 curl -d \"@ConfigureDataFilters.json\" -H \"Content-Type: application/json\" application json\" -X PUT \"http://localhost:5590/api/v1/configuration/\u003cComponentId\u003e/DataFilters\" \"http:  localhost:5590 api v1 configuration \u003cComponentId\u003e DataFilters\" edgecmd set dataFilters -cid OpcUa1 -file ConfigureDataFilters.json **Note:** If you installed the adapter to listen on a non-default port, update `5590` to the port number in use. For a list of other REST operations you can perform, like updating or deleting a data filters configuration, see [REST URLs](#rest-urls). \u003cbr/\u003e \u003cbr \u003e \u003cbr/\u003e \u003cbr \u003e On successful execution, the change that you have made to data filters takes effect immediately during runtime. Data filters parameters The following parameters are available for configuring data filters: Parameter Required Type Description Id Required string Unique identifier for the data filter. Allowed value: any string identifier AbsoluteDeadband Optional double Specifies the absolute change in data value that should cause the current value to pass the filter test. Note: You must specify AbsoluteDeadband or PercentChange . Allowed value: double value representing absolute deadband number Default value: null PercentChange Optional double Specifies the percent change from previous value that should cause the current value to pass the filter test. Note: You must specify AbsoluteDeadband or PercentChange . Allowed value: double value representing percent change Default value: null ExpirationPeriod Optional timespan The length in time that can elapse after an event before automatically sending the next event, regardless of whether the next event passes the filter or not. The expected format is HH:MM:SS.### or SSS.* Allowed value: any timespan Default value: null * Note: For example, \"ExpirationPeriod\": 5:00 and \"ExpirationPeriod\": 300 both specify an expiration period of 5 minutes and 0 seconds. Data filters example [ { \"Id\": \"DuplicateData\", \"AbsoluteDeadband\": 0, \"PercentChange\": null, \"ExpirationPeriod\": \"01:00:00\" } ] REST URLs Relative URL HTTP verb Action api/v1/configuration/ api v1 configuration  ComponentId /DataFilters  DataFilters GET Gets all configured data filters. api/v1/configuration/ api v1 configuration  ComponentId /DataFilters  DataFilters DELETE Deletes all configured data filters. api/v1/configuration/ api v1 configuration  ComponentId /DataFilters  DataFilters POST Adds an array of data filters or a single data filter. Fails if any data filter already exists. api/v1/configuration/ api v1 configuration  ComponentId /DataFilters  DataFilters PUT Replaces all data. api/v1/configuration/ api v1 configuration  ComponentId /DataFilters  DataFilters PATCH Allows partial updating of configured data filter. api/v1/configuration/ api v1 configuration  ComponentId /DataFilters/  DataFilters  id GET Gets configured data filter by Id . api/v1/configuration/ api v1 configuration  ComponentId /DataFilters/  DataFilters  id DELETE Deletes configured data filter by Id . api/v1/configuration/ api v1 configuration  ComponentId /DataFilters/  DataFilters  id PUT Replaces data filter by Id . Fails if data filter does not exist. Note: Replace ComponentId with the Id of your adapter component."
                                                                              },
    "content/shared-content/shared-content/configuration/diagnostics-and-metadata.html":  {
                                                                                              "href":  "content/shared-content/shared-content/configuration/diagnostics-and-metadata.html",
                                                                                              "title":  "Diagnostics and metadata",
                                                                                              "keywords":  "Diagnostics and metadata You can configure PI adapters to produce and store diagnostics data at a designated health endpoint and to send metadata for created streams. For more information about available diagnostics data, see Adapter diagnostics and Egress diagnostics . For more information about available metadata and what metadata are sent per metadata level, see Adapter Metadata . Configure general Start any of the Configuration tools capable of making HTTP requests. Run a PUT command to the following endpoint, setting EnableDiagnostics to either true or false , MetadataLevel to None , Low , Medium , or High and HealthPrefix to a string or null : http://localhost:5590/api/v1/configuration/system/general http:  localhost:5590 api v1 configuration system general Note: 5590 is the default port number. If you selected a different port number, replace it with that value. Example using curl : curl -d \"{ \\\"EnableDiagnostics\\\":true, \\\"MetadataLevel\\\":Medium, \\\"HealthPrefix\\\":\\\"Machine1\\\" }\" -X PUT \"http://localhost:5590/api/v1/configuration/system/general\" \"http:  localhost:5590 api v1 configuration system general\" General schema The full schema definition for the general configuration is in the System_General_schema.json file located in one of the following folders: Windows : %ProgramFiles%\\OSIsoft\\Adapters\\\u003cAdapterName\u003e\\Schemas Linux : /opt/OSIsoft/Adapters/\u003cAdapterName\u003e/Schemas  opt OSIsoft Adapters \u003cAdapterName\u003e Schemas General parameters The following parameters are available for configuring general: Parameter Required Type Description EnableDiagnostics Optional boolean Determines if diagnostics are enabled Allowed value: true or false Default value: true MetadataLevel Optional reference Defines amount of metadata sent to OMF endpoints. Allowed value: None , Low , Medium , and High Default value: Medium HealthPrefix Optional reference Prefix to use for health and diagnostics stream and asset IDs. Default value: null Example Retrieve the general configuration Example using curl : curl -X GET \"http://localhost:{port}/api/v1/configuration/system/general\" \"http:  localhost:{port} api v1 configuration system general\" Sample output: { \"EnableDiagnostics\": true, \"MetadataLevel\": \"Medium\", \"HealthPrefix\": \"Machine1\" } REST URLs Relative URL HTTP verb Action api/v1/configuration/system/General api v1 configuration system General GET Gets the general configuration api/v1/configuration/system/General api v1 configuration system General PUT Replaces the existing general configuration api/v1/configuration/system/General api v1 configuration system General PATCH Allows partial updating of general configuration"
                                                                                          },
    "content/shared-content/shared-content/configuration/discovery.html":  {
                                                                               "href":  "content/shared-content/shared-content/configuration/discovery.html",
                                                                               "title":  "Discovery",
                                                                               "keywords":  "Discovery You can perform data discovery for existing data items on demand. Data discovery is initiated through REST calls and it is tied to a specific discovery Id , which you can either specify or let the adapter generate it. Data discovery includes different routes. For example, you can choose to do the following: Retrieve the discovery results Query the discovery status Cancel or delete discoveries Merge discovery results with the data selection configuration Retrieve results from a current discovery and compare it with results from a previous or discovery Retrieve results from a current discovery and compare it with results from a current data selection configuration Configure discovery Start any of the Configuration tools capable of making HTTP requests. Run a POST command with the Id of the discovery and autoSelect set to either true or false to the following endpoint: http://localhost:5590/api/v1/configuration/\u003cComponentId\u003e/Discoveries http:  localhost:5590 api v1 configuration \u003cComponentId\u003e Discoveries . Notes: Including an Id is optional. If you do not include one, the adapter automatically generates one. 5590 is the default port number. If you selected a different port number, replace it with that value. Example using curl : curl -d \"{ \\\"Id\\\":\\\"TestDiscovery\\\", \\\"autoSelect\\\":true }\" -H \"Content-Type:application/json\" \"Content-Type:application json\" -X POST \"http://localhost:5590/api/v1/configuration/\u003cComponentId\u003e/Discoveries\" \"http:  localhost:5590 api v1 configuration \u003cComponentId\u003e Discoveries\" Discovery parameters Parameter Type Description id string The Id of the discovery Notes: ??? You cannot run multiple discoveries with the same Id . ??? Including an Id is optional. If you do not include one, the adapter automatically generates one. query string A filter that is specific to the data source. The query filter can limit the scope of the discovery. For more information, see the Data source configuration topic. startTime datetime Time when the discovery started endTime datetime Time when the discovery ended progress double Progress of the discovery itemsFound integer Number of data items that the discovery found on the data source newItems integer Number of new data items that the discovery found in comparison to the previous discovery resultUri integer URL at which you can access the results of the discovery autoSelect boolean When set to true , the result of the discovery gets pushed to the data selection configuration. status reference Status of the discovery, for example Active or Complete errors string Errors encountered during the discovery Discoveries status example The following example shows the status of all discoveries. The discovery id in this example was auto-generated. [ { \"id\": \"8ff855f1-a636-490a-bb31-207410a6e607\", \"query\": null, \"startTime\": \"2020-09-30T19:34:01.8180401+02:00\", \"endTime\": \"2020-09-30T19:34:01.8368776+02:00\", \"progress\": 30, \"itemsFound\": 4, \"newItems\": 0, \"resultUri\": \"http://127.0.0.1:5590/api/v1/Configuration/\u003cComponentId\u003e/Discoveries/8ff855f1-a636-490a-bb31-207410a6e607/result\", \"http:  127.0.0.1:5590 api v1 Configuration \u003cComponentId\u003e Discoveries 8ff855f1-a636-490a-bb31-207410a6e607 result\", \"autoSelect\": false, \"status\": \"Complete\", \"errors\": null } ] REST URLs Relative URL HTTP verb Action api/v1/configuration/ api v1 configuration  \u003ccomponentId\u003e /discoveries  discoveries GET Returns status of all discoveries api/v1/configuration/ api v1 configuration  \u003ccomponentId\u003e /discoveries  discoveries POST Initiates a new discovery and returns its Id api/v1/configuration/ api v1 configuration  \u003ccomponentId\u003e /discoveries  discoveries DELETE Cancels and deletes all saved discoveries api/v1/configuration/ api v1 configuration  \u003ccomponentId\u003e /discoveries/  discoveries  \u003cdiscoveryId\u003e GET Gets the status of an individual discovery Note: If a discovery with the specified Id does not exist, you will get an error message api/v1/configuration/ api v1 configuration  \u003ccomponentId\u003e /discoveries/  discoveries  \u003cdiscoveryId\u003e DELETE Cancels and deletes discovery and result api/v1/configuration/ api v1 configuration  \u003ccomponentId\u003e /discoveries/  discoveries  \u003cdiscoveryId\u003e /result  result GET Returns the result of a discovery api/v1/configuration/ api v1 configuration  \u003ccomponentId\u003e /discoveries/  discoveries  \u003cdiscoveryId\u003e /result?diff=  result?diff= previousId GET Returns the difference between the result and the previous result api/v1/configuration/ api v1 configuration  \u003ccomponentId\u003e /dataselection?diff=  dataselection?diff= \u003cdiscoveryId\u003e GET Returns the difference between the data selection configuration and the discovery results api/v1/configuration/ api v1 configuration  \u003ccomponentId\u003e /discoveries/  discoveries  \u003cdiscoveryId\u003e /result  result DELETE Cancels and deletes discovery result Note: The discovery Id is still valid, but a query will contain a status of canceled Only the Status property will contain a canceled status, but not the query api/v1/configuration/ api v1 configuration  \u003ccomponentId\u003e /discoveries/  discoveries  \u003cdiscoveryId\u003e /cancel  cancel POST Cancels the on-demand data source discovery api/v1/configuration/ api v1 configuration  \u003ccomponentId\u003e /dataselection/select?discoveryid=  dataselection select?discoveryid= \u003cdiscoveryId\u003e POST Adds the discovered items to data selection with selected set to true api/v1/configuration/ api v1 configuration  \u003ccomponentId\u003e /dataselection/unselect?discoveryid=  dataselection unselect?discoveryid= \u003cdiscoveryId\u003e POST Adds the discovered items to data selection with selected set to false Note: Replace \u003ccomponentId\u003e with the Id of your adapter component. Replace \u003cdiscoveryId\u003e with the Id of the discovery for which you want to perform the action."
                                                                           },
    "content/shared-content/shared-content/configuration/egress-endpoints.html":  {
                                                                                      "href":  "content/shared-content/shared-content/configuration/egress-endpoints.html",
                                                                                      "title":  "Egress endpoints",
                                                                                      "keywords":  "Egress endpoints PI adapters collect time series data, which they can send to a permanent data store (endpoint). This operation is called data egress. The following endpoints are available for data egress: AVEVA Data Hub PI servers through PI Web API For long term storage and analysis, you can configure any adapter to send time series data to one or several of these endpoints in any combination. An egress endpoint is comprised of the properties specified under Egress endpoint parameters . Data egress to a PI server creates a PI point in the PI adapter configuration. Data egress to AVEVA Data Hub creates a stream in the PI adapter configuration. The name of the PI point or AVEVA Data Hub stream is a combination of the StreamIdPrefix specified in the adapter data source configuration and the StreamId specified in the adapter data selection configuration. Configure egress endpoints Complete the following steps to configure egress endpoints. Use the PUT method in conjunction with the http://localhost:5590/api/v1/configuration/OmfEgress/dataendpoints http:  localhost:5590 api v1 configuration OmfEgress dataendpoints REST endpoint to initialize the configuration. Using a text editor, create an empty text file. Copy and paste an example configuration for egress endpoints into the file. For sample JSON, see Examples . Update the example JSON parameters for your environment. For a table of all available parameters, see Egress endpoint parameters . Save the file. For example, as ConfigureEgressEndpoints.json . Open a command line session. Change directory to the location of ConfigureEgressEndpoints.json . Enter the following cURL command (which uses the PUT method) to initialize the egress endpoints configuration. curl -d \"@ConfigureEgressEndpoints.json\" -H \"Content-Type: application/json\" application json\" -X PUT \"http://localhost:5590/api/v1/configuration/OmfEgress/dataendpoints\" \"http:  localhost:5590 api v1 configuration OmfEgress dataendpoints\" Note: : If you installed the adapter to listen on a non-default port, update 5590 to the port number in use. For a list of other REST operations you can perform, like updating or replacing an egress endpoints configuration, see REST URLs . Egress endpoint configuration schema The full schema definition for the egress endpoint configuration is in the OmfEgress_DataEndpoints_schema.json file located in one of the following folders: Windows : %ProgramFiles%\\OSIsoft\\Adapters\\\u003cAdapterName\u003e\\Schemas Linux : /opt/OSIsoft/Adapters/\u003cAdapterName\u003e/Schemas  opt OSIsoft Adapters \u003cAdapterName\u003e Schemas Egress endpoint parameters The following parameters are available for configuring egress endpoints: Parameter Required Type Description Id Optional string Unique identifier Allowed value: any string identifier Default value: new GUID Endpoint Required string Destination that accepts OMF v1.2 messages. Supported destinations include AVEVA Data Hub and PI Server. Allowed value: well-formed http or https endpoint string Default: null Username Required for PI server endpoint string Basic authentication to the PI Web API OMF endpoint PI server: Allowed value: any string Default: null Note: If your username contains a backslash, you must add an escape character, for example, type OilCompany\\TestUser as OilCompany\\\\TestUser . Password Required for PI server endpoint string Basic authentication to the PI Web API OMF endpoint PI server: Allowed value: any string Default: null ClientId Required for AVEVA Data Hub endpoint string Authentication with the AVEVA Data Hub OMF endpoint Allowed value: any string, can be null if the endpoint URL schema is HTTP Default: null ClientSecret Required for AVEVA Data Hub endpoint string Authentication with the AVEVA Data Hub OMF endpoint Allowed value: any string, can be null if the endpoint URL schema is HTTP Default: null TokenEndpoint Optional for AVEVA Data Hub endpoint string Retrieves an AVEVA Data Hub token from an alternative endpoint Allowed value: well-formed http or https endpoint string Default value: null ValidateEndpointCertificate Optional boolean Disables verification of destination certificate. Note: Only use for testing with self-signed certificates. Allowed value: true or false Default value: true Special characters encoding The adapter encodes special characters used in the data selection StreamId parameter string before sending it to configured endpoints. The encoded characters look as follows: Special character Encoded character * %2a \u0027 %27 ` %60 \" %22 ? %3f ; %3b | %7c \\ %5c { %7b } %7d [ %5b ] %5d Examples The following examples are valid egress configurations: Egress data to AVEVA Data Hub [{ \"Id\": \"AVEVA Data Hub\", \"Endpoint\": \"https://\u003cAVEVA \"https:  \u003cAVEVA Data Hub OMF endpoint\u003e\", \"ClientId\": \"\u003cclientid\u003e\", \"ClientSecret\": \"\u003cclientsecret\u003e\" }] Egress data to PI Web API [{ \"Id\": \"PI Web API\", \"Endpoint\": \"https://\u003cpi \"https:  \u003cpi web api server\u003e:\u003cport\u003e/piwebapi/omf/\", server\u003e:\u003cport\u003e piwebapi omf \", \"UserName\": \"\u003cusername\u003e\", \"Password\": \"\u003cpassword\u003e\" }] REST URLs Relative URL HTTP verb Action api/v1/configuration/omfegress/DataEndpoints api v1 configuration omfegress DataEndpoints GET Gets all configured egress endpoints api/v1/configuration/omfegress/DataEndpoints api v1 configuration omfegress DataEndpoints DELETE Deletes all configured egress endpoints api/v1/configuration/omfegress/DataEndpoints api v1 configuration omfegress DataEndpoints POST Adds an array of egress endpoints or a single endpoint. Fails if any endpoint already exists api/v1/configuration/omfegress/DataEndpoints api v1 configuration omfegress DataEndpoints PUT Replaces all egress endpoints api/v1/configuration/omfegress/DataEndpoints api v1 configuration omfegress DataEndpoints PATCH Allows partial updating of configured endpoints. Note: The request must be an array containing one or more endpoints. Each endpoint in the array must include its Id . api/v1/configuration/omfegress/DataEndpoints/{Id} api v1 configuration omfegress DataEndpoints {Id} GET Gets configured endpoint by Id api/v1/configuration/omfegress/DataEndpoints/{Id} api v1 configuration omfegress DataEndpoints {Id} DELETE Deletes configured endpoint by Id api/v1/configuration/omfegress/DataEndpoints/{Id} api v1 configuration omfegress DataEndpoints {Id} PUT Updates or creates a new endpoint with the specified Id api/v1/configuration/omfegress/DataEndpoints/{Id} api v1 configuration omfegress DataEndpoints {Id} PATCH Allows partial updating of configured endpoint by Id Egress execution details After configuring an egress endpoint, egress is immediately run for that endpoint. Egress is handled individually per configured endpoint. When data is egressed for the first time, types and containers are egressed to the configured endpoint. After that, only new or changed types or containers are egressed. Type creation must be successful in order to create containers. Container creation must be successful in order to egress data. If you delete an egress endpoint, data flow immediately stops for that endpoint. Buffered data in a deleted endpoint is permanently lost. Type, container, and data items are batched into one or more OMF messages when egressing. As per the requirements defined in OMF, a single message payload will not exceed 192KB in size. Compression is automatically applied to outbound egress messages. On the egress destination, failure to add a single item results in the message failing. Types, containers, and data are egressed as long as the destination continues to respond to HTTP requests."
                                                                                  },
    "content/shared-content/shared-content/configuration/health-endpoints.html":  {
                                                                                      "href":  "content/shared-content/shared-content/configuration/health-endpoints.html",
                                                                                      "title":  "Health endpoints",
                                                                                      "keywords":  "Health endpoints You can configure PI adapters to produce and store health data at a designated health endpoint. You can use health data to ensure that your adapters are running properly and that data flows to the configured OMF endpoints. For more information about health, see Edge Data Store health . Configure health endpoint A health endpoint designates an OMF endpoint where adapter health information is sent. You can configure multiple health endpoints. Complete the following steps to configure health endpoints. Use the PUT method in conjunction with the http://localhost:5590/api/v1/configuration/system/healthendpoints http:  localhost:5590 api v1 configuration system healthendpoints REST endpoint to initialize the configuration. Using a text editor, create an empty text file. Copy and paste an example configuration for health endpoints into the file. For sample JSON, see Examples . Update the example JSON parameters for your environment. For a table of all available parameters, see Health endpoint parameters . Save the file. For example, as ConfigureHealthEndpoints.json . Open a command line session. Change directory to the location of ConfigureHealthEndpoints.json . Enter the following cURL command (which uses the PUT method) to initialize the health endpoint configuration. curl -d \"@ConfigureHealthEndpoints.json\" -H \"Content-Type: application/json\" application json\" -X PUT \"http://localhost:5590/api/v1/configuration/system/healthendpoints\" \"http:  localhost:5590 api v1 configuration system healthendpoints\" Note: If you installed the adapter to listen on a non-default port, update 5590 to the port number in use. For a list of other REST operations you can perform, like updating or replacing a health endpoints configuration, see REST URLs . Health endpoints schema The full schema definition for the health endpoint configuration is in the System_HealthEndpoints_schema.json file located in one of the following folders: Windows : %ProgramFiles%\\OSIsoft\\Adapters\\\u003cAdapterName\u003e\\Schemas Linux : /opt/OSIsoft/Adapters/\u003cAdapterName\u003e/Schemas  opt OSIsoft Adapters \u003cAdapterName\u003e Schemas Health endpoint parameters The following parameters are available for configuring health endpoints: Parameter Required Type Description Id Optional string Uniquely identifies the endpoint. This can be any alphanumeric string. If left blank, a unique value is generated automatically. Allowed value: any string identifier Default value: new GUID Endpoint Required string The URL of the OMF endpoint to receive this health data Allowed value: well-formed http or https endpoint string Default: null Username Required for PI Web API endpoints string The username used to authenticate with a PI Web API OMF endpoint PI server: Allowed value: any string Default: null Password Required for PI Web API endpoints string The password used to authenticate with a PI Web API OMF endpoint PI server: Allowed value: any string Default: null ClientId Required for AVEVA Data Hub endpoints string The client ID used for authentication with an AVEVA Data Hub OMF endpoint Allowed value: any string Default: null ClientSecret Required for AVEVA Data Hub endpoints string The client secret used for authentication with an AVEVA Data Hub OMF endpoint Allowed value: any string Default: null TokenEndpoint Optional for AVEVA Data Hub endpoints string Retrieves an AVEVA Data Hub token from an alternative endpoint Allowed value: well-formed http or https endpoint string Default value: null ValidateEndpointCertificate Optional boolean Disables verification of destination security certificate. Use for testing only with self-signed certificates; OSIsoft recommends keeping this set to the default, true, in production environments. Allowed value: true or false Default value: true Examples AVEVA Data Hub endpoint { \"Id\": \"AVEVA Data Hub\", \"Endpoint\": \"https://\u003cAVEVA \"https:  \u003cAVEVA Data Hub OMF endpoint\u003e\", \"ClientId\": \"\u003cclientid\u003e\", \"ClientSecret\": \"\u003cclientsecret\u003e\" } PI Web API endpoint { \"Id\": \"PI Web API\", \"Endpoint\": \"https://\u003cpi \"https:  \u003cpi web api server\u003e:\u003cport\u003e/piwebapi/omf/\", server\u003e:\u003cport\u003e piwebapi omf \", \"UserName\": \"\u003cusername\u003e\", \"Password\": \"\u003cpassword\u003e\" } Note: When you use an adapter with a PI Web API health endpoint, the AF structure is required. If the elements are deleted, the adapter recreates the elements; if the account used to authenticate to the PI Web API has its permissions removed on the AF Server, the adapter retries sending health data to the PI Web API until the permissions are restored. REST URLs Relative URL HTTP verb Action api/v1/configuration/system/healthEndpoints api v1 configuration system healthEndpoints GET Gets all configured health endpoints api/v1/configuration/system/healthEndpoints api v1 configuration system healthEndpoints DELETE Deletes all configured health endpoints api/v1/configuration/system/healthEndpoints api v1 configuration system healthEndpoints POST Adds an array of health endpoints or a single endpoint. Fails if any endpoint already exists api/v1/configuration/system/healthEndpoints api v1 configuration system healthEndpoints PUT Replaces all health endpoints. Note: Requires an array of endpoints api/v1/configuration/system/healthEndpoints api v1 configuration system healthEndpoints PATCH Allows partial updating of configured health endpoints Note: The request must be an array containing one or more health endpoints. Each health endpoint in the array must include its Id . api/v1/configuration/system/healthEndpoints/ api v1 configuration system healthEndpoints  Id GET Gets configured health endpoint by Id api/v1/configuration/system/healthEndpoints/ api v1 configuration system healthEndpoints  Id DELETE Deletes configured health endpoint by Id api/v1/configuration/system/healthEndpoints/ api v1 configuration system healthEndpoints  Id PUT Updates or creates a new health endpoint with the specified Id api/v1/configuration/system/healthEndpoints/ api v1 configuration system healthEndpoints  Id PATCH Allows partial updating of configured health endpoint by Id Note: Replace Id with the Id of the health endpoint."
                                                                                  },
    "content/shared-content/shared-content/configuration/history-recovery.html":  {
                                                                                      "href":  "content/shared-content/shared-content/configuration/history-recovery.html",
                                                                                      "title":  "History recovery",
                                                                                      "keywords":  "History recovery The adapter you are using supports the following data collection modes which you configure in the DataCollectionMode parameter of your adapter\u0027s data source configuration: CurrentOnly : The adapter component operates normally. History recovery is disabled. CurrentWithBackfill (Default): The adapter component operates normally, but disconnections and shutdown events are recorded in the form of recovery intervals. When the adapter is reconnected to a data source, it automatically backfills data for the recorded intervals. HistoryOnly : The adapter component does not get started. The adapter can start collecting historical data on demand. History recovery for adapters supports the following two operations related to the data collection mode: On demand history recovery : Recovers data from a specified start time or start and end time. If end time is not specified, the default is utcnow . On demand history recovery is available only when the adapter is in HistoryOnly data collection mode. Limited automatic history recovery : Backfills data gaps that originated from connection disruptions, data source issues, PI adapter shutdown, or both. This is limited to a maximum time-range of four days. Limited automatic history recovery is available only when the adapter is in CurrentWithBackfill data collection mode."
                                                                                  },
    "content/shared-content/shared-content/configuration/history-recovery/automatic-history-recovery.html":  {
                                                                                                                 "href":  "content/shared-content/shared-content/configuration/history-recovery/automatic-history-recovery.html",
                                                                                                                 "title":  "Automatic history recovery",
                                                                                                                 "keywords":  "Automatic history recovery Besides on-demand history recovery, the PI adapter also supports automatic history recovery. For automatic history recovery, the adapter tracks changes to the DeviceStatus of each component. When the DeviceStatus changes to DeviceInError or Shutdown , the adapter starts a new History recovery interval . When the issue resolves or if the adapter is restarted and the DeviceStatus changes to Good , the adapter closes any current intervals for that component. The adapter tracks these intervals for each component and when DeviceStatus has a value of Good , it performs history recovery for these intervals starting from oldest to newest. For more information, see also Device status . Note: If you set the data collection mode to CurrentWithBackfill , the adapter clears periods not recovered for the component and stops tracking them. An automatic history recovery operation in progress is canceled only if the data collection mode is set to HistoryOnly , otherwise it will be finished. History recovery intervals Automatic history intervals cannot be longer than four days. If an interval is longer than four days, the adapter automatically changes the start time of the interval to be no earlier than four days before the end time prior to starting a recovery. If a current outage lasts longer than four days, when the device status finally improves, the adapter recovers up to four days before the current time. This avoids introducing additional data gaps."
                                                                                                             },
    "content/shared-content/shared-content/configuration/history-recovery/on-demand-history-recovery-configuration.html":  {
                                                                                                                               "href":  "content/shared-content/shared-content/configuration/history-recovery/on-demand-history-recovery-configuration.html",
                                                                                                                               "title":  "On-demand history recovery configuration",
                                                                                                                               "keywords":  "On-demand history recovery configuration The PI adapter supports performing history recovery on-demand by specifying a start and end time. Configure history recovery Start any of the Configuration tools capable of making HTTP requests. Run a POST command with the Id of the history recovery and the startTime and endTime to the following endpoint: http://localhost:5590/api/v1/configuration/\u003cComponentId\u003e/HistoryRecoveries http:  localhost:5590 api v1 configuration \u003cComponentId\u003e HistoryRecoveries . Example using curl : curl -d \"{ \\\"Id\\\":\\\"TestRecovery\\\", \\\"startTime\\\":\\\"2021-03-29T14:00:30Z\\\", \\\"endTime\\\":\\\"2021-03-29T15:00:15Z\\\" }\" -X POST \"http://localhost:5590/api/v1/configuration/\u003cComponentId\u003e/HistoryRecoveries\" \"http:  localhost:5590 api v1 configuration \u003cComponentId\u003e HistoryRecoveries\" Note: The default port is 5590 . If you selected a different port number, replace it with that value. If you do not specify an Id , the endpoint generates a unique Id . History recovery parameters Parameter Type Description Id string The Id of the history recovery Note: You cannot run multiple history recoveries with the same Id . StartTime datetime Time when the the first data items are collected EndTime datetime Time when the last data items are collected Checkpoint datetime The latest timestamp that the history recovery has completed with the range being between startTime and endTime Items double The amount of data selection items in the history recovery operation RecoveredEvents double Number of events that the history recovery found on the data source Progress double Progress of the history recovery (number of data items found through the history recovery) Status enum Status of the history recovery. The following statuses are available: - Active - The operation is currently in progress. - Complete - The operation has been completed. - Canceled - The operation has been canceled. - Failed - The operation failed. Errors string Errors encountered during the history recovery History recovery status example [ { \"Id\": \"HistoryRecovery1\", \"StartTime\": \"2021-01-09T05:55:00.0\", \"EndTime\": \"2021-01-26T13:20:00.0\", \"CheckPoint\": \"2021-01-13T14:55:00.0\", \"Items\": 7000, \"RecoveredEvents\": 800000, \"Progress\": 20, \"Status\": \"Active\", \"Errors\": null } ] Note: The result of the history recovery operation is added to the \u003ccomponentId\u003e_historyRecoveries.json file. REST URLs Relative URL HTTP verb Action api/v1/configuration/ api v1 configuration  \u003ccomponentId\u003e /historyRecoveries  historyRecoveries GET Returns all history recoveries statuses api/v1/configuration/ api v1 configuration  \u003ccomponentId\u003e /historyRecoveries  historyRecoveries POST Initiates a new history recovery and returns the Id of the operation api/v1/configuration/ api v1 configuration  \u003ccomponentId\u003e /historyRecoveries  historyRecoveries DELETE Cancels all active history recovery operations and removes states api/v1/configuration/ api v1 configuration  \u003ccomponentId\u003e /historyRecoveries/  historyRecoveries  \u003coperationId\u003e GET Gets the status of an individual history recovery api/v1/configuration/ api v1 configuration  \u003ccomponentId\u003e /historyRecoveries/  historyRecoveries  \u003coperationId\u003e DELETE Cancels history recovery and removes the state api/v1/configuration/ api v1 configuration  \u003ccomponentId\u003e /historyRecoveries/  historyRecoveries  \u003coperationId\u003e /cancel  cancel POST Cancels history recovery api/v1/configuration/ api v1 configuration  \u003ccomponentId\u003e /historyRecoveries/  historyRecoveries  \u003coperationId\u003e /resume  resume POST Resumes canceled or failed history recovery operation ( 202 ) from the checkpoint Note: If the \u003coperationId\u003e is not found, a 404 HTTP error message is returned. Note: Replace \u003ccomponentId\u003e with the Id of your adapter component. Replace \u003coperationId\u003e with the Id of the history recovery operation for which you want to perform the action."
                                                                                                                           },
    "content/shared-content/shared-content/configuration/logging.html":  {
                                                                             "href":  "content/shared-content/shared-content/configuration/logging.html",
                                                                             "title":  "Logging",
                                                                             "keywords":  "Logging PI adapters write daily log messages for the adapter, the system, and OMF egress to flat text files in the following locations: ??? Windows: %ProgramData%\\OSIsoft\\Adapters\\{AdapterInstance}\\Logs ??? Linux: /usr/share/OSIsoft/Adapters/{AdapterInstance}/Logs  usr share OSIsoft Adapters {AdapterInstance} Logs Each message in the log displays the message severity level, timestamp, and the message itself. Configure logging Complete the following steps to configure logging. Use the PUT method in conjunction with the http://localhost:5590/api/v1/configuration/\u003cComponentId\u003e/Logging http:  localhost:5590 api v1 configuration \u003cComponentId\u003e Logging REST endpoint to initialize the configuration. Using a text editor, create an empty text file. Copy and paste an example configuration for logging into the file. For sample JSON, see Example . Update the example JSON parameters for your environment. For a table of all available parameters, see Logging parameters . Save the file. For example, as ConfigureLogging.json . Open a command line session. Change directory to the location of ConfigureLogging.json . Enter the following cURL command (which uses the PUT method) to initialize the logging configuration. curl -d \"@ConfigureLogging.json\" -H \"Content-Type: application/json\" application json\" -X PUT \"http://localhost:5590/api/v1/configuration/\u003cComponentId\u003e/Logging\" \"http:  localhost:5590 api v1 configuration \u003cComponentId\u003e Logging\" Notes: If you installed the adapter to listen on a non-default port, update 5590 to the port number in use. For a list of other REST operations you can perform, like updating or retrieving a logging configuration, see REST URLs . Any parameter not specified in the updated configuration file reverts to the default schema value On successful execution, the log-level change takes effect immediately during runtime. The other configurations (log file size and file count) are updated after the adapter is restarted. Logging schema The full schema definition for the logging configuration is in the component specific logging file: AdapterName_Logging_schema.json , OmfEgress_Logging_schema.json , or System_Logging_schema.json file located in one of the following folders: Windows : %ProgramFiles%\\OSIsoft\\Adapters\\\u003cAdapterName\u003e\\Schemas Linux : /opt/OSIsoft/Adapters/\u003cAdapterName\u003e/Schemas  opt OSIsoft Adapters \u003cAdapterName\u003e Schemas Logging parameters The following parameters are available for configuring logging: Parameter Required Type Description LogLevel Optional reference The logLevel sets the minimum severity for messages to be included in the logs. Messages with a severity below the level set are not included. The log levels in their increasing order of severity are as follows: Trace , Debug , Information , Warning , Error , Critical , and None . Default log level: Information For detailed information about the log levels, see LogLevel . LogFileSizeLimitBytes Optional integer The maximum size (in bytes) of log files that the service will create for the component. The value must be a positive integer. Minimum value: 1000 Maximum value: 9223372036854775807 Default value: 34636833 LogFileCountLimit Optional integer The maximum number of log files that the service will create for the component. The value must be a positive integer. Minimum value: 1 Maximum value: 2147483647 Default value: 31 LogLevel Level Description Trace Logs that contain the most detailed messages. These messages may contain sensitive application data like actual received values, which is why these messages should not be enabled in production environment. Note: Trace is translated to Verbose in the log file. Debug Logs that can be used to troubleshoot data flow issues by recording metrics and detailed flow related information. Information Logs that track the general flow of the application. Any non-repetitive general information like the following can be useful for diagnosing potential application errors: - Version information related to the software at startup - External services used - Data source connection string - Number of measurements - Egress URL - Change of state ???Starting??? or ???Stopping??? - Configuration Warning Logs that highlight an abnormal or unexpected event in the application flow that does not otherwise cause the application execution to stop. Warning messages can indicate an unconfigured data source state, an insecure communication channel in use, or any other event that could require attention but that does not impact data flow. Error Logs that highlight when the current flow of execution is stopped due to a failure. These should indicate a failure in the current activity and not an application-wide failure. It can indicate an invalid configuration, unavailable external endpoint, internal flow error, and so on. Critical Logs that describe an unrecoverable application or system crash or a catastrophic failure that requires immediate attention. This can indicate application wide failures like beta timeout expired, unable to start self-hosted endpoint, unable to access vital resource (for example, Data Protection key file), and so on. Note: Critical is translated to Fatal in the log file. None Logging is disabled for the given component. Example Default logging configuration By default, logging captures Information, Warning, Error, and Critical messages in the message logs. The following logging configuration is the installation default for a component: { \"logLevel\": \"Information\", \"logFileSizeLimitBytes\": 34636833, \"logFileCountLimit\": 31 } REST URLs Relative URL HTTP verb Action api/v1/configuration/System/Logging api v1 configuration System Logging GET Retrieves the system logging configuration api/v1/configuration/System/Logging api v1 configuration System Logging PUT Updates the system logging configuration api/v1/configuration/ api v1 configuration  ComponentId /Logging  Logging GET Retrieves the logging configuration of the specified adapter component api/v1/configuration/ api v1 configuration  ComponentId /Logging  Logging PUT Updates the logging configuration of the specified adapter component Note: Replace ComponentId with the Id of your adapter component."
                                                                         },
    "content/shared-content/shared-content/configuration/schedules.html":  {
                                                                               "href":  "content/shared-content/shared-content/configuration/schedules.html",
                                                                               "title":  "Schedules",
                                                                               "keywords":  "Schedules You can configure the adapter to run scans based on a schedule. Each data item can be assigned to a schedule in the data selection configuration. The adapter samples data for those data items at the scheduled time. Note: You start an ingress component without a schedule configuration. A default schedule configuration is added to use as an example. Note: When the adapter framework scheduler misses or skips a scan for any reason, either one of the following messages is printed: Scan skipped for schedule id \u003cId\u003e or Scan missed for schedule \u003cid\u003e . Configure schedules Complete the following steps to change the schedules configuration: Using any text editor, create a file that contains the schedules configuration in the JSON format. For content structure, see the example schedule configuration . For all available parameters, see the schedules parameters . Save the file. For example, ConfigureSchedules.json . Use any of the Configuration tools capable of making HTTP requests to run a PUT command with the contents of the file to the following endpoint: http://localhost:5590/api/v1/configuration/\u003cComponentId\u003e/Schedules http:  localhost:5590 api v1 configuration \u003cComponentId\u003e Schedules . Note: Replace \u003cComponentId\u003e with the ComponentId of the adapter. 5590 is the default port number. If you selected a different port number, replace it with that value. Example using curl or EdgeCmd: Note: Run this curl command or EdgeCmd from the same directory where the file is located. curl EdgeCmd curl -d \"@Modbus1ConfigureSchedules.json\" -H \"Content-Type: application/json\" application json\" -X PUT \"http://localhost:5590/api/v1/configuration/\u003cComponentId\u003e/Schedules\" \"http:  localhost:5590 api v1 configuration \u003cComponentId\u003e Schedules\" edgecmd set dataSelection -cid Modbus1 -file Modbus1ConfigureSchedules.json On successful execution, the schedules change takes effect immediately during runtime. Schedules parameters The following parameters are available for configuring schedules: Parameter Required Type Description Id Required string Unique identifier for the schedule Allowed value: any string identifier Period Required string The data sampling rate of the schedule. The expected format is HH:MM:SS.###. Invalid inputs: null , negative timespan, or zero A default value must be specified. Offset Optional string The offset from the midnight when the schedule starts. The expected format is HH:MM:SS.### Invalid input: negative timespan A default value must be specified. Note: You can also specify timespans as numbers in seconds. For example, \"Period\": 25 specifies 25 seconds, or \"Period\": 125 specifies 2 minutes and 5 seconds. Example schedule configuration The following is an example of a complete schedule configuration: [ { \"Id\": \"schedule1\", \"Period\": \"00:00:01.500\", \"Offset\": \"00:02:03\" } ] Default schedule configuration If no schedule is configured, the adapter uses the following default schedule configuration: [ { \"Id\": \"1\", \"Period\": \"0:00:05\", \"Offset\": \"0:00:00\" } ] REST URLs Relative URL HTTP verb Action api/v1/configuration/ api v1 configuration  ComponentId /Schedules  Schedules GET Gets all configured schedules api/v1/configuration/ api v1 configuration  ComponentId /Schedules  Schedules DELETE Deletes all configured schedules api/v1/configuration/ api v1 configuration  ComponentId /Schedules  Schedules POST Adds an array of schedules or a single schedule. Fails if any schedule already exists api/v1/configuration/ api v1 configuration  ComponentId /Schedules  Schedules PUT Replaces all schedules api/v1/configuration/ api v1 configuration  ComponentId /Schedules/  Schedules  id GET Gets configured schedule by Id api/v1/configuration/ api v1 configuration  ComponentId /Schedules/  Schedules  id DELETE Deletes configured schedule by Id api/v1/configuration/ api v1 configuration  ComponentId /Schedules/  Schedules  id PUT Replaces schedule by Id . Fails if schedule does not exist api/v1/configuration/ api v1 configuration  ComponentId /Schedules/  Schedules  id PATCH Allows partial updating of configured schedule by Id Note: Replace ComponentId with the Id of your adapter component."
                                                                           },
    "content/shared-content/shared-content/configuration/system-and-adapter.html":  {
                                                                                        "href":  "content/shared-content/shared-content/configuration/system-and-adapter.html",
                                                                                        "title":  "System and adapter",
                                                                                        "keywords":  "System and adapter You can configure the system component and adapter component together using a single file. Change system and adapter configuration Complete the following steps to configure system and adapter. Use the PUT method in conjunction with the http://localhost:5590/api/v1/configuration http:  localhost:5590 api v1 configuration REST endpoint to initialize the configuration. Using a text editor, create an empty text file. Copy and paste an example configuration for system and adapter into the file. For sample JSON, see the corresponding adapter configuration examples topic. Save the file. For example, as ConfigureSystemAndAdapter.json . Open a command line session. Change directory to the location of ConfigureSystemAndAdapter.json . Enter the following cURL command (which uses the PUT method) to initialize the system and adapter configuration. curl -d \"@ConfigureSystemAndAdapter.json\" -H \"Content-Type: application/json\" application json\" -X PUT \"http://localhost:5590/api/v1/configuration\" \"http:  localhost:5590 api v1 configuration\" Notes: If you installed the adapter to listen on a non-default port, update 5590 to the port number in use. In order for some of the adapter specific configurations to take effect, you have to restart the adapter. Discoveries and HistoryRecoveries facet details are not required to be supplied as part of the configuration and supplied values will be ignored. Their results will be restored from the previous states. If the operation fails due to errors in the configuration, the count of the error and suitable error messages are returned in the result. REST URLs Relative URL HTTP verb Action api/v1/configuration/ api v1 configuration  PUT Replaces the configuration for the entire adapter"
                                                                                    },
    "content/shared-content/shared-content/configuration/text-parser/jsonpath-syntax-for-value-retrieval.html":  {
                                                                                                                     "href":  "content/shared-content/shared-content/configuration/text-parser/jsonpath-syntax-for-value-retrieval.html",
                                                                                                                     "title":  "JSONPath syntax for value retrieval",
                                                                                                                     "keywords":  "JSONPath syntax for value retrieval For information on which semantic is used for retrieving values from JSON files, see JSONPath Syntax . The following syntax is used to extract values from JSON documents. JSON - Simple JSONPath example [ { \"time\": \"2020-08-10T12:10:46.0928791Z\", \"value\": 1.234567890 }, { \"time\": \"2020-08-10T12:10:47.0928791Z\", \"value\": 12.34567890 }, { \"time\": \"2020-08-10T12:10:48.0928791Z\", \"value\": 123.4567890 }, { \"time\": \"2020-08-10T12:10:49.0928791Z\", \"value\": 1234.567890 }, { \"time\": \"2020-08-10T12:10:50.0928791Z\", \"value\": 12345.67890 }, { \"time\": \"2020-08-10T12:10:51.0928791Z\", \"value\": 123456.7890 }, { \"time\": \"2020-08-10T12:10:52.0928791Z\", \"value\": 12345678.90 }, { \"time\": \"2020-08-10T12:10:53.0928791Z\", \"value\": 123456789.0 } ] The following JSONPath configuration reads a series of values: { \"Id\": \"DoubleValue\", \"FieldDefinition\": \"value\", \"DataType\": \"Double\" }, { \"Id\": \"Timestamp\", \"FieldDefinition\": \"time\", \"DataType\": \"DateTime\", \"IsIndex\": true } JSON - Complex JSONPath examples The following example reads specific values from a JSON array: { \"StreamData\": { \"TPPrototype.uflsample.value_time\": [ { \"StreamId\": \"TPPrototype.uflsample.value_time\", \"DataType\": \"Double\", \"Timestamp\": \"2013-12-01T06:00:00Z\", \"Value\": 339.0 }, { \"StreamId\": \"TPPrototype.uflsample.value_time\", \"DataType\": \"Double\", \"Timestamp\": \"2013-12-01T07:00:00Z\", \"Value\": 344.0 }, { \"StreamId\": \"TPPrototype.uflsample.value_time\", \"DataType\": \"Double\", \"Timestamp\": \"2013-12-01T17:00:00Z\", \"Value\": 341.0 } ], \"TPPrototype.uflsample.value_timeString\": [ { \"StreamId\": \"TPPrototype.uflsample.value_timeString\", \"DataType\": \"String\", \"Timestamp\": \"2013-12-01T06:00:00Z\", \"Value\": \"339.0\" }, { \"StreamId\": \"TPPrototype.uflsample.value_timeString\", \"DataType\": \"String\", \"Timestamp\": \"2013-12-01T07:00:00Z\", \"Value\": \"344.0\" }, { \"StreamId\": \"TPPrototype.uflsample.value_timeString\", \"DataType\": \"String\", \"Timestamp\": \"2013-12-01T17:00:00Z\", \"Value\": \"341.0\" } ], \"TPPrototype.uflsample.pressure_time\": [ { \"StreamId\": \"TPPrototype.uflsample.pressure_time\", \"DataType\": \"Double\", \"Timestamp\": \"2013-12-01T06:00:00Z\", \"Value\": 339.0 }, { \"StreamId\": \"TPPrototype.uflsample.pressure_time\", \"DataType\": \"Double\", \"Timestamp\": \"2013-12-01T07:00:00Z\", \"Value\": 344.0 }, { \"StreamId\": \"TPPrototype.uflsample.pressure_time\", \"DataType\": \"Double\", \"Timestamp\": \"2013-12-01T17:00:00Z\", \"Value\": 341.0 } ] } } The following JSONPath configuration reads all the TPPrototype.uflsample.value_time values from the JSON above: { \"Id\": \"Value\", \"DataType\": \"Double\", \"FieldDefinition\": \"$[\u0027StreamData\u0027].[\u0027TPPrototype.uflsample.value_time\u0027][*].Value\" }, { \"Id\": \"Time\", \"DataType\": \"DateTime\", \"FieldDefinition\": \"$[\u0027StreamData\u0027].[\u0027TPPrototype.uflsample.value_time\u0027][*].Timestamp\", \"IsIndex\": true } The following example reads specific value from complex nested JSON: { \"success\": true, \"error\": null, \"result\": { \"type\": \"runtime_history\", \"chart\": { \"chart\": { \"type\": \"column\" }, \"title\": { \"text\": \"\" }, \"subtitle\": { \"text\": \"Daily History\" }, \"colors\": [ \"#fee292\", \"#fdc152\", \"#f69638\", \"#f17130\", \"#9f2d26\", \"#8acadc\", \"#184c8e\" ], \"series\": [ { \"name\": \"Stage 3 Aux Heat\", \"data\": [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ], \"stack\": \"heat\", \"state\": \"heat_aux_stage3\" }, { \"name\": \"Stage 2 Aux Heat\", \"data\": [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ], \"stack\": \"heat\", \"state\": \"heat_aux_stage2\" }, { \"name\": \"Aux Heat\", \"data\": [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ], \"stack\": \"heat\", \"state\": \"heat_aux\" }, { \"name\": \"Stage 2 Heat\", \"data\": [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ], \"stack\": \"heat\", \"state\": \"heat_stage2\" }, { \"name\": \"Heat\", \"data\": [ 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.3, 0.2, 0.0 ], \"stack\": \"heat\", \"state\": \"heat\" }, { \"name\": \"Stage 2 Cool\", \"data\": [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ], \"stack\": \"cool\", \"state\": \"cool_stage2\" }, { \"name\": \"Cool\", \"data\": [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ], \"stack\": \"cool\", \"state\": \"cool\" } ], \"xAxis\": { \"categories\": [ \"Friday\", \"Saturday\", \"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\" ], \"labels\": { \"rotation\": -45 } }, \"yAxis\": { \"allowDecimals\": false, \"min\": 0, \"max\": 24, \"tickInternval\": 4, \"title\": { \"text\": \"Runtime (Hours)\" } }, \"legend\": { \"layout\": \"vertical\", \"align\": \"center\", \"floating\": false, \"shadow\": false, \"itemStyle\": { \"fontSize\": \"1em\" } }, \"tooltip\": { \"shared\": true, \"borderColor\": \"#000000\" }, \"credits\": { \"enabled\": false }, \"plotOptions\": { \"column\": { \"stacking\": \"normal\" }, \"series\": { \"shadow\": false } } }, \"table\": { \"headings\": [ \"Fri\", \"Sat\", \"Sun\", \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\" ], \"series\": [ { \"name\": \"Aux Heat\", \"data\": [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ], \"stack\": \"heat\", \"state\": \"heat_aux\" }, { \"name\": \"Outdoor High Temp.\", \"data\": [ 72.0, 64.0, 73.0, 72.0, 67.0, 73.0, 77.0, 62.0, 51.0 ], \"stack\": null, \"state\": \"outdoor_high_temperature\" }, { \"name\": \"Outdoor Low Temp.\", \"data\": [ 55.0, 60.0, 62.0, 61.0, 51.0, 43.0, 46.0, 44.0, 35.0 ], \"stack\": null, \"state\": \"outdoor_low_temperature\" }, { \"name\": \"Avg Indoor Temp.\", \"data\": [ 76.0, 77.0, 78.0, 78.0, 77.0, 73.0, 74.0, 75.0, 72.0 ], \"stack\": null, \"state\": \"average_indoor_temperature\" }, { \"name\": \"Avg Indoor Humidity\", \"data\": [ 66.0, 68.0, 70.0, 70.0, 69.0, 67.0, 67.0, 66.0, 61.0 ], \"stack\": null, \"state\": \"average_indoor_humidity\" }, { \"name\": \"Fan Only Runtime\", \"data\": [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ], \"stack\": null, \"state\": \"fan_only\" }, { \"name\": \"Vent\", \"data\": [], \"stack\": null, \"state\": \"vent\" } ] }, \"show_monthly_runtime_history\": true } } The following JSONPath configuration reads Sunday Average Indoor Temperature. The timestamp comes from Adapter local time. { \"Id\": \"Temperature\", \"DataType\": \"Double\", \"FieldDefinition\": \"$.result.table.series[3].data[2]\" }, { \"Id\": \"Timestamp\", \"DataType\": \"DateTime\", \"Format\": \"Adapter\", \"IsIndex\": true } Error handling If you encounter text parser related errors, errors for the ValueField or IndexField , check the StreamId associated with the error message. Possible errors include the following: The JSONPath expression of ValueField or IndexField is pointing to a non-existing value. The JSONPath expression of ValueField or IndexField is missing a value altogether. DataType does not match the value."
                                                                                                                 },
    "content/shared-content/shared-content/configuration/text-parser/text-parser.html":  {
                                                                                             "href":  "content/shared-content/shared-content/configuration/text-parser/text-parser.html",
                                                                                             "title":  "Text parser",
                                                                                             "keywords":  "Text parser The adapter you are using includes the text parser component which ensures consistent parsing of text from different files. For more information on which file types are supported for your adapter, see the topics in this chapter. Designed to be a document parser, the text parser parses a semantically complete document in its entirety. The text parser produces OMF compatible output, which in turn is compatible with the AVEVA Data Hub backing SDS (Sequential Data Store) that stores data in streams consisting of multiple values and indexes. Data types supported by the text parser The following data types are supported by the text parser: DateTime DateTimeOffset TimeSpan sbyte byte short ushort int uint long ulong float double decimal bool char string Note: Not all data types supported by the text parser are also supported by OMF. Special characters support As part of the default StreamId logic, the text parser replaces special characters as follows: Special character Replacement character * empty string \u0027 empty string ` empty string \" empty string ? empty string ; - \\| - \\ - { ( } ) [ ( ] ) Culture support Some numeric values and datetimes support cultures when they are parsed. The default culture is en-US (US English) (InvariantCulture). OSIsoft recommends that you leave the adapter at the default unless you expect culturally variant input. Note: Installed cultures vary by machine with both Linux and Windows. If the specified culture is not installed, the text parser fails to parse input that requires that culture. Time zone support A time zone or offset specified by a time is always used to convert to UTC time. Time zones are only used if there is no offset or time zone specifier in a text date and time string. For time zones that support time changes between daylight and standard times, a text file may temporarily contain invalid or ambiguous datetimes during the time change, which are possible only for a two-hour period each year. When these time changes occur, the text parser logs them, but the datetime is parsed and passed to the callback. Ambiguous times are reported as standard times, which is the Microsoft recommendation. Date and time processing The text parser can use time zones, cultures, and custom formats to read dates and times from ingress data. You can specify date and time formats when you configure data selection. Set the date and time using the IndexFormat property. If you leave the IndexFormat property unset, the data selection configuration defaults to the ISO 8601 date format. If you are using a culture other than default en-US , use the name of day or month specific to the culture. For example, use \"Juni\" instead of \"June\" for the de-DE culture. The following date and time syntaxes have been tested and are supported. \"MM/dd/yyyy \"MM dd yyyy H:mm:ss zzz\" \"06/15/2018 \"06 15 2018 15:15:30 -05:00\" \"MM/dd/yyyy \"MM dd yyyy H:mm:ss.fff zzz\" \"06/15/2018 \"06 15 2018 15:15:30.123 -05:00\" \"dd/MM/yyyy \"dd MM yyyy H:mm:ss.fff K\" \"15/06/2018 \"15 06 2018 15:15:30.123 Z\" \"MMMM/dd/yyyy \"MMMM dd yyyy H:mm:ss.fff K\" \"June/15/2018 \"June 15 2018 15:15:30.123 Z\" (InvariantCulture/English) (InvariantCulture English) \"MMMM/dd/yyyy \"MMMM dd yyyy H:mm:ss.fff K\" \"Juni/15/2018 \"Juni 15 2018 15:15:30.123 Z\" (German) \"MMM/dd/yyyy \"MMM dd yyyy H:mm:ss.fff K\" \"Jun/15/2018 \"Jun 15 2018 15:15:30.123 Z\" \"MMM-dd-yyyy H:mm:ss.fff K\" \"Jun-15-2018 15:15:30.123 Z\" \"MMM-dd-yyyy H:mm:ss.fff K\" \"Jun-15-2018 15:15:30.123 Z\" \"MMM-dd-yyyy H:mm:ss.fff K\" \"Jun-15-2018 15:15:30.123 Z\" \"yyyy-MM-dd H:mm:ss.fff K\" \"2018-06-15 15:15:30.123 Z\" \"yyyy-M-d H:mm:ss.fff K\" \"2018-6-5 15:15:30.123 Z\" \"yyyy-M-d H:mm:ss.fff zzz\" \"2018-6-5 15:15:30.123 +05:00\" \"ddd dd MMM yyyy h:mm tt zzz\" \"Sun 15 Jun 2008 8:30 AM -06:00\" \"dddd dd MMM yyyy h:mm tt zzz\" \"Sunday 15 Jun 2008 8:30 AM -06:00\" \"dddd dd MMM yyyy h:mm tt zzz\" \"Sunday 15 Jun 2008 8:30 AM -06:00\" \"dddd dd MMMM yyyy h:mm tt zzz\" \"Sunday 15 June 2008 8:30 AM -06:00\" Adapter date and time processing uses Microsoft datetime parsing . For more documentation on standard datetime formats, which fit most use cases, see Standard date and time format strings . For documentation on custom datetime formation, see Custom date and time format strings ."
                                                                                         },
    "content/shared-content/shared-content/configuration/text-parser/xpath-and-csv-syntax-for-value-retrieval.html":  {
                                                                                                                          "href":  "content/shared-content/shared-content/configuration/text-parser/xpath-and-csv-syntax-for-value-retrieval.html",
                                                                                                                          "title":  "XPath and CSV syntax for value retrieval",
                                                                                                                          "keywords":  "XPath and CSV syntax for value retrieval For information on which semantics are used for retrieving values from XML and CSV files, see the following documentation: XML - XML Path Language (XPath) CSV - Column Index (1 based) or Header value (if header defined) Use the following syntaxes to extract values from XML or CSV documents. XML - Simple XPath example \u003cvalues\u003e \u003cvalue\u003e \u003ctime\u003e2020-08-10T12:10:46.0928791Z\u003c/time\u003e \u003ctime\u003e2020-08-10T12:10:46.0928791Z\u003c time\u003e \u003cvalue\u003e1.234567890\u003c/value\u003e \u003cvalue\u003e1.234567890\u003c value\u003e \u003c/value\u003e \u003c value\u003e \u003cvalue\u003e \u003ctime\u003e2020-08-10T12:10:47.0928791Z\u003c/time\u003e \u003ctime\u003e2020-08-10T12:10:47.0928791Z\u003c time\u003e \u003cvalue\u003e12.34567890\u003c/value\u003e \u003cvalue\u003e12.34567890\u003c value\u003e \u003c/value\u003e \u003c value\u003e \u003cvalue\u003e \u003ctime\u003e2020-08-10T12:10:48.0928791Z\u003c/time\u003e \u003ctime\u003e2020-08-10T12:10:48.0928791Z\u003c time\u003e \u003cvalue\u003e123.4567890\u003c/value\u003e \u003cvalue\u003e123.4567890\u003c value\u003e \u003c/value\u003e \u003c value\u003e \u003cvalue\u003e \u003ctime\u003e2020-08-10T12:10:49.0928791Z\u003c/time\u003e \u003ctime\u003e2020-08-10T12:10:49.0928791Z\u003c time\u003e \u003cvalue\u003e1234.567890\u003c/value\u003e \u003cvalue\u003e1234.567890\u003c value\u003e \u003c/value\u003e \u003c value\u003e \u003cvalue\u003e \u003ctime\u003e2020-08-10T12:10:50.0928791Z\u003c/time\u003e \u003ctime\u003e2020-08-10T12:10:50.0928791Z\u003c time\u003e \u003cvalue\u003e12345.67890\u003c/value\u003e \u003cvalue\u003e12345.67890\u003c value\u003e \u003c/value\u003e \u003c value\u003e \u003cvalue\u003e \u003ctime\u003e2020-08-10T12:10:51.0928791Z\u003c/time\u003e \u003ctime\u003e2020-08-10T12:10:51.0928791Z\u003c time\u003e \u003cvalue\u003e123456.7890\u003c/value\u003e \u003cvalue\u003e123456.7890\u003c value\u003e \u003c/value\u003e \u003c value\u003e \u003cvalue\u003e \u003ctime\u003e2020-08-10T12:10:52.0928791Z\u003c/time\u003e \u003ctime\u003e2020-08-10T12:10:52.0928791Z\u003c time\u003e \u003cvalue\u003e12345678.90\u003c/value\u003e \u003cvalue\u003e12345678.90\u003c value\u003e \u003c/value\u003e \u003c value\u003e \u003cvalue\u003e \u003ctime\u003e2020-08-10T12:10:53.0928791Z\u003c/time\u003e \u003ctime\u003e2020-08-10T12:10:53.0928791Z\u003c time\u003e \u003cvalue\u003e123456789.0\u003c/value\u003e \u003cvalue\u003e123456789.0\u003c value\u003e \u003c/value\u003e \u003c value\u003e \u003c/values\u003e \u003c values\u003e The following XPath configuration reads a series of values: { \"Id\": \"DoubleValue\", \"FieldDefinition\": \"./values/value/value\", \". values value value\", \"DataType\": \"Double\" }, { \"Id\": \"Timestamp\", \"FieldDefinition\": \"./values/value/time\", \". values value time\", \"DataType\": \"DateTime\", \"IsIndex\": true } CSV - Simple CSV column index example 2020-08-10T12:10:46.0928791Z,1.234567890 2020-08-10T12:10:47.0928791Z,12.34567890 2020-08-10T12:10:48.0928791Z,123.4567890 2020-08-10T12:10:49.0928791Z,1234.567890 2020-08-10T12:10:50.0928791Z,12345.67890 2020-08-10T12:10:51.0928791Z,123456.7890 2020-08-10T12:10:52.0928791Z,12345678.90 2020-08-10T12:10:53.0928791Z,123456789.0 The following CSV column index configuration requires you to configure the text parser with HasHeader=false . The column indexes are 1 based and configured as strings. { \"Id\": \"DoubleValue\", \"FieldDefinition\": \"2\", \"DataType\": \"Double\" }, { \"Id\": \"Timestamp\", \"FieldDefinition\": \"1\", \"DataType\": \"DateTime\", \"IsIndex\": true } CSV - Simple CSV column header example Date,Value 2020-08-10T12:10:46.0928791Z,1.234567890 2020-08-10T12:10:47.0928791Z,12.34567890 2020-08-10T12:10:48.0928791Z,123.4567890 2020-08-10T12:10:49.0928791Z,1234.567890 2020-08-10T12:10:50.0928791Z,12345.67890 2020-08-10T12:10:51.0928791Z,123456.7890 2020-08-10T12:10:52.0928791Z,12345678.90 2020-08-10T12:10:53.0928791Z,123456789.0 The following CSV column header configuration requires you to configure the text parser with HasHeader=true . { \"Id\": \"DoubleValue\", \"FieldDefinition\": \"Value\", \"DataType\": \"Double\" }, { \"Id\": \"Timestamp\", \"FieldDefinition\": \"Date\", \"DataType\": \"DateTime\", \"IsIndex\": true }"
                                                                                                                      },
    "content/shared-content/shared-content/diagnostics/diagnostics.html":  {
                                                                               "href":  "content/shared-content/shared-content/diagnostics/diagnostics.html",
                                                                               "title":  "Diagnostics",
                                                                               "keywords":  "Diagnostics The adapter and its components produce various kinds of diagnostics data that is sent to all health endpoints. The System_Diagnostics.json file contains a flag that determines whether diagnostics are enabled. You can change this at runtime through REST calls or the EdgeCmd utility. Diagnostics data are collected by default. To egress diagnostics related data, you have to configure an adapter health endpoint first. See Health endpoint configuration . Available diagnostics data Every minute, dynamic data is sent to configured health endpoints. The following diagnostics data are available: System Stream count IO rate Error rate"
                                                                           },
    "content/shared-content/shared-content/diagnostics/egress.html":  {
                                                                          "href":  "content/shared-content/shared-content/diagnostics/egress.html",
                                                                          "title":  "Egress",
                                                                          "keywords":  "Egress The Egress component of the adapter produces the following diagnostics stream: IO rate The Diagnostics.Egress.IORate dynamic type includes the following values, which are logged in a stream with the Id {machineName}.{serviceName}.OmfEgress.{EndpointId}.IORate . IORate includes only sequential data successfully sent to an egress endpoint. Property Type Description timestamp string Timestamp of event IORate double One-minute rolling average of data rate (streams/second) (streams second)"
                                                                      },
    "content/shared-content/shared-content/diagnostics/error-rate.html":  {
                                                                              "href":  "content/shared-content/shared-content/diagnostics/error-rate.html",
                                                                              "title":  "Error rate",
                                                                              "keywords":  "Error rate The Diagnostics.Adapter.ErrorRate dynamic type includes the following values, which are logged in a stream with the Id {componentid}.ErrorRate . Property Type Description timestamp string Timestamp of event ErrorRate double One-minute rolling average of error rate (streams/second) (streams second)"
                                                                          },
    "content/shared-content/shared-content/diagnostics/io-rate.html":  {
                                                                           "href":  "content/shared-content/shared-content/diagnostics/io-rate.html",
                                                                           "title":  "IO rate",
                                                                           "keywords":  "IO rate The Diagnostics.Adapter.IORate dynamic type includes the following values, which are logged in a stream with the Id {componentid}.IORate . IORate includes only sequential data collected from a data source. Property Type Description timestamp string Timestamp of event IORate double One-minute rolling average of data rate (streams/second) (streams second)"
                                                                       },
    "content/shared-content/shared-content/diagnostics/stream-count.html":  {
                                                                                "href":  "content/shared-content/shared-content/diagnostics/stream-count.html",
                                                                                "title":  "Stream count",
                                                                                "keywords":  "Stream count The Diagnostics.StreamCountEvent dynamic type includes the following values, which are logged in a stream with the Id {componentid}.StreamCount . The StreamCount and TypeCount include only types and streams created for sequential data received from a data source. Property Type Description timestamp string Timestamp of event StreamCount int Number of streams created by the adapter instance TypeCount int Number of types created by the adapter instance"
                                                                            },
    "content/shared-content/shared-content/diagnostics/system.html":  {
                                                                          "href":  "content/shared-content/shared-content/diagnostics/system.html",
                                                                          "title":  "System",
                                                                          "keywords":  "System The Diagnostics.System dynamic type includes the following values which are logged in a stream with the Id System.Diagnostics . This diagnostic stream contains system level information related to the host platform that the adapter is running on. Property Type Description timestamp string Timestamp of event ProcessIdentifier int Process Id of the host process StartTime string Time at which the host process started WorkingSet long Amount of physical memory in bytes, allocated for the host process TotalProcessorTime double Total processor time for the host process expressed in seconds TotalUserProcessorTime double User processor time for the host process expressed in seconds TotalPrivilegedProcessorTime double Privileged processor time for the host process expressed in seconds ThreadCount int Number of threads in the host process HandleCount int Number of handles opened by the host process ManagedMemorySize double Number of bytes currently thought to be allocated in managed memory Unit of Measure = megabytes PrivateMemorySize double Amount of paged memory in bytes allocated for the host process Unit of Measure = megabytes PeakPagedMemorySize double Maximum amount of memory in the virtual memory paging file in bytes used by the host process. Unit of Measure = megabytes StorageTotalSize double Total size of the storage medium in use by the system Unit of Measure = megabytes StorageFreeSpace double Free space available Unit of Measure = megabytes Each adapter component produces its own diagnostics streams."
                                                                      },
    "content/shared-content/shared-content/installation/installation.html":  {
                                                                                 "href":  "content/shared-content/shared-content/installation/installation.html",
                                                                                 "title":  "Installation",
                                                                                 "keywords":  "Installation Adapters are installed on a local machine using an install kit downloaded from the OSIsoft Customer Portal. For instructions on downloading and installing adapters, see Install the adapter . Alternatively, you can build custom installers or containers for Linux. For more information, see the Docker instructions in the documentation of the respective adapter."
                                                                             },
    "content/shared-content/shared-content/installation/install-the-adapter.html":  {
                                                                                        "href":  "content/shared-content/shared-content/installation/install-the-adapter.html",
                                                                                        "title":  "Install the adapter",
                                                                                        "keywords":  "Install the adapter You can install adapters on either a Windows or Linux operating system. Before installing the adapter, see the respective system requirements to ensure your machine is properly configured to provide optimum adapter operation. Windows Complete the following steps to install a PI adapter on a Windows computer: Download \u003cINSTALLER_NAME\u003e-x64_.msi from the OSIsoft Customer portal . Note: Customer login credentials are required to access the portal. Run \u003cINSTALLER_NAME\u003e-x64_.msi file. Follow the setup wizard. You can change the installation folder or port number during setup. The default port number is 5590 . Optional: To verify the installation, run the following curl command with the port number that you specified during installation: curl http://localhost:5590/api/v1/configuration http:  localhost:5590 api v1 configuration If you receive an error, wait a few seconds and try the script again. If the installation was successful, a JSON copy of the default system configuration is returned. Linux Complete the following steps to install an adapter on a Linux computer: Download the appropriate Linux distribution file ( \u003cINSTALLER_NAME\u003e- platform _.deb ) from the OSIsoft Customer portal . Note: Customer login credentials are required to access the portal. Open a terminal. Run the sudo apt update command to update available packages information. Run the sudo apt install command against the Linux distribution file ( \u003cINSTALLER_NAME\u003e- platform _.deb ) selected in step 1 of this procedure. For example: sudo apt install ./\u003cINSTALLER_NAME\u003e-x64_.deb . \u003cINSTALLER_NAME\u003e-x64_.deb Optional: To verify the installation, run the following curl command with the port number that you specified during installation: curl http://localhost:5590/api/v1/configuration http:  localhost:5590 api v1 configuration If you receive an error, wait a few seconds and run the command again. If the installation was successful, a JSON copy of the default system configuration is returned."
                                                                                    },
    "content/shared-content/shared-content/installation/install-using-docker.html":  {
                                                                                         "href":  "content/shared-content/shared-content/installation/install-using-docker.html",
                                                                                         "title":  "Installation using Docker",
                                                                                         "keywords":  "Installation using Docker Docker is a set of tools that you can use on Linux to manage application deployments. This topic provides examples of how to create a Docker container with the adapter. Note: The use of Docker is only recommended if your environment requires it. Only users proficient with Docker should use it to install the adapter. Docker is not required to use the adapter. Create a startup script To create a startup script for the adapter, follow the instructions below. Use a text editor to create a script similar to one of the following examples: Note: The script varies slightly by processor. \u003c!-- PRERELEASE REMINDER: Update {adapter} and {version} placeholders. Example: bacnet, 1.1.0.192 --\u003e ARM32 #!/bin/sh #! bin sh if [ -z $portnum ] ; then exec /PI-Adapter-for-{adapter}_{version}-arm_/OSIsoft.Data.System.Host  PI-Adapter-for-{adapter}_{version}-arm_ OSIsoft.Data.System.Host else exec /PI-Adapter-for-{adapter}_{version}-arm_/OSIsoft.Data.System.Host  PI-Adapter-for-{adapter}_{version}-arm_ OSIsoft.Data.System.Host --port:$portnum fi ARM64 #!/bin/sh #! bin sh if [ -z $portnum ] ; then exec /PI-Adapter-for-{adapter}_{version}-arm64_/OSIsoft.Data.System.Host  PI-Adapter-for-{adapter}_{version}-arm64_ OSIsoft.Data.System.Host else exec /PI-Adapter-for-{adapter}_{version}-arm64_/OSIsoft.Data.System.Host  PI-Adapter-for-{adapter}_{version}-arm64_ OSIsoft.Data.System.Host --port:$portnum fi AMD64 #!/bin/sh #! bin sh if [ -z $portnum ] ; then exec /PI-Adapter-for-{adapter}_{version}-x64_/OSIsoft.Data.System.Host  PI-Adapter-for-{adapter}_{version}-x64_ OSIsoft.Data.System.Host else exec /PI-Adapter-for-{adapter}_{version}-x64_/OSIsoft.Data.System.Host  PI-Adapter-for-{adapter}_{version}-x64_ OSIsoft.Data.System.Host --port:$portnum fi Name the script {adapter}dockerstart.sh and save it to the directory where you plan to create the container. \u003c!-- PRERELEASE REMINDER: Update {adapter} placeholders. Example: bacnet --\u003e Create a Docker container To create a Docker container that runs the adapter, follow the instructions below. Create the following Dockerfile in the directory where you want to create and run the container. Note: Dockerfile is the required name of the file. Use the variation according to your operating system: \u003c!-- PRERELEASE REMINDER: Update {adapter} and {version} placeholders. Example: bacnet, 1.1.0.192 --\u003e ARM32 FROM ubuntu WORKDIR /   RUN apt-get update \u0026\u0026 DEBIAN_FRONTEND=noninteractive apt-get install -y ca-certificates libicu60 libssl1.1 curl COPY {adapter}dockerstart.sh /   RUN chmod +x /{adapter}dockerstart.sh  {adapter}dockerstart.sh ADD ./PI-Adapter-for-{adapter}_{version}-arm_.tar.gz . PI-Adapter-for-{adapter}_{version}-arm_.tar.gz . ENTRYPOINT [\"/{adapter}dockerstart.sh\"] [\" {adapter}dockerstart.sh\"] ARM64 FROM ubuntu WORKDIR /   RUN apt-get update \u0026\u0026 DEBIAN_FRONTEND=noninteractive apt-get install -y ca-certificates libicu66 libssl1.1 curl COPY {adapter}dockerstart.sh /   RUN chmod +x /{adapter}dockerstart.sh  {adapter}dockerstart.sh ADD ./PI-Adapter-for-{adapter}_{version}-arm64_.tar.gz . PI-Adapter-for-{adapter}_{version}-arm64_.tar.gz . ENTRYPOINT [\"/{adapter}dockerstart.sh\"] [\" {adapter}dockerstart.sh\"] AMD64 (x64) FROM ubuntu WORKDIR /   RUN apt-get update \u0026\u0026 DEBIAN_FRONTEND=noninteractive apt-get install -y ca-certificates libicu66 libssl1.1 curl COPY {adapter}dockerstart.sh /   RUN chmod +x /{adapter}dockerstart.sh  {adapter}dockerstart.sh ADD ./PI-Adapter-for-{adapter}_{version}-x64_.tar.gz . PI-Adapter-for-{adapter}_{version}-x64_.tar.gz . ENTRYPOINT [\"/{adapter}dockerstart.sh\"] [\" {adapter}dockerstart.sh\"] Copy the \u003cINSTALLER_NAME\u003e- platform _.tar.gz file to the same directory as the Dockerfile . Copy the \u003cscript_name.sh\u003e script to the same directory as the Dockerfile . Run the following command line in the same directory ( sudo may be necessary): \u003c!-- PRERELEASE REMINDER: Customize for {docker-image}. Example:bacnetadapter --\u003e docker build -t {docker-image} . Docker container startup The following procedures contain instructions on how to run the adapter inside a Docker container with different options enabled. Run the Docker container with REST access enabled To run the adapter inside a Docker container with access to its REST API from the local host, complete the following steps: Use the docker container image \u003cscript_name.sh\u003e created previously. Type the following in the command line ( sudo may be necessary): \u003c!-- PRERELEASE REMINDER: Customize for {docker-image}. Example:bacnetadapter --\u003e docker run -d --network host {docker-image} Port 5590 is accessible from the host and you can make REST calls to the adapter from applications on the local host computer. In this example, all data stored by the adapter is stored in the container itself. When you delete the container, the stored data is also deleted. Run the Docker container with persistent storage To run the adapter inside a Docker container while using the host for persistent storage, complete the following steps. This procedure also enables access to the adapter REST API from the local host. Use the docker container image \u003cscript_name.sh\u003e created previously. Type the following in the command line ( sudo may be necessary): \u003c!-- PRERELEASE REMINDER: Customize for {adapter} and {container-name}. Example:bacnetadapter, bacnet --\u003e docker run -d --network host -v /{adapter}:/usr/share/OSIsoft/  {adapter}: usr share OSIsoft  {container-name} Port 5590 is accessible from the host and you can make REST calls to the adapter from applications on the local host computer. In this example, all data that is written to the container is instead written to the host directory and the host directory is a directory on the local machine, \u003c!-- customize --\u003e /{adapter}  {adapter} . You can specify any directory. Change port number To use a different port other than 5590 , you can specify a portnum variable on the docker run command line. For example, to start the adapter using port 6000 instead of 5590 , use the following command: \u003c!-- PRERELEASE REMINDER: Customize for {container-name}. Example:bacnetadapter --\u003e docker run -d -e portnum=6000 --network host {container-name} This command accesses the REST API with port 6000 instead of port 5590 . The following curl command returns the configuration for the container. curl http://localhost:6000/api/v1/configuration http:  localhost:6000 api v1 configuration Remove REST access If you remove the --network host option from the docker run command, REST access is not possible from outside the container. This may be of value where you want to host an application in the same container as the adapter but do not want to have external REST access enabled."
                                                                                     },
    "content/shared-content/shared-content/installation/uninstall-the-adapter.html":  {
                                                                                          "href":  "content/shared-content/shared-content/installation/uninstall-the-adapter.html",
                                                                                          "title":  "Uninstall the adapter",
                                                                                          "keywords":  "Uninstall the adapter Complete the procedure corresponding to your specific operating system to uninstall the adapter: Windows To delete the PI adapter program files from a Windows device, use the Windows Control Panel uninstall application process. Note: The configuration, data, and log files are not deleted by the uninstall process. Optional: To delete data, configuration, and log files, delete the directory: %ProgramData%\\OSIsoft\\Adapters\\\\[!include[product-name](../_includes/inline/component-type.md)] %ProgramData%\\OSIsoft\\Adapters\\\\[!include[product-name](.. _includes inline component-type.md)] This deletes all data processed by the adapter, in addition to the configuration and log files. Linux To delete PI Adapter software from a Linux device, open a terminal window and run the following command: \u003c!-- PRERELEASE REMINDER: Customize for {adapter-name}. Example:BACnet, EventHubs, StructuredDataFiles, etc --\u003e sudo apt remove pi.adapter.{adapter-name} Optional: To delete data, configuration, and log files, run the following command: \u003c!-- PRERELEASE REMINDER: Customize for {adapter-name}. Example:BACnet, EventHubs, StructuredDataFiles, etc --\u003e sudo rm -r /usr/share/OSIsoft/Adapters/{adapter-name}  usr share OSIsoft Adapters {adapter-name} This deletes all data processed by the adapter in addition to the configuration and log files."
                                                                                      },
    "content/shared-content/shared-content/installation/upgrade-the-adapter.html":  {
                                                                                        "href":  "content/shared-content/shared-content/installation/upgrade-the-adapter.html",
                                                                                        "title":  "Upgrade the adapter",
                                                                                        "keywords":  "Upgrade the adapter When a new version of the adapter is released, you can upgrade to the latest version by running the new installation package. Windows upgrade Complete the following steps to upgrade a PI adapter on a Windows computer: Download \u003cINSTALLER_NAME\u003e-x64_.msi from the OSIsoft Customer Portal . Note: Customer login credentials are required to access the portal. Run \u003cINSTALLER_NAME\u003e-x64_.msi . Complete the setup wizard. Optional: To verify the upgrade, run the following curl command with the port number that you specified when completing the wizard: curl -X GET \"http://localhost:5590/api/v1/Diagnostics/ProductInformation\" \"http:  localhost:5590 api v1 Diagnostics ProductInformation\" Upon successful upgrade, the JSON response lists the updated application version: { \"Application Version\": \"1.1.0.128\", //    upgraded version \".Net Core Version\": \".NET Core 3.1.15\", \"Operating System\": \"Microsoft Windows 10.0.19041\" } Linux upgrade Complete the following steps to upgrade a PI adapter on a Linux computer: Download the appropriate Linux distribution file from the OSIsoft Customer Portal . Note: Customer login credentials are required to access the portal. Open a terminal session. Move the Linux distribution file to the target host and run the sudo apt upgrade command. Platform Command * Linux x64 sudo apt upgrade ./\u003cINSTALLER_NAME\u003e-x64_.deb . \u003cINSTALLER_NAME\u003e-x64_.deb * Linux ARM32 Debian sudo apt upgrade ./\u003cINSTALLER_NAME\u003e-arm_.deb . \u003cINSTALLER_NAME\u003e-arm_.deb * Linux ARM64 Debian sudo apt upgrade ./\u003cINSTALLER_NAME\u003e-arm64_.deb . \u003cINSTALLER_NAME\u003e-arm64_.deb Optional: To verify the upgrade, run the following curl command with the port number that you specified: curl -X GET \"http://localhost:5590/api/v1/Diagnostics/ProductInformation\" \"http:  localhost:5590 api v1 Diagnostics ProductInformation\" Upon successful upgrade, the JSON response lists the updated application version: { \"Application Version\": \"1.1.0.128\", //    upgraded version \".Net Core Version\": \".NET Core 3.1.15\", \"Operating System\": \"Microsoft Windows 10.0.19041\" }"
                                                                                    },
    "content/shared-content/shared-content/introduction/intro-to-pi-adapters.html":  {
                                                                                         "href":  "content/shared-content/shared-content/introduction/intro-to-pi-adapters.html",
                                                                                         "title":  "\u003cPRODUCT_NAME\u003e \u003cPRODUCT_VERSION\u003e",
                                                                                         "keywords":  "\u003cPRODUCT_NAME\u003e \u003cPRODUCT_VERSION\u003e \u003cPRODUCT_NAME\u003e is a data collection technology that collects time-series operations data from a data source over the protocol and then sends it to a supported storage location in the Open Message Format (OMF). Subtree wiki article \u003c!-- Add content about the protocol here --\u003e \u003cPRODUCT_NAME\u003e data flow The following diagram depicts the collection and processing of data for an operational \u003cPRODUCT_NAME\u003e, collecting and processing data. Refer to the list below the diagram for more information on each callout depicted. \u003c!-- Mark Bishop 3/3/22: 3 3 22: The SVG file referenced below can be opened and edited using https://app.diagrams.net/ https:  app.diagrams.net  --\u003e The user installs and configures \u003cPRODUCT_NAME\u003e on a host system. You can configure the adapter using either a REST interface or EdgeCmd, a command line utility specifically designed for interfacing with edge systems. The adapter collects data from assets over the protocol, a process known as data ingress . The adapter converts ingress data to the Open Message Format (OMF), a format that supported storage locations understand. The adapter sends OMF data to a supported storage location in a process known as data egress . Supported egress endpoints include: PI Server AVEVA Data Hub"
                                                                                     },
    "content/shared-content/shared-content/metadata/metadata.html":  {
                                                                         "href":  "content/shared-content/shared-content/metadata/metadata.html",
                                                                         "title":  "Metadata",
                                                                         "keywords":  "Metadata If you set the metadataLevel to Low , Medium , or High in the General configuration , adapter streams created by the ingress components include the following metadata: Datasource: {ComponentId} AdapterType: {ComponentType} ComponentId corresponds to the adapter components\u0027 data source configured in the Components configuration . ComponentType corresponds to the adapter type. Metadata for health and diagnostics streams If you configure a health endpoint and enable metadata, they are included in the health streams ( Device status and Next health message expected ) together with ComponentId and ComponentType . If you enable diagnostics in General configuration , metadata are included in the diagnostics streams ( Stream count , IO rate , Error rate ) together with ComponentId and ComponentType . The adapter may also send its own stream metadata that does not include health and diagnostics streams. For more information about what custom metadata is included in each stream, see the user guide for your adapter. Note: Metadata is only sent for streams created by the ingress components. Currently, the only endpoint that persists sent metadata is AVEVA Data Hub (AVEVA Data Hub)."
                                                                     },
    "content/shared-content/shared-content/technical-support-and-feedback.html":  {
                                                                                      "href":  "content/shared-content/shared-content/technical-support-and-feedback.html",
                                                                                      "title":  "Technical support and feedback",
                                                                                      "keywords":  "Technical support and feedback OSIsoft provides several ways to report issues and provide feedback on PI Adapters. Technical support For technical assistance with PI Adapters, contact OSIsoft Technical Support through the OSIsoft Customer Portal . We can help you identify the problem, provide workarounds and address any concerns you may have. Remote access to your facilities may be necessary during the session. Note: You must have an account set up in the OSIsoft Customer Portal before you can open a case. If you do not have a portal account, see How to Get a Login to OSIsoft Customer Portal . Alternatively, call OSIsoft Technical Support at +1 510-297-5828. When you contact OSIsoft Technical Support, be prepared to provide this information: Product name, version, and build numbers Details about your computer platform (CPU type, operating system, and version number) Time that the difficulty started Log files at that time Details of any environment changes prior to the start of the issue Summary of the issue, including any relevant log files during the time the issue occurred \u003c!--To view a brief primer on PI Adapters, see the [PI Adapters playbook](https://customers.osisoft.com/s/knowledgearticle?knowledgeArticleUrl=Playbook-PI-adapters) playbook](https:  customers.osisoft.com s knowledgearticle?knowledgeArticleUrl=Playbook-PI-adapters) in the OSIsoft Customer Portal.--\u003e Product feedback To submit product feedback for PI Adapters, visit the PI Adapters feedback page . The product team at OSIsoft regularly monitors the page. Documentation feedback To submit documentation feedback for PI Adapters, send an email to documentation@osisoft.com . Be sure to include the following information with your feedback: Product name and version Documentation topic URL Details of the suggestion or error The technical documentation team at OSIsoft will review and respond to your feedback."
                                                                                  },
    "content/shared-content/shared-content/troubleshooting/troubleshooting.html":  {
                                                                                       "href":  "content/shared-content/shared-content/troubleshooting/troubleshooting.html",
                                                                                       "title":  "Troubleshooting",
                                                                                       "keywords":  "Troubleshooting PI adapters provide features for troubleshooting issues related to connectivity, data flow, and configuration. Resources include adapter logs and the Wireshark troubleshooting tool. If you are still unable to resolve issues or need additional guidance, contact OSIsoft Technical Support through the OSIsoft Customer Portal . Note: Make sure to also check the troubleshooting information specific to your adapter in this user guide. Logs Messages from the System and OmfEgress logs provide information on the status of the adapter. For example, they show if a connection from the adapter to an egress endpoint exists. Perform the following steps to view the System and OmfEgress logs: Navigate to the logs directory: Windows : %ProgramData%\\OSIsoft\\Adapters\\\u003cAdapterName\u003e\\Logs Linux : /usr/share/OSIsoft/Adapters/\u003cAdapterName\u003e/Logs  usr share OSIsoft Adapters \u003cAdapterName\u003e Logs . Example: A successful connection to a PI Web API egress endpoint displays the following message in the OmfEgress log: 2020-11-02 11:08:51.870 -06:00 [Information] Data will be sent to the following OMF endpoint: Id: \u003comfegress id\u003e Endpoint: \u003cpi web api URL\u003e (note: the pi web api default port is 443) ValidateEndpointCertificate: \u003ctrue or false\u003e Optional: Change the log level of the adapter to receive more information and context. For more information, see Logging configuration . ASP .NET Core platform log The ASP .NET Core platform log provides information from the Kestrel web server that hosts the application. The log could contain information that the adapter is overloaded with incoming data. Perform the following steps to spread the load among multiple adapters: Decrease the scan frequency. Lower the amount of data selection items. Wireshark Wireshark is a protocol-specific troubleshooting tool that supports all current adapter protocols. Perform the following steps if you want to use Wireshark to capture traffic from the data source to the adapter or from the adapter to the OMF destination. Download Wireshark . Familiarize yourself with the tool and read the Wireshark user guide . Health and diagnostics egress to PI Web API The adapter sends health and diagnostics data to PI Web API; in some cases, conflicts may occur that are due to changes or perceived changes in PI Web API. For example, a 409 - Conflict error message displays if you upgrade your adapter version and the PI points do not match in the upgraded version; however, data continues to be sent as long as containers are created so buffering only starts if no containers are created. To resolve the conflict, perform the following steps: Stop the adapter. Delete the Health folder inside of the Buffers folder. Stop PI Web API. Delete the relevant adapter created AF structure. Delete the associated health and diagnostics PI points on any or all PI Data Archives created by PI Web API. Start PI Web API. Start the adapter. Adapter connection to egress endpoint Certain egress health information in both PI Web API and AVEVA Data Hub show if an adapter connection to an egress endpoint exists. To verify an active connection, perform one of the following procedures: PI Web API connection Perform the following steps to determine if a connection to the PI Web API endpoint exists: Open PI Web API. Select the OmfEgress component of your adapter, for example GVAdapterUbuntu.\u003cAdapterName\u003e.OmfEgress . Make sure that the following PI points have been created for your egress endpoint: DeviceStatus NextHealthMessageExpected IORate AVEVA Data Hub connection Perform the following steps to determine if a connection to the AVEVA Data Hub endpoint exists: Open AVEVA Data Hub. Click Sequential Data Store \u003e Streams . Makes sure that the following streams have been created for your egress endpoint: DeviceStatus NextHealthMessageExpected IORate TCP connection Perform the following steps to see all established TCP sessions in Linux: Open a terminal. Type the following command: ss -o state established -t -p Press Enter."
                                                                                   },
    "content/support/edge-data-store-support.html":  {
                                                         "href":  "content/support/edge-data-store-support.html",
                                                         "title":  "Edge Data Store support",
                                                         "keywords":  "Edge Data Store support Documentation feedback If you have questions, comments, or other feedback about the Edge Data Store documentation, send an email to documentation@osisoft.com . Technical support and other resources For technical assistance, contact OSIsoft Technical Support at +1 510-297-5828 or through the OSIsoft Customer Portal Contact Us page . The Contact Us page offers additional contact options for customers outside of the United States. When you contact OSIsoft Technical Support, be prepared to provide this information: Product name, version, and build numbers Details about your computer platform (CPU type, operating system, and version number) Time that the difficulty started Log files at that time Details of any environment changes prior to the start of the issue Summary of the issue, including any relevant log files during the time the issue occurred To ask questions of others who use OSIsoft software, join the OSIsoft user community, PI Square . Members of the community can request advice and share ideas about the PI System. The PI Developers Club space within PI Square offers resources to help you with the programming and integration of OSIsoft products."
                                                     },
    "content/troubleshooting/disaster-recovery.html":  {
                                                           "href":  "content/troubleshooting/disaster-recovery.html",
                                                           "title":  "Disaster recovery",
                                                           "keywords":  "Disaster recovery If a device with Edge Data Store installed fails, use the following procedures to recover data from the failed device and restore it to a new device. The disaster recovery process is similar for both Windows and Linux systems and includes the following steps: Back up the EDS data from the failed device to another location. Install EDS on a new device. For instructions, see Install Edge Data Store . Move the backed-up data to the new device. Restore the backed-up data files to the new device. Reenter credentials. Windows recovery Use the following procedures to recover EDS data from a Windows device. You need administrative access on the device to successfully restore on a Windows system. Create a backup of EDS data from the failed device To create a backup of data from the failed device: If your device is still able to boot, verify that Edge Data Store service has stopped. Use the Windows Task Manager to stop the Edge Data Store service. Locate the storage and configuration files. Note: Windows storage and configuration files are stored in the following locations: C:\\ProgramData\\OSIsoft\\EdgeDataStore\\Configuration C:\\ProgramData\\OSIsoft\\EdgeDataStore\\Storage Stream data files are stored in the following locations: Default namespace: C:\\ProgramData\\OSIsoft\\EdgeDataStore\\Storage\\data\\default\\default Diagnostics namespace: C:\\ProgramData\\OSIsoft\\EdgeDataStore\\Storage\\data\\default\\diagnostics The ProgramData folder is typically hidden; to view it, go to the View tab in Windows Explorer and select the Hidden Items checkbox. Create a zip or tar file containing the storage and configuration directories, and move it to a USB device or other safe location. File transfer can be done with WinSCP, SFTP, or external device. Delete default storage and configuration folders When EDS is installed on the new device, the new system has a default configuration. To delete the default storage and configuration folders on the new device: Use the Windows Task Manager to stop the Edge Data Store service. Once the service has stopped, navigate to the C:\\ProgramData\\OSIsoft\\EdgeDataStore directory, and delete the default storage and configuration folders from the new device. Restore backed up data files To restore the data files on the new device: Copy or unzip the backup storage and configuration files into the C:\\ProgramData\\OSIsoft\\EdgeDataStore directory. Note: The C: drive may not be the default drive letter of your system. Refer to My Computer, This PC, or open a command prompt to verify the default drive letter. Use the Windows Task Manager to restart the Edge Data Store service. Reenter credentials All credentials are encrypted for security purposes, so they cannot be copied or transferred. After the the storage and configuration files are copied to the new system, and the service has started, follow these steps to enter credentials: Reenter the credentials for the operating system using API calls. After updating, restart the Edge Data Store service. The new EDS device runs as the previous device, and contains all the data up to the point when the previous device failed. Linux recovery Use the following procedures to recover EDS data from a Linux device. Root access on the Linux device is required. Create a backup of EDS data from the failed device To create a backup of data from the failed device: If your device is still able to boot, open a terminal window and verify that Edge Data Store service has stopped using the following command: sudo systemctl stop osisoft.edgedatastore Locate the storage and configuration files. Note: Linux storage and configuration files are stored in the following locations: /usr/share/OSIsoft/EdgeDataStore/Configuration  usr share OSIsoft EdgeDataStore Configuration /usr/share/OSIsoft/EdgeDataStore/Storage  usr share OSIsoft EdgeDataStore Storage Stream data files are stored in the following locations: Default namespace: /usr/share/OSIsoft/EdgeDataStore/Storage/data/default/default  usr share OSIsoft EdgeDataStore Storage data default default Diagnostics namespace: /usr/share/OSIsoft/EdgeDataStore/Storage/data/default/diagnostics  usr share OSIsoft EdgeDataStore Storage data default diagnostics Create a zip or tar file containing the storage and configuration directories, and move it to a USB device or other safe location. Use WinSCP, SFTP, or the external device to transfer the file. Note: Using the cp command may result in a change in file ownership to the current user. Move the files to the new device When EDS is installed on the new device, the new system has a default configuration. To copy the backed up storage and configuration folders to the new device: Open a terminal window and stop the Edge Data Store service using the following command: sudo systemctl stop osisoft.edgedatastore Once the service has stopped, navigate to the /usr/share/OSIsoft/EdgeDataStore  usr share OSIsoft EdgeDataStore directory and extract the zip or tar file in that directory using WinSCP, SFTP, or external device. Restore backed up data files To restore the data files on the new device: Delete the default storage and configuration folders from the /usr/share/OSIsoft/EdgeDataStore  usr share OSIsoft EdgeDataStore directory. Copy or unzip the backup storage and configuration files into the EdgeDataStore directory. If the ownership of the two directories does not match, update it to edgedatastore for the user and group. Start the Edge Data Store service with the following command: sudo systemctl start osisoft.edgedatastore Verify that EDS is running with the following command: sudo systemctl status osisoft.edgedatastore Note: Default directory permissions are set to 755 , and each subsequent file is 644 . If you do not use tar it is possible to have permission issues with the recovery files. Tar matches via string name rather than the account ID/UID. ID UID. Reenter credentials All credentials are encrypted for security purposes, so they cannot be copied or transferred. After the the storage and configuration files are copied to the new system, and the service has started, follow these steps to enter credentials: Re-enter the credentials for the operating system using API calls. After updating, restart the Edge Data Store service. The new EDS device runs as the previous device, and contains all the data up to the point when the previous device failed."
                                                       },
    "content/troubleshooting/troubleshooting.html":  {
                                                         "href":  "content/troubleshooting/troubleshooting.html",
                                                         "title":  "Troubleshoot Edge Data Store",
                                                         "keywords":  "Troubleshoot Edge Data Store Edge Data Store includes both local and remote means of diagnosing issues encountered while using or developing against EDS. Edge Data Store supports a diagnostics namespace that stores streams containing diagnostic information from Edge Data Store itself. Egress this data to either a PI Server or AVEVA Data Hub to monitor the state of a system remotely. For details about egressing diagnostic data, see Diagnostics configuration . In addition to diagnostics data, all components in Edge Data Store support OMF health messages. Configure health messages to send health data to either PI Server or AVEVA Data Hub endpoints for remote monitoring of devices. For more information, see Health endpoints configuration . OMF ingress When a custom application fails to write stream data to EDS: Verify the custom application is sending OMF messages in the correct order: 1) OMF type, 2) OMF container, 3) OMF data. Note: OMF messages must be sent in the correct order to be ingressed into Edge Data Store. Refer to logging of warnings, errors, and messages for help with diagnosing these issues. OMF ingress logging Ingress logging messages provide a record of ingress events. To capture the most information for troubleshooting: Refer to Configure logging to set logging parameters. For maximum message logging information, set the log level to Trace . OMF ingress message debugging Every incoming OMF message logs a request and response log file. Log files are located in the following location and grouped by the associated OMF message type: C:\\ProgramData\\OSIsoft\\EdgeDataStore\\Logs\\IngressDebugLogs\\ Each log file leverages the following filename format: {ticks}-{operationId}-{Request/Response}.txt {ticks}-{operationId}-{Request Response}.txt Use debugging information to troubleshoot problems between an OMF application and Edge Data Store. To enable debugging: Refer to Storage runtime configuration to enable debugging. Set an appropriate time value for the IngressDebugExpiration property. Note: You can also disable debugging by setting the expiration value to null . Date and time strings should use the following formats: UTC: yyyy-mm-ddThh:mm:ssZ Local: yyyy-mm-ddThh:mm:ss Egress EDS egress extracts data from SDS streams and sends the appropriate sequences of type, container, and data OMF messages on startup. If unexpected data appears in an AVEVA Data Hub or PI System, check if multiple devices are writing to the same SDS stream. To check egress configuration: Check all egress configuration files in Edge Data Store to verify whether any endpoints are duplicated. A duplicate endpoint means that more than one device is egressing data to it, resulting in unexpected data. Assign stream prefixes in the periodic egress configuration to ensure that output data streams are logically separated in the systems of record. For instructions, see Configure data egress . Note: Type prefixes may be helpful if you have changed a stream type definition on EDS. OMF types on both AVEVA Data Hub and the PI System are immutable once created. If the type of the data stream changes, it is best to either delete the old type definition (if nothing is still using it) or add a type prefix to create a new unique type that will be used by new streams egressing from EDS to the systems of record. Egress logging Egress logging messages provide a record of egress events. To capture maximum information for troubleshooting: Refer to Configure logging to set logging parameters. For maximum message logging information, set the log level to Trace . Egress debugging Use debugging information to troubleshoot problems between Edge Data Store and the egress destination. To enable debugging: Refer to Data egress configuration to enable debugging. Set an appropriate time value for the DebugExpiration property. Note: Disable debugging by setting the expiration value to null . Date and time strings should use the following formats: UTC: yyyy-mm-ddThh:mm:ssZ Local: yyyy-mm-ddThh:mm:ss Debugging folder/file folder file structure Debug folders and files are created under the Edge Data Store folder as follows: Windows: %programdata%\\OSIsoft\\EdgeDataStore\\Logs\\EgressDebugLogs\\Data\\{egressType}\\{egressId}\\{omfType}\\{Ticks}-{Guid}-{Request/Response}.txt %programdata%\\OSIsoft\\EdgeDataStore\\Logs\\EgressDebugLogs\\Data\\{egressType}\\{egressId}\\{omfType}\\{Ticks}-{Guid}-{Request Response}.txt Linux: /usr/share/OSIsoft/EdgeDataStore/Logs/EgressDebugLogs/Data{egressType}/{egressId}/{omfType}/{Ticks}-{Guid}-{Request/Response}.txt  usr share OSIsoft EdgeDataStore Logs EgressDebugLogs Data{egressType} {egressId} {omfType} {Ticks}-{Guid}-{Request Response}.txt The OMF specific elements of the file structure are defined in the following table. Element Represents omfType The OMF message type: Type, Container, or Data. Ticks The time in milliseconds (tick count) for UTC DateTime when determined message would be written to disk. Guid The unique GUID for each request/response request response pair. Req/Res Req Res Whether the message was HTTP request or response. Additional resources For information about specific log messages and possible resolutions, refer to the OSIsoft Customer Portal ."
                                                     },
    "README.html":  {
                        "href":  "README.html",
                        "title":  "Edge Data Store",
                        "keywords":  "Edge Data Store Edge Data Store (EDS) is a lightweight data collection and storage application designed to capture data at the edge of networks for historical storage and analysis. This repository contains the documentation for Edge Data Store. You can access a readable version of this documentation here . License ?? 2020 - 2022 OSIsoft, LLC. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at: http://www.apache.org/licenses/LICENSE-2.0 http:  www.apache.org licenses LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
                    },
    "z-archive/Reference/Edgecmd commands_1-0.html":  {
                                                          "href":  "z-archive/Reference/Edgecmd commands_1-0.html",
                                                          "title":  "EdgeCmd commands",
                                                          "keywords":  "EdgeCmd commands The following tables list the commands available in the EdgeCmd utility. Every EdgeCmd utility command has to be preceded by edgecmd . Help edgecmd command Description Examples edgecmd Help Display general instructions on how to use the EdgeCmd utility. edgecmd Help \u003ccomponentName\u003e Display help for a specific Edge Data Store component. edgecmd Help System edgecmd Help \u003ccomponentName\u003e \u003cfacetName\u003e Display help for a specific facet of an Edge Data store component. edgecmd Help System Port Configuration System edgecmd command Description Examples edgecmd Configuration Display the entire configuration for every Edge Data Store component. edgecmd Configuration System Components Display the components that are currently configured. edgecmd Configuration System Components componentId=\u003ccomponentId\u003e componentType=\u003ccomponentType\u003e Add a new component. edgecmd Configuration System Components componentId=Modbus1 componentType=Modbus edgecmd Configuration System Components id=\u003ccomponentId\u003e delete Delete a component. edgecmd Configuration System Components id=Modbus1 delete Components edgecmd command Description Examples edgecmd Configuration \u003ccomponentId\u003e Display component specific configuration. edgecmd Configuration System or edgecmd Configuration OpcUa1 edgecmd Configuration \u003ccomponentId\u003e \u003cfacetName\u003e Display facet specific configuration of an Edge Data Store component. edgecmd Configuration Storage Logging edgecmd Configuration \u003ccomponentId\u003e \u003cfacetName\u003e id=\u003cIndexToRetrieve\u003e Display the configuration of specific entry of a facet. edgecmd Configuration Storage PeriodicEgressEndpoints id=Endpoint1 edgecmd Configuration \u003ccomponentId\u003e DataSource Configure the data source for a Modbus TCP EDS adapter component or an OPC UA EDS adapter component. For examples, see OPC UA data source configuration and Modbus TCP data source configuration . edgecmd Configuration \u003ccomponentId\u003e DataSelection Configure the data selection for a Modbus TCP EDS adapter component or an OPC UA EDS adapter component. For examples, see OPC UA data selection configuration and Modbus TCP data selection configuration . edgecmd Configuration \u003ccomponentId\u003e Logging Configure logging for a Modbus TCP EDS adapter component or an OPC UA EDS adapter component. For examples, see Component-level logging configuration . Configuration with JSON files edgecmd command Description Examples edgecmd Configuration file=\u003cPathToJsonFile\u003e Import a bulk configuration through a JSON file. edgecmd Configuration file=\"~/Bulk_Storage_Runtime.json\" file=\"~ Bulk_Storage_Runtime.json\" edgecmd Configuration \u003ccomponentId\u003e \u003cfacetName\u003e file=\u003cPathToJsonFile\u003e Import a facet specific configuration file for a component. edgecmd Configuration Modbus1 DataSource file=\"~/Modbus_DataSource.json\" file=\"~ Modbus_DataSource.json\""
                                                      },
    "z-archive/Reference/Reference_1-0.html":  {
                                                   "href":  "z-archive/Reference/Reference_1-0.html",
                                                   "title":  "Reference",
                                                   "keywords":  "Reference This topic provides documentation of Sequential Data Store functions. It also includes current Release Notes for the Edge Data Store."
                                               },
    "z-archive/Reference/REST commands_1-0.html":  {
                                                       "href":  "z-archive/Reference/REST commands_1-0.html",
                                                       "title":  "REST commands",
                                                       "keywords":  "REST commands The following tables provide an overview of available REST commands that you can use with components of Edge Data Store. Note: The difference between the POST and PUT methods is that POST enables you to create a configuration, while PUT replaces a configuration. If you use POST on an existing configuration, the request will fail. Administration Description Method Endpoint Delete and reset all event and configuration data related to the Edge Data Store component POST http://localhost:5590/api/v1/administration/Storage/Reset http:  localhost:5590 api v1 administration Storage Reset Reset Edge Data Store POST http://localhost:5590/api/v1/administration/System/Reset http:  localhost:5590 api v1 administration System Reset Stop an individual EDS adapter POST http://localhost:5590/api/v1/administration/EDS http:  localhost:5590 api v1 administration EDS adapterId/Stop adapterId Stop Start an individual EDS adapter POST http://localhost:5590/api/v1/administration/EDS http:  localhost:5590 api v1 administration EDS adapterId/Start adapterId Start Configuration Description Method Endpoint Verify correct installation of Edge Data Store Configure minimum Edge Data Store Configure maximum Ede Data Store PUT http://localhost:5590/api/v1/configuration http:  localhost:5590 api v1 configuration System Description Method Endpoint Configure system components PUT http://localhost:5590/api/v1/configuration/system/components http:  localhost:5590 api v1 configuration system components Configure system port PUT http://localhost:5590/api/v1/configuration/system/port http:  localhost:5590 api v1 configuration system port Configure system logging PUT http://localhost:5590/api/v1/configuration/System/Logging http:  localhost:5590 api v1 configuration System Logging Configure system health endpoints PUT http://localhost:5590/api/v1/configuration/System/HealthEndpoints http:  localhost:5590 api v1 configuration System HealthEndpoints Storage Description Method Endpoint Configure data egress (to either AVEVA Data Hub or PI Web API) PUT http://localhost:5590/api/v1/configuration/storage/PeriodicEgressEndpoints/ http:  localhost:5590 api v1 configuration storage PeriodicEgressEndpoints  EDS adapters Note: Substitute the ID number of the adapter that you are configuring, for example OpcUa1 or OpcUa2 or Modbus3 , and so on. OPC UA Description Method Endpoint Configure an OPC UA data source PUT http://localhost:5590/api/v1/configuration/OpcUa1/Datasource http:  localhost:5590 api v1 configuration OpcUa1 Datasource Configure OPC UA data selection PUT http://localhost:5590/api/v1/configuration/OpcUa1/Dataselection http:  localhost:5590 api v1 configuration OpcUa1 Dataselection Change OPC UA logging configuration PUT http://localhost:5590/api/v1/configuration/OpcUa1/Logging http:  localhost:5590 api v1 configuration OpcUa1 Logging Modbus TCP Description Method Endpoint Configure a Modbus TCP data source PUT http://localhost:5590/api/v1/configuration/Modbus1/Datasource http:  localhost:5590 api v1 configuration Modbus1 Datasource Configure Modbus TCP data selection PUT http://localhost:5590/api/v1/configuration/Modbus1/Datasource http:  localhost:5590 api v1 configuration Modbus1 Datasource Change Modbus TCP logging configuration PUT http://localhost:5590/api/v1/configuration/Modbus1/Logging http:  localhost:5590 api v1 configuration Modbus1 Logging Tenants Types Description Method Endpoint Create an SDS type POST http://localhost:5590/api/v1/tenants/default/namespaces/default/types/Simple http:  localhost:5590 api v1 tenants default namespaces default types Simple Streams Description Method Endpoint Create an SDS stream POST http://localhost:5590/api/v1/tenants/default/namespaces/default/streams/Simple http:  localhost:5590 api v1 tenants default namespaces default streams Simple View streams that have been created in Storage POST http://localhost:5590/api/v1/tenants/default/namespaces/default/streams/ http:  localhost:5590 api v1 tenants default namespaces default streams  Write data events to the SDS stream POST http://localhost:5590/api/v1/tenants/default/namespaces/default/streams/Simple/Data http:  localhost:5590 api v1 tenants default namespaces default streams Simple Data Read last data value written to server POST http://localhost:5590/api/v1/tenants/default/namespaces/default/streams/Simple/Data/Last http:  localhost:5590 api v1 tenants default namespaces default streams Simple Data Last Read a time range of values written to server. (Example) POST http://localhost:5590/api/v1/tenants/default/namespaces/default/streams/Simple/Data?startIndex=2017-07-08T13:00:00Z\u0026count=100 http:  localhost:5590 api v1 tenants default namespaces default streams Simple Data?startIndex=2017-07-08T13:00:00Z\u0026count=100 Read last container data value written to the server (using SDS) POST http://localhost:5590/api/v1/tenants/default/namespaces/default/streams/MyCustomContainer/Data/Last http:  localhost:5590 api v1 tenants default namespaces default streams MyCustomContainer Data Last Read a time range of container values written to server (using SDS) (Example) POST http://localhost:5590/api/v1/tenants/default/namespaces/default/streams/MyCustomContainer/Data?startIndex=2017-07-08T13:00:00Z\u0026count=100 http:  localhost:5590 api v1 tenants default namespaces default streams MyCustomContainer Data?startIndex=2017-07-08T13:00:00Z\u0026count=100 OMF Description Method Endpoint Create an OMF type Create an OMF container Write data events to the OMF container POST http://localhost:5590/api/v1/tenants/default/namespaces/default/omf/ http:  localhost:5590 api v1 tenants default namespaces default omf  Create an OMF container POST http://localhost:5590/api/v1/tenants/default/namespaces/default/omf/ http:  localhost:5590 api v1 tenants default namespaces default omf "
                                                   },
    "z-archive/Schemas/ConfigurationSchemaList.html":  {
                                                           "href":  "z-archive/Schemas/ConfigurationSchemaList.html",
                                                           "title":  "Configuration schemas",
                                                           "keywords":  "Configuration schemas Edge Data Store is configured through a series of JSON files. The schemas for these files are provided in the installation directory and are documented following. Edge Data Store configuration schemas Use the following schemas to configure Edge Data Store: EdgeLoggerConfiguration PortConfiguration OmfHealthEndpointConfiguration EdgeDataStoreConfig EDS adapters configuration schemas Use the following schemas to configure EDS adapters: OPC UA DataSourceConfiguration DataCollectionItem EdgeLoggerConfiguration Modbus TCP DataSourceConfiguration DataSelectionConfiguration EdgeLoggerConfiguration Storage configuration schemas Use the following schemas to configure the Storage component: EdgeLoggerConfiguration StorageRuntimeConfiguration OEMConfiguration Periodic Egress Endpoints"
                                                       },
    "z-archive/Schemas/EdgeSystem_schema.html":  {
                                                     "href":  "z-archive/Schemas/EdgeSystem_schema.html",
                                                     "title":  "Sample Edge Data Store configuration",
                                                     "keywords":  "Sample Edge Data Store configuration The Edge Data Store configuration schema specifies how to formally describe the system parameters (logging, components, health endpoints, port). \"System\": { \"Logging\": { \"logLevel\": \"Information\", \"logFileSizeLimitBytes\": 34636833, \"logFileCountLimit\": 31 }, \"Components\": [{ \"componentId\": \"OpcUa1\", \"componentType\": \"OpcUa\" }, { \"componentId\": \"Modbus1\", \"componentType\": \"Modbus\" }, { \"componentId\": \"Storage\", \"componentType\": \"EDS.Component\" } ], \"HealthEndpoints\": [], \"Port\": { \"port\": 5590 } } Edge Data Store configuration properties Property Type Required Nullable Defined by Storage StorageConfiguration Optional Yes StorageConfiguration System SystemConfiguration Optional Yes SystemConfiguration {ComponentName} {ComponentConfiguration} Optional Yes {ComponentConfiguration}"
                                                 },
    "z-archive/Schemas/Modbus_DataSelection_schema.html":  {
                                                               "href":  "z-archive/Schemas/Modbus_DataSelection_schema.html",
                                                               "title":  "Sample Modbus TCP data selection configuration",
                                                               "keywords":  "Sample Modbus TCP data selection configuration [{ \"Selected\": \"true\", \"UnitId\": 1, \"RegisterType\": 3, \"RegisterOffset\": 1, \"DataTypeCode\": 20, \"BitMap\": \"16151413\", \"ConversionFactor\": 2, \"ConversionOffset\": 3.4, \"ScanRate\": 500 }, { \"Selected\": \"true\", \"UnitId\": 1, \"RegisterType\": 3, \"RegisterOffset\": 2, \"DataTypeCode\": 20, \"BitMap\": \"16151413\", \"ConversionFactor\": 2, \"ConversionOffset\": 3.4, \"ScanRate\": 500 }, { \"Selected\": \"true\", \"UnitId\": 1, \"RegisterType\": 3, \"RegisterOffset\": 3, \"DataTypeCode\": 20, \"BitMap\": \"16151413\", \"ConversionFactor\": 2, \"ConversionOffset\": 3.4, \"ScanRate\": 500 }, { \"Selected\": \"true\", \"UnitId\": 1, \"RegisterType\": 3, \"RegisterOffset\": 4, \"DataTypeCode\": 20, \"BitMap\": \"16151413\", \"ConversionFactor\": 2, \"ConversionOffset\": 3.4, \"ScanRate\": 500 }, { \"Selected\": \"true\", \"UnitId\": 1, \"RegisterType\": 3, \"RegisterOffset\": 5, \"DataTypeCode\": 20, \"BitMap\": \"16151413\", \"ConversionFactor\": 2, \"ConversionOffset\": 3.4, \"ScanRate\": 500 } ] Modbus TCP data selection configuration properties Property Type Required Nullable Defined by BitMap string Optional Yes DataSelectionConfiguration (this schema) ConversionFactor number Optional Yes DataSelectionConfiguration (this schema) ConversionOffset number Optional Yes DataSelectionConfiguration (this schema) DataTypeCode integer Optional No DataSelectionConfiguration (this schema) Name string Optional Yes DataSelectionConfiguration (this schema) RegisterOffset integer Optional No DataSelectionConfiguration (this schema) RegisterType reference #/definitions/ModbusRegisterType # definitions ModbusRegisterType Optional No DataSelectionConfiguration (this schema) ScanRate integer Optional No DataSelectionConfiguration (this schema) Selected boolean Optional No DataSelectionConfiguration (this schema) StreamId string Optional Yes DataSelectionConfiguration (this schema) UnitId integer Optional No DataSelectionConfiguration (this schema) Note: All of the following requirements need to be fulfilled. Requirement 1 ??? #/definitions/EdgeConfigurationBase # definitions EdgeConfigurationBase Requirement 2 object with following properties: Property Type Required BitMap string Optional ConversionFactor number Optional ConversionOffset number Optional DataTypeCode integer Optional Name string Optional RegisterOffset integer Optional RegisterType Optional ScanRate integer Optional Selected boolean Optional StreamId string Optional UnitId integer Optional"
                                                           },
    "z-archive/Schemas/Modbus_DataSource_schema.html":  {
                                                            "href":  "z-archive/Schemas/Modbus_DataSource_schema.html",
                                                            "title":  "Sample Modbus TCP data source configuration",
                                                            "keywords":  "Sample Modbus TCP data source configuration { \"IpAddress\": \"\u003cModbus IP Address\u003e\", \"Port\": \u003cPort - usually 502\u003e, \"ConnectTimeout\": 15000, \"ReconnectInterval\": 5000, \"RequestTimeout\": 9000, \"DelayBetweenRequests\": 0, \"MaxResponseDataLength\": 250 } DataSourceConfiguration properties Property Type Required Nullable Defined by ConnectTimeout integer Optional No DataSourceConfiguration (this schema) DelayBetweenRequests integer Optional No DataSourceConfiguration (this schema) IpAddress string Optional Yes DataSourceConfiguration (this schema) MaxResponseDataLength integer Optional No DataSourceConfiguration (this schema) Port integer Optional No DataSourceConfiguration (this schema) ReconnectInterval integer Optional No DataSourceConfiguration (this schema) RequestTimeout integer Optional No DataSourceConfiguration (this schema) StreamIdPrefix string Optional Yes DataSourceConfiguration (this schema) Note: All of the following requirements need to be fulfilled. Requirement 1 ??? #/definitions/EdgeConfigurationBase # definitions EdgeConfigurationBase Requirement 2 object with following properties: Property Type Required ConnectTimeout integer Optional DelayBetweenRequests integer Optional IpAddress string Optional MaxResponseDataLength integer Optional Port integer Optional ReconnectInterval integer Optional RequestTimeout integer Optional StreamIdPrefix string Optional"
                                                        },
    "z-archive/Schemas/Modbus_Logging_schema.html":  {
                                                         "href":  "z-archive/Schemas/Modbus_Logging_schema.html",
                                                         "title":  "ModbusLoggerConfiguration Properties",
                                                         "keywords":  "ModbusLoggerConfiguration Properties Property Type Required Nullable Defined by LogFileCountLimit integer Optional Yes EdgeLoggerConfiguration (this schema) LogFileSizeLimitBytes integer Optional Yes EdgeLoggerConfiguration (this schema) LogLevel reference Optional No EdgeLoggerConfiguration (this schema) LogFileCountLimit LogFileCountLimit is optional type: integer defined in this schema LogFileCountLimit type integer , nullable LogFileSizeLimitBytes LogFileSizeLimitBytes is optional type: integer defined in this schema LogFileSizeLimitBytes type integer , nullable LogLevel LogLevel is optional type: reference defined in this schema LogLevel type ??? #/definitions/EdgeLogLevel # definitions EdgeLogLevel All of the following requirements need to be fulfilled. Requirement 1 ??? #/definitions/EdgeConfigurationBase # definitions EdgeConfigurationBase Requirement 2 object with following properties: Property Type Required LogFileCountLimit integer Optional LogFileSizeLimitBytes integer Optional LogLevel Optional LogFileCountLimit LogFileCountLimit is optional type: integer LogFileCountLimit type integer , nullable LogFileSizeLimitBytes LogFileSizeLimitBytes is optional type: integer LogFileSizeLimitBytes type integer , nullable LogLevel LogLevel is optional type: reference LogLevel type ??? #/definitions/EdgeLogLevel # definitions EdgeLogLevel"
                                                     },
    "z-archive/Schemas/Modbus_schema.html":  {
                                                 "href":  "z-archive/Schemas/Modbus_schema.html",
                                                 "title":  "Modbus TCP configuration properties",
                                                 "keywords":  "Modbus TCP configuration properties Property Type Required Nullable Defined by Logging ModbusLoggingConfiguration Optional Yes EdgeLoggerConfiguration DataSource DataSourceConfiguration Optional Yes ComponentsConfiguration DataSelection [ModbusDataSelectionConfiguration] Optional Yes DataSelectionConfiguration"
                                             },
    "z-archive/Schemas/OpcUa_DataSelection_schema.html":  {
                                                              "href":  "z-archive/Schemas/OpcUa_DataSelection_schema.html",
                                                              "title":  "Sample OPC UA data selection configuration",
                                                              "keywords":  "Sample OPC UA data selection configuration The OPC UA data selection configuration schema specifies how to formally describe the data selection parameters for OPC UA. [{ \"Selected\": true, \"Name\": \"Cold Side Inlet Temperature\", \"NodeId\": \"ns=2;s=Line1.HeatExchanger1001.ColdSideInletTemperature\", \"StreamId\": null }, { \"Selected\": true, \"Name\": \"Hot Side Inlet Temperature\", \"NodeId\": \"ns=2;s=Line1.HeatExchanger1001.HotSideInletTemperature\", \"StreamId\": null }, { \"Selected\": true, \"Name\": \"Hot Side Outlet Temperature\", \"NodeId\": \"ns=2;s=Line1.HeatExchanger1001.HotSideOutletTemperature\", \"StreamId\": null }, { \"Selected\": true, \"Name\": \"Cold Side Inlet Temperature\", \"NodeId\": \"ns=2;s=Line1.HeatExchanger1002.ColdSideInletTemperature\", \"StreamId\": null }, { \"Selected\": true, \"Name\": \"Hot Side Outlet Temperature\", \"NodeId\": \"ns=2;s=Line1.HeatExchanger1002.HotSideOutletTemperature\", \"StreamId\": null } ] OPC UA data selection configuration properties Property Type Required Nullable Defined by Name string Optional Yes DataCollectionItem (this schema) NodeId string Optional Yes DataCollectionItem (this schema) Selected boolean Optional No DataCollectionItem (this schema) StreamId string Optional Yes DataCollectionItem (this schema) Note: All of the following requirements need to be fulfilled. Requirement 1 ??? #/definitions/EdgeConfigurationBase # definitions EdgeConfigurationBase Requirement 2 object with following properties: Property Type Required Name string Optional NodeId string Optional Selected boolean Optional StreamId string Optional"
                                                          },
    "z-archive/Schemas/OpcUa_DataSource_schema.html":  {
                                                           "href":  "z-archive/Schemas/OpcUa_DataSource_schema.html",
                                                           "title":  "Sample OPC UA data source configuration",
                                                           "keywords":  "Sample OPC UA data source configuration The OPC UA data source configuration schema specifies how to formally describe the data source parameters for OPC UA. { \"EndpointUrl\": \"opc.tcp://\u003cip \"opc.tcp:  \u003cip address\u003e:\u003cport - often 62541\u003e/\u003cserver 62541\u003e \u003cserver path\u003e\", \"UseSecureConnection\": false, \"UserName\": null, \"Password\": null, \"RootNodeIds\": null, \"IncomingTimestamp\": \"Source\", \"StreamIdPrefix\": \"OpcUa\" } OPC UA data source configuration properties Property Type Required Nullable Defined by EndpointUrl string Optional Yes DataSourceConfiguration (this schema) IncomingTimestamp reference Optional No DataSourceConfiguration (this schema) Password string Optional Yes DataSourceConfiguration (this schema) RootNodeIds string Optional Yes DataSourceConfiguration (this schema) StreamIdPrefix string Optional Yes DataSourceConfiguration (this schema) UseSecureConnection boolean Optional No DataSourceConfiguration (this schema) UserName string Optional Yes DataSourceConfiguration (this schema) Note: All of the following requirements need to be fulfilled. Requirement 1 ??? #/definitions/EdgeConfigurationBase # definitions EdgeConfigurationBase Requirement 2 object with following properties: Property Type Required EndpointUrl string Optional IncomingTimestamp Optional Password string Optional RootNodeIds string Optional StreamIdPrefix string Optional UseSecureConnection boolean Optional UserName string Optional"
                                                       },
    "z-archive/Schemas/OpcUa_Logging_schema.html":  {
                                                        "href":  "z-archive/Schemas/OpcUa_Logging_schema.html",
                                                        "title":  "OPC UA logging configuration properties",
                                                        "keywords":  "OPC UA logging configuration properties Property Type Required Nullable Defined by LogFileCountLimit integer Optional Yes EdgeLoggerConfiguration (this schema) LogFileSizeLimitBytes integer Optional Yes EdgeLoggerConfiguration (this schema) LogLevel reference Optional No EdgeLoggerConfiguration (this schema) Note: All of the following requirements need to be fulfilled. Requirement 1 ??? #/definitions/EdgeConfigurationBase # definitions EdgeConfigurationBase Requirement 2 object with following properties: Property Type Required LogFileCountLimit integer Optional LogFileSizeLimitBytes integer Optional LogLevel Optional"
                                                    },
    "z-archive/Schemas/OpcUa_schema.html":  {
                                                "href":  "z-archive/Schemas/OpcUa_schema.html",
                                                "title":  "OPC UA configuration properties",
                                                "keywords":  "OPC UA configuration properties Property Type Required Nullable Defined by Logging OpcUaLoggingConfiguration Optional Yes EdgeLoggerConfiguration DataSource DataSourceConfiguration Optional Yes ComponentsConfiguration DataSelection [OpcUaDataSelectionConfiguration] Optional Yes DataSelectionConfiguration"
                                            },
    "z-archive/Schemas/Storage_Logging_schema.html":  {
                                                          "href":  "z-archive/Schemas/Storage_Logging_schema.html",
                                                          "title":  "Storage logging configuration properties",
                                                          "keywords":  "Storage logging configuration properties Property Type Required Nullable Defined by LogFileCountLimit integer Optional Yes EdgeLoggerConfiguration (this schema) LogFileSizeLimitBytes integer Optional Yes EdgeLoggerConfiguration (this schema) LogLevel reference Optional No EdgeLoggerConfiguration (this schema) Note: All of the following requirements need to be fulfilled. Requirement 1 ??? #/definitions/EdgeConfigurationBase # definitions EdgeConfigurationBase Requirement 2 object with following properties: Property Type Required LogFileCountLimit integer Optional LogFileSizeLimitBytes integer Optional LogLevel Optional"
                                                      },
    "z-archive/Schemas/Storage_PeriodicEgressEndpoints_schema.html":  {
                                                                          "href":  "z-archive/Schemas/Storage_PeriodicEgressEndpoints_schema.html",
                                                                          "title":  "Sample periodic egress configuration",
                                                                          "keywords":  "Sample periodic egress configuration [{ \"Id\": \"AVEVA Data Hub Data\", \"ExecutionPeriod\": \"00:00:50\", \"Name\": null, \"NamespaceId\": \"default\", \"Description\": null, \"Enabled\": true, \"Backfill\": false, \"EgressFilter\": \"\", \"StreamPrefix\": \"\u003cmakeunique\u003e\", \"TypePrefix\": \"\u003cmakeunique\u003e\", \"Endpoint\": \"https://\u003cAVEVA \"https:  \u003cAVEVA Data Hub OMF endpoint\u003e\", \"ClientId\": \"\u003cclientid\u003e\", \"ClientSecret\": \"\u003cclientsecret\u003e\", \"UserName\": null, \"Password\": null, \"DebugExpiration\": null, \"TokenEndpoint\": null, \"ValidateEndpointCertificate\": true }, { \"Id\": \"PI Web API Data\", \"ExecutionPeriod\": \"00:00:50\", \"Name\": null, \"NamespaceId\": \"default\", \"Description\": null, \"Enabled\": true, \"Backfill\": false, \"EgressFilter\": \"\", \"StreamPrefix\": \"\u003cmakeunique\u003e\", \"TypePrefix\": \"\u003cmakeunique\u003e\", \"Endpoint\": \"https://\u003cyourserver\u003e/piwebapi/omf/\", \"https:  \u003cyourserver\u003e piwebapi omf \", \"ClientId\": null, \"ClientSecret\": null, \"UserName\": \"\u003cusername\u003e\", \"Password\": \"\u003cpassword\u003e\", \"DebugExpiration\": null, \"TokenEndpoint\": null, \"ValidateEndpointCertificate\": true }, { \"Id\": \"AVEVA Data Hub Diagnostics\", \"ExecutionPeriod\": \"00:00:50\", \"Name\": null, \"NamespaceId\": \"diagnostics\", \"Description\": null, \"Enabled\": true, \"Backfill\": false, \"EgressFilter\": \"\", \"StreamPrefix\": \"\u003cmakeunique\u003e\", \"TypePrefix\": \"\u003cmakeunique\u003e\", \"Endpoint\": \"https://\u003cAVEVA \"https:  \u003cAVEVA Data Hub OMF endpoint\u003e\", \"ClientId\": \"\u003cclientid\u003e\", \"ClientSecret\": \"\u003cclientsecret\u003e\", \"UserName\": null, \"Password\": null, \"DebugExpiration\": null, \"TokenEndpoint\": null, \"ValidateEndpointCertificate\": true }, { \"Id\": \"PI Web API Diagnostics\", \"ExecutionPeriod\": \"00:00:50\", \"Name\": null, \"NamespaceId\": \"diagnostics\", \"Description\": null, \"Enabled\": true, \"Backfill\": false, \"EgressFilter\": \"\", \"StreamPrefix\": \"\u003cmakeunique\u003e\", \"TypePrefix\": \"\u003cmakeunique\u003e\", \"Endpoint\": \"https://\u003cyourserver\u003e/piwebapi/omf/\", \"https:  \u003cyourserver\u003e piwebapi omf \", \"ClientId\": null, \"ClientSecret\": null, \"UserName\": \"\u003cusername\u003e\", \"Password\": \"\u003cpassword\u003e\", \"DebugExpiration\": null, \"TokenEndpoint\": null, \"ValidateEndpointCertificate\": true } ] Periodic egress configuration properties Property Type Required Nullable Defined by Backfill boolean Optional No PeriodicEgressConfiguration (this schema) ClientId string Optional Yes PeriodicEgressConfiguration (this schema) ClientSecret string Optional Yes PeriodicEgressConfiguration (this schema) DebugExpiration string Optional Yes PeriodicEgressConfiguration (this schema) Description string Optional Yes PeriodicEgressConfiguration (this schema) EgressFilter string Optional Yes PeriodicEgressConfiguration (this schema) Enabled boolean Optional No PeriodicEgressConfiguration (this schema) Endpoint string Required No PeriodicEgressConfiguration (this schema) ExecutionPeriod string Required No PeriodicEgressConfiguration (this schema) Id string Optional Yes PeriodicEgressConfiguration (this schema) Name string Optional Yes PeriodicEgressConfiguration (this schema) NamespaceId string Optional Yes PeriodicEgressConfiguration (this schema) Password string Optional Yes PeriodicEgressConfiguration (this schema) StreamPrefix string Optional Yes PeriodicEgressConfiguration (this schema) TokenEndpoint string Optional Yes PeriodicEgressConfiguration (this schema) TypePrefix string Optional Yes PeriodicEgressConfiguration (this schema) UserName string Optional Yes PeriodicEgressConfiguration (this schema) ValidateEndpointCertificate boolean Optional No PeriodicEgressConfiguration (this schema) Note: All of the following requirements need to be fulfilled. Requirement 1 ??? #/definitions/EdgeConfigurationBase # definitions EdgeConfigurationBase Requirement 2 object with following properties: Property Type Required Backfill boolean Optional ClientId string Optional ClientSecret string Optional DebugExpiration string Optional Description string Optional EgressFilter string Optional Enabled boolean Optional Endpoint string Required ExecutionPeriod string Required Id string Optional Name string Optional NamespaceId string Optional Password string Optional StreamPrefix string Optional TokenEndpoint string Optional TypePrefix string Optional UserName string Optional ValidateEndpointCertificate boolean Optional"
                                                                      },
    "z-archive/Schemas/Storage_Runtime_schema.html":  {
                                                          "href":  "z-archive/Schemas/Storage_Runtime_schema.html",
                                                          "title":  "Sample storage runtime configuration",
                                                          "keywords":  "Sample storage runtime configuration { \"StreamStorageLimitMb\": 2, \"StreamStorageTargetMb\": 1, \"IngressDebugExpiration\": \"0001-01-01T00:00:00\" } Storage runtime configuration properties Property Type Required Nullable Defined by IngressDebugExpiration string Required No StorageRuntimeConfiguration (this schema) StreamStorageLimitMb integer Required No StorageRuntimeConfiguration (this schema) StreamStorageTargetMb integer Required No StorageRuntimeConfiguration (this schema) IngressDebugExpiration Ingress Debug Expiration is a property that can be used when debugging OMF. If the date and time is the future incoming OMF messages will be logged until the date and time specified. Once the configured time is past OMF messages will no longer be logged for debugging purposes. IngressDebugExpiration type string format: date-time ??? date and time (according to RFC 3339, section 5.6 ) minimum length: 1 characters StreamStorageLimitMb StreamStorageLimitMb is the maximum size in megabytes that a stream can reach. When a stream exceeds the size specified, older data will be deleted from the file. Data will be removed from the stream until the stream is at or below the StreamStorageTargetMb value. It is recommended that the target value be smaller than the limit since trimming can be an expensive operation and should be done infrequently. StreamStorageLimitMb type integer minimum value: 2 maximum value: 2147483647 StreamStorageTargetMb StreamStorageTargetMb is the size in megabytes that a stream will be reduced to after StreamStorageLimitMb size is reached for a single stream. When a stream exceeds the size specified, older data will be deleted from the file. Data will be removed from the stream until the stream is at or below the StreamStorageTargetMb value. It is recommended that the target value be smaller than the limit since trimming can be an expensive operation and should be done infrequently. StreamStorageTargetMb type integer minimum value: 1 maximum value: 2147483647 Note: All of the following requirements need to be fulfilled. Requirement 1 ??? #/definitions/EdgeConfigurationBase # definitions EdgeConfigurationBase Requirement 2 object with following properties: Property Type Required IngressDebugExpiration string Required StreamStorageLimitMb integer Required StreamStorageTargetMb integer Required"
                                                      },
    "z-archive/Schemas/Storage_schema.html":  {
                                                  "href":  "z-archive/Schemas/Storage_schema.html",
                                                  "title":  "Sample Storage configuration",
                                                  "keywords":  "Sample Storage configuration \"Storage\": { \"Runtime\": { \"streamStorageLimitMb\": 2, \"streamStorageTargetMb\": 1, \"ingressDebugExpiration\": \"0001-01-01T00:00:00\" }, \"Logging\": { \"logLevel\": \"Information\", \"logFileSizeLimitBytes\": 34636833, \"logFileCountLimit\": 31 }, \"PeriodicEgressEndpoints\": [] } Storage configuration properties Property Type Required Nullable Defined by Runtime StorageRuntimeConfiguration Optional Yes StorageRuntimeConfiguration Logging StorageLoggingConfiguration Optional Yes StorageLoggingConfiguration PeriodicEgressEndpoints [PeriodicEgressEndpointsConfiguration] Optional Yes PeriodicEgressEndpointsConfiguration"
                                              },
    "z-archive/Schemas/System_Components_schema.html":  {
                                                            "href":  "z-archive/Schemas/System_Components_schema.html",
                                                            "title":  "Sample Edge Data Store components configuration",
                                                            "keywords":  "Sample Edge Data Store components configuration [ { \"ComponentId\": \"OpcUa1\", \"ComponentType\": \"OpcUa\" }, { \"ComponentId\": \"Modbus1\", \"ComponentType\": \"Modbus\" }, { \"ComponentId\": \"Storage\", \"ComponentType\": \"EDS.Component\" } ] Edge Data Store components configuration properties Property Type Required Nullable Group ComponentId string Optional Yes #/definitions/EdgeComponentConfig # definitions EdgeComponentConfig ComponentType string Optional Yes #/definitions/EdgeComponentConfig # definitions EdgeComponentConfig Edge Data Store configuration properties Property Type Required Nullable Defined by ComponentConfigurations reference #/definitions/EdgeComponentConfig # definitions EdgeComponentConfig Optional Yes EdgeDataStoreConfig (this schema) Note: All of the following requirements need to be fulfilled. Requirement 1 ??? #/definitions/EdgeConfigurationBase # definitions EdgeConfigurationBase Requirement 2 object with following properties: Property Type Required ComponentConfigurations reference #/definitions/EdgeComponentConfig # definitions EdgeComponentConfig Optional"
                                                        },
    "z-archive/Schemas/System_HealthEndpoints_schema.html":  {
                                                                 "href":  "z-archive/Schemas/System_HealthEndpoints_schema.html",
                                                                 "title":  "Sample OMF health endpoint configuration",
                                                                 "keywords":  "Sample OMF health endpoint configuration The OMF health endpoint configuration schema specifies how to formally describe the OMF health endpoint parameters. [{ \"endpoint\": \"https://\u003cpi \"https:  \u003cpi web api server\u003e/piwebapi/omf/\", server\u003e piwebapi omf \", \"UserName\": \"\u003cusername\u003e\", \"Password\": \"\u003cpassword\u003e\", \"buffering\": 0, \"maxBufferSizeMB\": 0 }, { \"Endpoint\": \"https://\u003cAVEVA \"https:  \u003cAVEVA Data Hub OMF endpoint\u003e\", \"ClientId\": \"\u003cclientid\u003e\", \"ClientSecret\": \"\u003cclientsecret\u003e\", \"buffering\": 0, \"maxBufferSizeMB\": 0 } ] OMF health endpoint configuration properties Property Type Required Nullable Defined by Buffering reference #/definitions/BufferType # definitions BufferType Optional No OmfHealthEndpointConfiguration (this schema) ClientId string Optional Yes OmfHealthEndpointConfiguration (this schema) ClientSecret string Optional Yes OmfHealthEndpointConfiguration (this schema) Endpoint string Optional Yes OmfHealthEndpointConfiguration (this schema) Id string Optional Yes OmfHealthEndpointConfiguration (this schema) MaxBufferSizeMB integer Optional No OmfHealthEndpointConfiguration (this schema) Password string Optional Yes OmfHealthEndpointConfiguration (this schema) UserName string Optional Yes OmfHealthEndpointConfiguration (this schema) ValidateEndpointCertificate boolean Optional No OmfHealthEndpointConfiguration (this schema) Note: All of the following requirements need to be fulfilled. Requirement 1 ??? #/definitions/EdgeConfigurationBase # definitions EdgeConfigurationBase Requirement 2 object with following properties: Property Type Required Buffering reference #/definitions/BufferType # definitions BufferType Optional ClientId string Optional ClientSecret string Optional Endpoint string Optional Id string Optional MaxBufferSizeMB integer Optional Password string Optional UserName string Optional ValidateEndpointCertificate boolean Optional"
                                                             },
    "z-archive/Schemas/System_Logging_schema.html":  {
                                                         "href":  "z-archive/Schemas/System_Logging_schema.html",
                                                         "title":  "System logging configuration properties",
                                                         "keywords":  "System logging configuration properties Property Type Required Nullable Defined by LogFileCountLimit integer Optional Yes EdgeLoggerConfiguration (this schema) LogFileSizeLimitBytes integer Optional Yes EdgeLoggerConfiguration (this schema) LogLevel reference #/definitions/EdgeLogLevel # definitions EdgeLogLevel Optional No EdgeLoggerConfiguration (this schema) Note: All of the following requirements need to be fulfilled. Requirement 1 ??? #/definitions/EdgeConfigurationBase # definitions EdgeConfigurationBase Requirement 2 object with following properties: Property Type Required LogFileCountLimit integer Optional LogFileSizeLimitBytes integer Optional LogLevel reference #/definitions/EdgeLogLevel # definitions EdgeLogLevel Optional"
                                                     },
    "z-archive/Schemas/System_Port_schema.html":  {
                                                      "href":  "z-archive/Schemas/System_Port_schema.html",
                                                      "title":  "Sample system port configuration",
                                                      "keywords":  "Sample system port configuration { \"Port\": 5590 } System port configuration properties Property Type Required Nullable Defined by Port integer minimum value: 1024 maximum value: 65535 Optional No PortConfiguration (this schema) Note: All of the following requirements need to be fulfilled. Requirement 1 ??? #/definitions/EdgeConfigurationBase # definitions EdgeConfigurationBase Requirement 2 object with following properties: Property Type Required Port integer Optional"
                                                  },
    "z-archive/Schemas/System_schema.html":  {
                                                 "href":  "z-archive/Schemas/System_schema.html",
                                                 "title":  "Sample system configuration",
                                                 "keywords":  "Sample system configuration \"System\": { \"Logging\": { \"logLevel\": \"Information\", \"logFileSizeLimitBytes\": 34636833, \"logFileCountLimit\": 31 }, \"Components\": [{ \"componentId\": \"OpcUa1\", \"componentType\": \"OpcUa\" }, { \"componentId\": \"Modbus1\", \"componentType\": \"Modbus\" }, { \"componentId\": \"Storage\", \"componentType\": \"EDS.Component\" } ], \"HealthEndpoints\": [], \"Port\": { \"port\": 5590 } } System configuration properties Property Type Required Nullable Defined by Logging SystemLoggingConfiguration Optional Yes EdgeLoggerConfiguration Components [SystemComponentsConfiguration] Optional Yes ComponentsConfiguration HealthEndpoints [SystemHealthEndpointsConfiguration] Optional Yes HealthEndpointsConfiguration Port SystemPortConfiguration Optional Yes PortConfiguration"
                                             }
}
